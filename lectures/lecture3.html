
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Baseline models &amp; k-Nearest Neighbours &#8212; BAIT 509&lt;br&gt;Business Applications of Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="canonical" href="https://bait509-ubc.github.io/BAIT509/intro.html/lectures/lecture3.html" />
    <link rel="shortcut icon" href="../_static/bait_logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. kNN regression, Support Vector Machines, and Feature Preprocessing" href="lecture4.html" />
    <link rel="prev" title="2. Splitting and Cross-validation" href="lecture2.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/bait_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">BAIT 509<br>Business Applications of Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Things You Should Know
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/who.html">
   Who: Quan Nguyen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/how.html">
   How: The Course Structure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/what.html">
   What: Learning Outcomes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture1.html">
   1. Intro to ML &amp;  Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2.html">
   2. Splitting and Cross-validation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Baseline models &amp; k-Nearest Neighbours
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4.html">
   4. kNN regression, Support Vector Machines, and Feature Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5.html">
   5. Preprocessing Categorical Features and Column Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6.html">
   6. Naive Bayes and Hyperparameter Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture7.html">
   7. Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8.html">
   8. Business Objectives/Statistical Questions and Feature Selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9.html">
   9. Classification and Regression Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10a.html">
   10. Data Science Ethics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10b.html">
   11. Multi-Class Classification (Optional)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/attribution.html">
   Attribution
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/bait509-ubc/BAIT509/issues/new?title=Issue%20on%20page%20%2Flectures/lecture3.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/lecture3.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lecture-learning-objectives">
   3.1. Lecture Learning Objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#five-minute-recap-lightning-questions">
   3.2. Five Minute Recap/ Lightning Questions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-lingering-questions">
     3.2.1. Some lingering questions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#baseline-models">
   3.3. Baseline Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dummy-classifier">
     3.3.1. Dummy Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dummy-regressor">
     3.3.2. Dummy Regressor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-practice">
   3.4. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-analogy-based-models">
   3.5. Similarity/analogy-based models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     3.5.1. Example:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-analogy-based-models-in-real-life">
     3.5.2. Similarity/Analogy-based models in real life
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#terminology">
   3.6. Terminology
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-vectors">
     3.6.1. Feature Vectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance">
   3.7. Distance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#euclidean-distance">
     3.7.1. Euclidean distance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-euclidean-distance-by-hand">
       3.7.1.1. Calculating Euclidean distance “by hand”
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-euclidean-distance-with-sklearn">
       3.7.1.2. Calculating Euclidean distance  with
       <code class="docutils literal notranslate">
        <span class="pre">
         sklearn
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-nearest-neighbour">
   3.8. Finding the Nearest Neighbour
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nearest-city-to-a-query-point">
     3.8.1. Nearest city to a query point
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.9. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-nearest-neighbours-k-nns-classifier">
   3.10.
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbours (
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -NNs) Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-k">
   3.11. Choosing K
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-choose-k-n-neighbors">
     3.11.1. How to choose
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     (
     <code class="docutils literal notranslate">
      <span class="pre">
       n_neighbors
      </span>
     </code>
     )?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#curse-of-dimensionality">
   3.12. Curse of Dimensionality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   3.13. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-we-ve-learned-today-a-id-9-a">
   3.14. What We’ve Learned Today
   <a id="9">
   </a>
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Baseline models & k-Nearest Neighbours</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lecture-learning-objectives">
   3.1. Lecture Learning Objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#five-minute-recap-lightning-questions">
   3.2. Five Minute Recap/ Lightning Questions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-lingering-questions">
     3.2.1. Some lingering questions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#baseline-models">
   3.3. Baseline Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dummy-classifier">
     3.3.1. Dummy Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dummy-regressor">
     3.3.2. Dummy Regressor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-practice">
   3.4. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-analogy-based-models">
   3.5. Similarity/analogy-based models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     3.5.1. Example:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-analogy-based-models-in-real-life">
     3.5.2. Similarity/Analogy-based models in real life
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#terminology">
   3.6. Terminology
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-vectors">
     3.6.1. Feature Vectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance">
   3.7. Distance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#euclidean-distance">
     3.7.1. Euclidean distance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-euclidean-distance-by-hand">
       3.7.1.1. Calculating Euclidean distance “by hand”
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-euclidean-distance-with-sklearn">
       3.7.1.2. Calculating Euclidean distance  with
       <code class="docutils literal notranslate">
        <span class="pre">
         sklearn
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-nearest-neighbour">
   3.8. Finding the Nearest Neighbour
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nearest-city-to-a-query-point">
     3.8.1. Nearest city to a query point
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.9. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-nearest-neighbours-k-nns-classifier">
   3.10.
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbours (
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -NNs) Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-k">
   3.11. Choosing K
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-choose-k-n-neighbors">
     3.11.1. How to choose
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     (
     <code class="docutils literal notranslate">
      <span class="pre">
       n_neighbors
      </span>
     </code>
     )?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#curse-of-dimensionality">
   3.12. Curse of Dimensionality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   3.13. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-we-ve-learned-today-a-id-9-a">
   3.14. What We’ve Learned Today
   <a id="9">
   </a>
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="baseline-models-k-nearest-neighbours">
<h1><span class="section-number">3. </span>Baseline models &amp; k-Nearest Neighbours<a class="headerlink" href="#baseline-models-k-nearest-neighbours" title="Permalink to this headline">#</a></h1>
<div class="section" id="lecture-learning-objectives">
<h2><span class="section-number">3.1. </span>Lecture Learning Objectives<a class="headerlink" href="#lecture-learning-objectives" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">DummyRegressor</span></code> as baselines for machine learning problems.</p></li>
<li><p>Explain the notion of similarity-based algorithms .</p></li>
<li><p>Broadly describe how KNNs use distances.</p></li>
<li><p>Discuss the effect of using a small/large value of the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> when using the KNN algorithm</p></li>
<li><p>Describe the problem of the curse of dimensionality.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">plotting_functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="five-minute-recap-lightning-questions">
<h2><span class="section-number">3.2. </span>Five Minute Recap/ Lightning Questions<a class="headerlink" href="#five-minute-recap-lightning-questions" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>What are the 4 types of data/splits that we discussed last class?</p></li>
<li><p>What is the “Golden Rule of Machine Learning”?</p></li>
<li><p>What do we use to split our data?</p></li>
<li><p>What is overfitting and underfitting?</p></li>
</ul>
<div class="section" id="some-lingering-questions">
<h3><span class="section-number">3.2.1. </span>Some lingering questions<a class="headerlink" href="#some-lingering-questions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Are decision trees the most basic model?</p></li>
<li><p>What other models can we build?</p></li>
</ul>
</div>
</div>
<div class="section" id="baseline-models">
<h2><span class="section-number">3.3. </span>Baseline Models<a class="headerlink" href="#baseline-models" title="Permalink to this headline">#</a></h2>
<p>We saw in the last 2 lectures how to build decision tree models which are based on rules (if-else statements), but how can we be sure that these models are doing a good job besides just accuracy?</p>
<p>Back in high school in chemistry or biology, we’ve all likely seen and heard of the “control group” where we have an experimental group, does not receive any experimental treatment.  This control group increases the reliability of the results, often through a comparison between control measurements and the other measurements.</p>
<p>Our baseline model is something like a control group in the sense that it provides a way to sanity-check your machine learning model. We make baseline models not to use for prediction purposes, but as a reference point when we are building other more sophisticated models.</p>
<p>So what is a baseline model then?</p>
<ul class="simple">
<li><p>Baseline: A simple machine learning algorithm based on simple rules of thumb. For example,</p>
<ul>
<li><p>most frequent baseline: always predicts the most frequent label in the training set.</p></li>
</ul>
</li>
</ul>
<div class="section" id="dummy-classifier">
<h3><span class="section-number">3.3.1. </span>Dummy Classifier<a class="headerlink" href="#dummy-classifier" title="Permalink to this headline">#</a></h3>
<p>We are going to build a most frequent baseline model which always predicts the most frequently labelled in the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import dataset</span>
<span class="n">voting_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/cities_USA.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">voting_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># feature table</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">voting_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;vote&#39;</span><span class="p">)</span>

<span class="c1"># the target variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">voting_df</span><span class="p">[[</span><span class="s1">&#39;vote&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split into train and test</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We build our model, in the same way as we built a decision tree model but this time using <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code>.</p>
<p>Since we are using a “most frequent” baseline model, we specify the argument <code class="docutils literal notranslate"><span class="pre">strategy</span></code> as <code class="docutils literal notranslate"><span class="pre">&quot;most_frequent&quot;</span></code></p>
<p>There are other options that you you read about in the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html">sklearn documentation</a> but you just need to know <code class="docutils literal notranslate"><span class="pre">most_frequent</span></code> for now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a dummy classifier</span>
<span class="n">dummy_clf</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;most_frequent&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In the last lecture, we stated that it’s at this point that we would usually perform cross-validation
to evaluate the model performance.</p>
<p>With Dummy Classifiers, we won’t need to because we are not hyperparameter tuning. We are using this just to get a base training score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dummy_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we see what our model predicts on the feature table for our training split <code class="docutils literal notranslate"><span class="pre">X_train</span></code>, our model will predict the most frequent class from our training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also now take the test score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a good example of when we occasionally have test scores better than the training scores.</p>
<p>In this case, it’s higher because our test split has a higher proportion of observations that are of class blue and so more of them will be predicted correctly.</p>
<p>Now if we build a decision tree, we can compare it to the baseline model score.</p>
<p>Here we use cross-validation to score the model although we are not tuning any hyperparameters.
It is general a good idea to get into the habit of doing this for all models except baseline/dummy models,
because the split can influence the model score (this is true to a much lesser extent for baseline models)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a decision tree classifier</span>
<span class="n">dt_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># cross validate the decision tree classifier</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt_clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># mean score</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dt_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can see that our decision tree is doing better than a model build on this simple “most frequently” occurring model.
This makes us trust our model a little more
and we know that it has identified some structure in the data that is more complicated than simply predicting the most common label.
This is especially important if we have an unbalanced training data set with much more of one label than of another,
but there are also more sophisticated strategies to handle that situation,
which we will discuss later.</p>
</div>
<div class="section" id="dummy-regressor">
<h3><span class="section-number">3.3.2. </span>Dummy Regressor<a class="headerlink" href="#dummy-regressor" title="Permalink to this headline">#</a></h3>
<p>For a Dummy regressor, the same principles can be applied but by using different strategies.</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html?highlight=dummyregressor">“mean”, “median”, “quantile”, “constant”</a></p>
<p>The one we are going to become most familiar with is:</p>
<p><strong>Average (mean) target value:</strong> always predicts the mean of the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">house_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/kc_house_data.csv&quot;</span><span class="p">)</span>
<span class="n">house_df</span> <span class="o">=</span> <span class="n">house_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">])</span>
<span class="n">house_df</span>
</pre></div>
</div>
</div>
</div>
<p>Let get our <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> objects and split our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">house_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">house_df</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We still need to make sure we split our data with baseline models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>we need to import <code class="docutils literal notranslate"><span class="pre">DummyRegressor</span></code> and construct our model.</p>
<p>We specify <code class="docutils literal notranslate"><span class="pre">strategy=&quot;mean&quot;</span></code> however this is the default value so technically we don’t need to specify this.</p>
<p>We train our model and again, it’s not needed to cross-validate for this type of algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_reg</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">dummy_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we predict on our training data, we see it’s making the same prediction for each observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>if we compare the mean value of the target, we see that our model is simply predicting the average of the training data which is exactly what we expect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>How well does it do?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We get an <span class="math notranslate nohighlight">\(R^2\)</span> value of 0.0.</p>
<p>When a model has an  <span class="math notranslate nohighlight">\(R^2\)</span>=0 that means that the model is doing no better than a model that using the mean which is exactly the case here.</p>
<p>Looking at the test score we see that our model get’s a slightly negative value,
which means that it is doing a little worse than constantly predicting the mean of the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="let-s-practice">
<h2><span class="section-number">3.4. </span>Let’s Practice<a class="headerlink" href="#let-s-practice" title="Permalink to this headline">#</a></h2>
<p>1. Below we have the output of <code class="docutils literal notranslate"><span class="pre">y_train.value_counts()</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Position</span>
<span class="n">Forward</span>     <span class="mi">13</span>
<span class="n">Defense</span>      <span class="mi">7</span>
<span class="n">Goalie</span>       <span class="mi">2</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<p>In this scenario, what would a <code class="docutils literal notranslate"><span class="pre">DummyClassifier(strategy='most_frequent')</span></code> model predict on the following observation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="n">No</span><span class="o">.</span>  <span class="n">Age</span>  <span class="n">Height</span>  <span class="n">Weight</span>  <span class="n">Experience</span>     <span class="n">Salary</span>
<span class="mi">1</span>   <span class="mi">83</span>   <span class="mi">34</span>     <span class="mi">191</span>     <span class="mi">210</span>          <span class="mi">11</span>  <span class="mf">3200000.0</span>
</pre></div>
</div>
<p>2. When using a regression model, which of the following is not a possible return value from .score(X,y) ?</p>
<p>a) 0.0<br />
b) 1.0<br />
c) -0.1<br />
d) 1.5</p>
<p>3.  Below are the values for <code class="docutils literal notranslate"><span class="pre">y</span></code> that were used to train  <code class="docutils literal notranslate"><span class="pre">DummyRegressor(strategy='mean')</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Grade</span>
<span class="mi">0</span>     <span class="mi">75</span>
<span class="mi">1</span>     <span class="mi">80</span>
<span class="mi">2</span>     <span class="mi">90</span>
<span class="mi">3</span>     <span class="mi">95</span>
<span class="mi">4</span>     <span class="mi">85</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<p>What value will the model predict for every example?</p>
<div class="dropdown admonition">
<p class="admonition-title">Solutions!</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Forward</span></code></p></li>
<li><p>d) 1.5</p></li>
<li><p>85</p></li>
</ol>
</div>
</div>
<div class="section" id="similarity-analogy-based-models">
<h2><span class="section-number">3.5. </span>Similarity/analogy-based models<a class="headerlink" href="#similarity-analogy-based-models" title="Permalink to this headline">#</a></h2>
<p>Let’s switch topics from baseline models and talked about the concept of similarity based models.</p>
<p>Suppose you are given the following training images with the corresponding actor’s name as the label.
Then we are given an unseen test images of the same actors but from different angles and are asked to label a given test example.</p>
<p>An intuitive way to classify the test example is by finding the most “similar” example from the training images and use that label for the test example.</p>
<a class="reference internal image-reference" href="../_images/knn-motivation.png"><img alt="../_images/knn-motivation.png" src="../_images/knn-motivation.png" style="width: 100%;" /></a>
<div class="section" id="example">
<h3><span class="section-number">3.5.1. </span>Example:<a class="headerlink" href="#example" title="Permalink to this headline">#</a></h3>
<p>Suppose we are given many images and their labels.</p>
<p><code class="docutils literal notranslate"><span class="pre">X</span></code> = set of pictures</p>
<p><code class="docutils literal notranslate"><span class="pre">y</span></code> = names associated with those pictures.</p>
<p>Then we are given a new unseen test example, a picture in this particular case.</p>
<a class="reference internal image-reference" href="../_images/test_pic.png"><img alt="../_images/test_pic.png" src="../_images/test_pic.png" style="width: 5%;" /></a>
<p>We want to find out the label for this new test picture.</p>
<p>Naturally, we would try and find the most similar picture in our training set and using the label of the most similar picture as the label of this new test example.</p>
<p>That’s the basic idea behind similarity/analogy-based algorithms.</p>
<p>This is different from Decision trees, where we tried to learn a set of rules/conditions to label observations correctly.</p>
</div>
<div class="section" id="similarity-analogy-based-models-in-real-life">
<h3><span class="section-number">3.5.2. </span>Similarity/Analogy-based models in real life<a class="headerlink" href="#similarity-analogy-based-models-in-real-life" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a href="https://www.hertasecurity.com/en" target="_blank">Herta’s High-tech Facial Recognition</a></p></li>
</ul>
<a class="reference internal image-reference" href="../_images/face_rec.png"><img alt="404 image" src="../_images/face_rec.png" style="width: 20%;" /></a>
<ul class="simple">
<li><p>Recommendation systems</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/book_rec.png"><img alt="404 image" src="../_images/book_rec.png" style="width: 90%;" /></a>
</div>
</div>
<div class="section" id="terminology">
<h2><span class="section-number">3.6. </span>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">#</a></h2>
<p>In analogy-based algorithms, our goal is to come up with a way to find similarities between examples.
“similarity” is a bit ambiguous so we need some terminology.</p>
<ul class="simple">
<li><p>data: think of observations (rows) as points in a high dimensional space.</p></li>
<li><p>Each feature: Additional dimension.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/3d-table.png"><img alt="404 image" src="../_images/3d-table.png" style="width: 60%;" /></a>
<p>Above we have:</p>
<ul class="simple">
<li><p>Three features; speed attack and defense.</p></li>
<li><p>7 points in this three-dimensional space.</p></li>
</ul>
<p>Let’s go back to our Canada/USA cities dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">cities_train_df</span><span class="p">,</span> <span class="n">cities_test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">cities_train_df</span>
</pre></div>
</div>
</div>
</div>
<p>We have 2 features, so 2 dimensions (<code class="docutils literal notranslate"><span class="pre">longitude</span></code> and <code class="docutils literal notranslate"><span class="pre">latitude</span></code>)  and 167 points.
Visualizing this in 2 dimensions gives us the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_viz</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">cities_train_df</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;longitude:Q&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">140</span><span class="p">,</span> <span class="o">-</span><span class="mi">40</span><span class="p">])),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;latitude:Q&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">])),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;country:N&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Canada&#39;</span><span class="p">,</span> <span class="s1">&#39;USA&#39;</span><span class="p">],</span>
                                           <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">]))</span>
<span class="p">)</span>
<span class="n">cities_viz</span>
</pre></div>
</div>
</div>
</div>
<p>What about the housing training dataset we saw?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">house_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/kc_house_data.csv&quot;</span><span class="p">)</span>
<span class="n">house_df</span> <span class="o">=</span> <span class="n">house_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">house_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">house_df</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span>

<span class="n">house_X_train</span><span class="p">,</span> <span class="n">house_X_test</span><span class="p">,</span> <span class="n">house_y_train</span><span class="p">,</span> <span class="n">house_y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">house_X_train</span>
</pre></div>
</div>
</div>
</div>
<p>Notice a problem?!</p>
<p>We can only visualize data when the dimensions &lt;= 3.</p>
<p>BUT, in ML, we usually deal with high-dimensional problems where examples are hard to visualize.
High dimensional = 100s or 1000s of dimensions (roughly speaking, it depends on the specific problem at hands and the density of the data).</p>
<div class="section" id="feature-vectors">
<h3><span class="section-number">3.6.1. </span>Feature Vectors<a class="headerlink" href="#feature-vectors" title="Permalink to this headline">#</a></h3>
<p><strong>Feature vector</strong>: a vector composed of feature values associated with an example.</p>
<p>An example feature vector from the cities dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>An example feature vector from the housing dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">house_X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="distance">
<h2><span class="section-number">3.7. </span>Distance<a class="headerlink" href="#distance" title="Permalink to this headline">#</a></h2>
<p>We have our feature vectors, one for each observation, but how we calculate the similarity between these feature vectors?</p>
<p>One way to calculate the similarity between two points in high-dimensional space is by calculating the distance between them.</p>
<p>So, if the distance is higher, that means that the points are less similar and when the distance is smaller, that means that the points are more similar.</p>
<div class="section" id="euclidean-distance">
<h3><span class="section-number">3.7.1. </span>Euclidean distance<a class="headerlink" href="#euclidean-distance" title="Permalink to this headline">#</a></h3>
<p>There are different ways to calculate distance but we are going to focus on Euclidean distance.</p>
<p><strong>Euclidean distance:</strong> Euclidean distance is a measure of the true straight line distance between two points in Euclidean space. (<a class="reference external" href="https://hlab.stanford.edu/brian/euclidean_distance_in.html">source</a>)</p>
<p>The Euclidean distance between vectors</p>
<p><span class="math notranslate nohighlight">\(u = &lt;u_1, u_2, \dots, u_n&gt;\)</span> and</p>
<p><span class="math notranslate nohighlight">\(v = &lt;v_1, v_2, \dots, v_n&gt;\)</span> is defined as:</p>
<br>
<p><span class="math notranslate nohighlight">\(distance(u, v) = \sqrt{\sum_{i =1}^{n} (u_i - v_i)^2}\)</span></p>
<p>Because that equation can look a bit intimidating, let’s use it in an example, particularly our Canadian/US cities data.</p>
<div class="section" id="calculating-euclidean-distance-by-hand">
<h4><span class="section-number">3.7.1.1. </span>Calculating Euclidean distance “by hand”<a class="headerlink" href="#calculating-euclidean-distance-by-hand" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_train_df</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s take 2 points (two feature vectors) from the cities dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">two_cities</span> <span class="o">=</span> <span class="n">cities_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span>
<span class="n">two_cities</span>
</pre></div>
</div>
</div>
</div>
<p>The two sampled points are shown as black circles below.</p>
<p>Our goal is to find how similar these two points are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_viz</span> <span class="o">+</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">two_cities</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">130</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;latitude&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>First, we subtract these two cities. We are subtracting the city at index 0 from the city at index 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we square the differences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>Then we sum up the squared differences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And then take the square root of the value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># To the power of 0.5 is the same as square root</span>
<span class="p">((</span><span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">two_cities</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span>
</pre></div>
</div>
</div>
</div>
<p>We end with a value of 13.3898 which is the distance between the two cities in feature space.</p>
</div>
<div class="section" id="calculating-euclidean-distance-with-sklearn">
<h4><span class="section-number">3.7.1.2. </span>Calculating Euclidean distance  with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#calculating-euclidean-distance-with-sklearn" title="Permalink to this headline">#</a></h4>
<p>That’s more work than we really have time for and since <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> knows we are very busy people, they have a function that does this for us.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">two_cities</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When we call this function on our two cities data, it outputs this matrix with four values.</p>
<ul class="simple">
<li><p>Our first value is the distance between city 0 and itself.</p></li>
<li><p>Our second value is the distance between city 0 and city1.</p></li>
<li><p>Our third value is the distance between city 1and city 0.</p></li>
<li><p>Our fourth value is the distance between city 1 and itself.</p></li>
</ul>
<p>As we can see, the distances are symmetric. If we calculate the distance between city 0 and city1, it’s going to have the same value as if we calculated the distance between city 1 and city 0.</p>
<p>This isn’t always the case if we use a different metric to calculate distances.</p>
</div>
</div>
</div>
<div class="section" id="finding-the-nearest-neighbour">
<h2><span class="section-number">3.8. </span>Finding the Nearest Neighbour<a class="headerlink" href="#finding-the-nearest-neighbour" title="Permalink to this headline">#</a></h2>
<p>Now that we know how to calculate the distance between two points,
we can use this metric as a definition for what “most similar” means in our cities data.
In this case,
we would say that it is the cities that have the shortest distance between them.
These are often called the “Nearest Neighbors”.</p>
<p>Let’s find the closest cities to City 0 from our <code class="docutils literal notranslate"><span class="pre">cities_train_df</span></code> dataframe.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">euclidean_distances</span></code> on the entire dataset will calculate the distances from all the cities to all other cities in our dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">cities_train_df</span><span class="p">[[</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">]])</span>
<span class="n">dists</span>
</pre></div>
</div>
</div>
</div>
<p>This is going to be of shape 167 by 167 as this was the number of examples in our training portion.</p>
<p>Each row here gives us the distance of that particular city to all other cities in the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dists</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The distance of each city to itself is going to be zero.</p>
<p>If we don’t replace the 0’s on the diagonal with infinity, each city’s most similar city is going to be itself which is not useful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dists</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s look at the distance between city 0 and some other cities.</p>
<p>We can look at city 0 with its respective <code class="docutils literal notranslate"><span class="pre">longitude</span></code> and <code class="docutils literal notranslate"><span class="pre">latitude</span></code> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>And the distances from city 0 to the other cities in the training dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Remember that our goal is to find the closest example to city 0.</p>
<p>We can find the closest city to city 0 by finding the city with the minimum distance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The closest city in the training dataset is the city with index 157.</p>
<p>This corresponds to index 96 from the original dataset before shuffling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">157</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>If we look at the <code class="docutils literal notranslate"><span class="pre">longitude</span></code> and <code class="docutils literal notranslate"><span class="pre">latitude</span></code> values for the city at index 157 (labelled 96), they look pretty close to those of city 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">157</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>So, in this case, the closest city to city 0 is city 157 and the Euclidean distance between the two cities is 0.184.</p>
<div class="section" id="nearest-city-to-a-query-point">
<h3><span class="section-number">3.8.1. </span>Nearest city to a query point<a class="headerlink" href="#nearest-city-to-a-query-point" title="Permalink to this headline">#</a></h3>
<p>We can also find the distances to a new “test” or “query” city:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query_point</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="mi">25</span><span class="p">]]</span>

<span class="n">dists</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">cities_train_df</span><span class="p">[[</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="s2">&quot;latitude&quot;</span><span class="p">]],</span> <span class="n">query_point</span><span class="p">)</span>
<span class="n">dists</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># Only show the first few observations</span>
</pre></div>
</div>
</div>
</div>
<p>We can find the city closest to the query point (-80, 25) using:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dists</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dists</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>So the city at index 147 is the closest point to (-80, 25) with the Euclidean distance between the two equal to 3.838</p>
<p>Instead of doing this manually we can use Sklearn’s <code class="docutils literal notranslate"><span class="pre">NearestNeighbors</span></code> function to get the closest example and the distance between the query point and the closest example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cities_train_df</span><span class="p">[[</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="s1">&#39;latitude&#39;</span><span class="p">]]);</span>
<span class="n">nn</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">([[</span><span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="mi">25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">kneighbors</span></code> to find the 5 nearest cities in the training split to one of the cities in the test split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_test_X</span> <span class="o">=</span> <span class="n">cities_test_df</span><span class="p">[[</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="s1">&#39;latitude&#39;</span><span class="p">]]</span>

<span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cities_train_df</span><span class="p">[[</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="s1">&#39;latitude&#39;</span><span class="p">]]);</span>
<span class="n">nn</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">cities_test_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>On the last line,
we need to be careful and make sure that we pass in either a 2D numpy array
or a dataframe as our input.
This is why we use 2 sets of square brackets with our city above.</p>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">3.9. </span>Let’s Practice<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="n">seeds</span>   <span class="n">shape</span>  <span class="n">sweetness</span>   <span class="n">water</span><span class="o">-</span><span class="n">content</span>      <span class="n">weight</span>    <span class="n">fruit_veg</span>
<span class="mi">0</span>      <span class="mi">1</span>        <span class="mi">0</span>        <span class="mi">35</span>          <span class="mi">84</span>               <span class="mi">100</span>        <span class="n">fruit</span>
<span class="mi">1</span>      <span class="mi">0</span>        <span class="mi">0</span>        <span class="mi">23</span>          <span class="mi">75</span>               <span class="mi">120</span>        <span class="n">fruit</span>
<span class="mi">2</span>      <span class="mi">1</span>        <span class="mi">1</span>        <span class="mi">15</span>          <span class="mi">90</span>              <span class="mi">1360</span>         <span class="n">veg</span>
<span class="mi">3</span>      <span class="mi">1</span>        <span class="mi">1</span>         <span class="mi">7</span>          <span class="mi">96</span>               <span class="mi">600</span>         <span class="n">veg</span>
<span class="mi">4</span>      <span class="mi">0</span>        <span class="mi">0</span>        <span class="mi">37</span>          <span class="mi">80</span>                 <span class="mi">5</span>        <span class="n">fruit</span>
<span class="mi">5</span>      <span class="mi">0</span>        <span class="mi">0</span>        <span class="mi">45</span>          <span class="mi">78</span>                <span class="mi">40</span>        <span class="n">fruit</span>  
<span class="mi">6</span>      <span class="mi">1</span>        <span class="mi">0</span>        <span class="mi">27</span>          <span class="mi">83</span>               <span class="mi">450</span>         <span class="n">veg</span>
<span class="mi">7</span>      <span class="mi">1</span>        <span class="mi">1</span>        <span class="mi">18</span>          <span class="mi">73</span>                 <span class="mi">5</span>         <span class="n">veg</span>
<span class="mi">8</span>      <span class="mi">1</span>        <span class="mi">1</span>        <span class="mi">32</span>          <span class="mi">80</span>                <span class="mi">76</span>         <span class="n">veg</span>
<span class="mi">9</span>      <span class="mi">0</span>        <span class="mi">0</span>        <span class="mi">40</span>          <span class="mi">83</span>                <span class="mi">65</span>        <span class="n">fruit</span>
</pre></div>
</div>
<p>1. Giving the table above and that we are trying to predict if each example is either a fruit or a vegetable, what would be the dimension of feature vectors in this problem?</p>
<p>2. Which of the following would be the feature vector for example 0.</p>
<p>a) <code class="docutils literal notranslate"><span class="pre">array([1,</span>&#160; <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0])</span></code><br />
b) <code class="docutils literal notranslate"><span class="pre">array([fruit,</span>&#160; <span class="pre">fruit,</span> <span class="pre">veg,</span> <span class="pre">veg,</span> <span class="pre">fruit,</span> <span class="pre">fruit,</span> <span class="pre">veg,</span> <span class="pre">veg,</span> <span class="pre">veg,</span> <span class="pre">fruit])</span></code><br />
c) <code class="docutils literal notranslate"><span class="pre">array([1,</span> <span class="pre">0,</span> <span class="pre">35,</span> <span class="pre">84,</span> <span class="pre">100])</span></code><br />
d) <code class="docutils literal notranslate"><span class="pre">array([1,</span> <span class="pre">0,</span> <span class="pre">35,</span> <span class="pre">84,</span> <span class="pre">100,</span>&#160; <span class="pre">fruit])</span></code></p>
<p>3. Given the following 2 feature vectors, what is the Euclidean distance between the following two feature vectors?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="o">-</span><span class="mi">11</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>True or False</strong></p>
<p>4. Analogy-based models find examples from the test set that are most similar to the test example we are predicting.<br />
5. Feature vectors can only be of length 3 since we cannot visualize past that.<br />
6. Euclidean distance will always have a positive value.<br />
7. When finding the nearest neighbour in a dataset using <code class="docutils literal notranslate"><span class="pre">kneighbors()</span></code> from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library, we must <code class="docutils literal notranslate"><span class="pre">fit</span></code>  the data first.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solutions!</p>
<ol class="simple">
<li><p>5 dimensions.</p></li>
<li><p>c) <code class="docutils literal notranslate"><span class="pre">array([1,</span> <span class="pre">0,</span> <span class="pre">35,</span> <span class="pre">84,</span> <span class="pre">100])</span></code></p></li>
<li><p>7</p></li>
<li><p>False</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
<li><p>True</p></li>
</ol>
</div>
</div>
<div class="section" id="k-nearest-neighbours-k-nns-classifier">
<h2><span class="section-number">3.10. </span><span class="math notranslate nohighlight">\(k\)</span> -Nearest Neighbours (<span class="math notranslate nohighlight">\(k\)</span>-NNs) Classifier<a class="headerlink" href="#k-nearest-neighbours-k-nns-classifier" title="Permalink to this headline">#</a></h2>
<p>Now that we have learned how to find similar examples, can we use this idea in a predictive model?</p>
<ul class="simple">
<li><p>Yes! The k Nearest Neighbors (kNN) algorithm</p></li>
<li><p>This is a fairly simple algorithm that is best understood by example</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/scatter.png"><img alt="404 image" src="../_images/scatter.png" style="width: 30%;" /></a>
<p>We have two features in our toy example; feature 1 and feature 2.</p>
<p>We have two targets; 0 represented with  <font color="blue">blue</font> points and 1 represented with  <font color="orange">orange</font> points.</p>
<p>We want to predict the point in gray.</p>
<p>Based on what we have been doing so far, we can find the closest example (<span class="math notranslate nohighlight">\(k\)</span>=1) to this gray point and use its class as the class for our grey point.</p>
<p>In this particular case, we will predict orange as the class for our query point.</p>
<a class="reference internal image-reference" href="../_images/scatter_k1.png"><img alt="404 image" src="../_images/scatter_k1.png" style="width: 30%;" /></a>
<p>What if we consider more than one nearest example and let them vote on the target of the query example.</p>
<p>Let’s consider the nearest 3 neighbours and let them vote.</p>
<a class="reference internal image-reference" href="../_images/scatter_k3.png"><img alt="404 image" src="../_images/scatter_k3.png" style="width: 30%;" /></a>
<p>Let’s try this with a smaller set of our data and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_train_df</span> <span class="o">=</span> <span class="n">cities_train_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1223</span><span class="p">)</span>
<span class="n">small_X_train</span> <span class="o">=</span> <span class="n">small_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span>
<span class="n">small_y_train</span> <span class="o">=</span> <span class="n">small_train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">one_city</span> <span class="o">=</span> <span class="n">cities_test_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
<span class="n">one_city</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chart_knn</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">small_train_df</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">140</span><span class="p">,</span> <span class="o">-</span><span class="mi">40</span><span class="p">])),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">])),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Canada&#39;</span><span class="p">,</span> <span class="s1">&#39;USA&#39;</span><span class="p">],</span> <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])))</span>

<span class="n">one_city_point</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">one_city</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">(</span>
    <span class="n">shape</span><span class="o">=</span><span class="s1">&#39;triangle-up&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;darkgreen&#39;</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;latitude&#39;</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">chart_knn</span> <span class="o">+</span>  <span class="n">one_city_point</span>
</pre></div>
</div>
</div>
</div>
<p>We want to find the class for this green triangle city.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neigh_clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">neigh_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">small_X_train</span><span class="p">,</span> <span class="n">small_y_train</span><span class="p">)</span>
<span class="n">neigh_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">one_city</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>We can set <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> equal to 1 to classify this triangle based on one neighbouring point.</p>
<p>Our prediction here is Canada since the closest point to the green triangle is a city with the class “Canada”.</p>
<p>Now, what if we consider the nearest 3 neighbours?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neigh_clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">neigh_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">small_X_train</span><span class="p">,</span> <span class="n">small_y_train</span><span class="p">)</span>
<span class="n">neigh_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">one_city</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>When we change our model to consider the nearest 3 neighbours, our prediction changes!</p>
<p>It now predicts “USA” since the majority of the 3 nearest points are “USA” cities.</p>
<p>Let’s use our entire training dataset and calculate our training and validation scores</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_X_train</span> <span class="o">=</span> <span class="n">cities_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;country&#39;</span><span class="p">])</span>
<span class="n">cities_y_train</span> <span class="o">=</span> <span class="n">cities_train_df</span><span class="p">[</span><span class="s1">&#39;country&#39;</span><span class="p">]</span>
<span class="n">cities_X_test</span> <span class="o">=</span> <span class="n">cities_test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;country&#39;</span><span class="p">])</span>
<span class="n">cities_y_test</span> <span class="o">=</span> <span class="n">cities_test_df</span><span class="p">[</span><span class="s1">&#39;country&#39;</span><span class="p">]</span>


<span class="n">kn1_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">kn1_model</span><span class="p">,</span> <span class="n">cities_X_train</span><span class="p">,</span> <span class="n">cities_y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">scores_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_df</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="choosing-k">
<h2><span class="section-number">3.11. </span>Choosing K<a class="headerlink" href="#choosing-k" title="Permalink to this headline">#</a></h2>
<p>Ok, so we saw our validation and training scores for <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> =1. What happens when we change that?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kn90_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

<span class="n">scores_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">kn90_model</span><span class="p">,</span> <span class="n">cities_X_train</span><span class="p">,</span> <span class="n">cities_y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">scores_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_df</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Comparing this with the results of <code class="docutils literal notranslate"><span class="pre">n_neighbors=1</span></code> we see that we went from overfitting to underfitting.</p>
<p>Let’s look at the decision boundaries now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_knn_decision_boundaries</span><span class="p">(</span><span class="n">cities_X_train</span><span class="p">,</span> <span class="n">cities_y_train</span><span class="p">,</span> <span class="n">k_values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If we plot these two models with <span class="math notranslate nohighlight">\(k=1\)</span> on the left and <span class="math notranslate nohighlight">\(k=90\)</span> on the right.</p>
<p>The left plot shows a much more complex model where it is much more specific and attempts to get every example correct.</p>
<p>The plot on right is plotting a simpler model and we can see more training examples are being predicted incorrectly.</p>
<div class="section" id="how-to-choose-k-n-neighbors">
<h3><span class="section-number">3.11.1. </span>How to choose <span class="math notranslate nohighlight">\(K\)</span> (<code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>)?<a class="headerlink" href="#how-to-choose-k-n-neighbors" title="Permalink to this headline">#</a></h3>
<p>So we saw the model was overfitting with <span class="math notranslate nohighlight">\(k\)</span>=1 and  when <span class="math notranslate nohighlight">\(k\)</span>=90, the model was underfitting.</p>
<p>So, the question is how do we pick <span class="math notranslate nohighlight">\(k\)</span>?</p>
<ul class="simple">
<li><p>Since <span class="math notranslate nohighlight">\(k\)</span> is a hyperparameter (<code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>), we can use hyperparameter optimization to choose <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ul>
<p>Here we are looping over different values of <span class="math notranslate nohighlight">\(k\)</span> and performing cross-validation on each one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;n_neighbors&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span> <span class="s2">&quot;mean_train_score&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span> <span class="s2">&quot;mean_cv_score&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">()}</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">cities_X_train</span><span class="p">,</span> <span class="n">cities_y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">results_dict</span><span class="p">[</span><span class="s2">&quot;n_neighbors&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">results_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]))</span>
    <span class="n">results_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]))</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_dict</span><span class="p">)</span>
<span class="n">results_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotting_source</span> <span class="o">=</span> <span class="n">results_df</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">id_vars</span><span class="o">=</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">,</span>
    <span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_train_score&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_cv_score&#39;</span><span class="p">],</span>
    <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;score_type&#39;</span><span class="p">,</span>
    <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span>
<span class="p">)</span>

<span class="n">K_plot</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">plotting_source</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;n_neighbors:Q&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;accuracy:Q&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="mf">.67</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">])),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;score_type:N&#39;</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Accuracies of n_neighbors for KNeighborsClassifier&quot;</span><span class="p">)</span>

<span class="n">K_plot</span>
</pre></div>
</div>
</div>
</div>
<p>Looking at this graph with k on the x-axis and accuracy on the y-axis, we can see there is a sweet spot where the gap between the validation and training scores is the lowest and cross-validation score is the highest. Here it’s when <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> is 11.</p>
<p>How do I know it’s 11?
Here’s how!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;mean_cv_score&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cities_X_train</span><span class="p">,</span> <span class="n">cities_y_train</span><span class="p">);</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">cities_X_test</span><span class="p">,</span> <span class="n">cities_y_test</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>This testing accuracy even higher than the validation mean accuracy we had earlier.</p>
</div>
</div>
<div class="section" id="curse-of-dimensionality">
<h2><span class="section-number">3.12. </span>Curse of Dimensionality<a class="headerlink" href="#curse-of-dimensionality" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(k\)</span> -NN usually works well when the number of dimensions is small.</p>
</div></blockquote>
<p>In the previous module, we discussed one of the most important problems in machine learning which was overfitting.
The second most important problem in machine learning is <strong>the curse of dimensionality</strong>.
This refers to that datasets with many many features generally have a sparser distribution of observations within this feature space,
since there are so many possible unique combinations of features.
This makes it both harder to detect the same general patterns between observations in the data
and easier to accidentally mistake “noise” (meaningless correlations) for general patterns.</p>
<p>We will talk more later about what can be done to alleviate the curse of dimensionality,
but for now it is enough to know that many supervised ML algorithms, including k-NN,
can perform poorly when there are too high dimensionality in the data.
With enough irrelevant features, the accidental similarity between features wipes out any meaningful similarity and <span class="math notranslate nohighlight">\(k\)</span>-NN is no better than random guessing.</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">3.13. </span>Let’s Practice<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>Consider this toy dataset:</p>
<div class="math notranslate nohighlight">
\[\begin{split} X = \begin{bmatrix}5 &amp; 2\\4 &amp; 3\\  2 &amp; 2\\ 10 &amp; 10\\ 9 &amp; -1\\ 9&amp; 9\end{bmatrix}, \quad y = \begin{bmatrix}A\\A\\B\\B\\B\\C\end{bmatrix}.\end{split}\]</div>
<p>What would you predict for <span class="math notranslate nohighlight">\(x=\begin{bmatrix} 0 &amp; 0\end{bmatrix}\)</span>:</p>
<p>1. If <span class="math notranslate nohighlight">\(k=1\)</span>?<br />
2. If <span class="math notranslate nohighlight">\(k=3\)</span>?</p>
<p><strong>True or False</strong></p>
<p>3. The classification of the closest neighbour to the test example always contributes the most to the prediction.<br />
4. The <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> hyperparameter must be less than the number of examples in the training set.<br />
5. Similar to decision trees, <span class="math notranslate nohighlight">\(k\)</span>-NNs find a small set of good features.<br />
6. With  <span class="math notranslate nohighlight">\(k\)</span>-NN, setting the hyperparameter  <span class="math notranslate nohighlight">\(k\)</span>  to larger values typically increases training score.<br />
7. <span class="math notranslate nohighlight">\(k\)</span>-NN may perform poorly in high-dimensional space (say, d &gt; 100)<br />
8. Based on the graph below, what value of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> would you choose to train your model on?</p>
<a class="reference internal image-reference" href="../_images/Q18a.png"><img alt="404 image" src="../_images/Q18a.png" style="width: 70%;" /></a>
<div class="dropdown admonition">
<p class="admonition-title">Solutions!</p>
<ol class="simple">
<li><p>B</p></li>
<li><p>A</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
<li><p>False</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
<li><p>12</p></li>
</ol>
</div>
</div>
<div class="section" id="what-we-ve-learned-today-a-id-9-a">
<h2><span class="section-number">3.14. </span>What We’ve Learned Today<a id="9"></a><a class="headerlink" href="#what-we-ve-learned-today-a-id-9-a" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>The concept of baseline models.</p></li>
<li><p>How to initiate a Dummy Classifier and Regressor.</p></li>
<li><p>How to measure Euclidean distance.</p></li>
<li><p>How the <span class="math notranslate nohighlight">\(k\)</span>NN algorithm works for classification.</p></li>
<li><p>How changing <span class="math notranslate nohighlight">\(k\)</span> (<code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>) affects a model.</p></li>
<li><p>What the curse of dimensionality is.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="lecture2.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Splitting and Cross-validation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture4.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>kNN regression, Support Vector Machines, and Feature Preprocessing</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Quan Nguyen<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>