{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Class meetings 01, 02, and 03. \n",
    "\n",
    "__Due__: Wednesday, March 7 at 10:00am (i.e., the start of Class Meeting 04).\n",
    "\n",
    "__Attribution__: Many of these exercises are adapted from Michael Gelbart's DSCI 571 exercises and Mark Schmidt's CPSC 340 exercises.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- You must use proper spelling and grammar.\n",
    "- Add your responses to the questions in this very jupyter notebook. \n",
    "- Submit your assignment through [UBC Connect](https://connect.ubc.ca/) by the deadline, being sure to upload your jupyter notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals\n",
    "\n",
    "\n",
    "Here we will try classification of the famous handwritten digits data set. \n",
    "\n",
    "This data set exists in many forms; we will use the one bundled in `sklearn.datasets`. We will also use `sklearn` for classification.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the documentation for the data by running `print(digits['DESCR'])`. We'll extract the features and labels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits['data'] # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "Y = digits['target'] # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1148a70b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a12fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title('This is a %d' % digits['target'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to element 0 (`X[0]`) -- element 1 (`X[1]`) or element 2 (`X[2]`)? Report the two distances (Euclidean).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error\n",
    "\n",
    "You'll be using the scikit-learn implementation of the $k$-NN classifier. Documentation is available at http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Using `k=10`, fit a $k$-NN classifier using `X` and `Y` using all of the data as your training data. Obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error\n",
    "\n",
    "Now fit the classifier using `k=1`, using all of your data as training data, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error\n",
    "\n",
    "This is a continuation of Exercise 1. Each part asks you to investigate some scenario.\n",
    "\n",
    "__Note__: For this specific data set, you might not be able to overfit with $k$-NN! So don't worry if you can't find an example of overfitting.\n",
    "\n",
    "### 2(a) The influence of k\n",
    "\n",
    "Now, split the data into _training_ and _test_ sets. You can choose any reasonable fraction for training vs. testing (50% will do). \n",
    "\n",
    "__Note__: It's always a good idea to randomly shuffle the data before splitting, in case the data comes ordered in some way. (For example, if they are ordered by label, then your training set will be all the digits 0-4, and your test set all the digits 5-9, which would be bad... you might end up with 100% error!!) To shuffle your data, you can use [`numpy.random.shuffle`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html).\n",
    "\n",
    "For various values of $k$, fit (a.k.a. _train_) a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? Make a plot to show the trends, and briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Fundamental Tradeoff\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Recall the two parts of the fundamental trade-off in machine learning:\n",
    "\n",
    "1. How small we can make the training error.\n",
    "2. How well the training error approximates the test error.\n",
    "\n",
    "In a $k$-nearest neighbour classifier, how does the parameter $k$ affect each of the two parts of the trade-off? What is one way that we could choose $k$ in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c) The influence of data partition\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Now, choose your favourite value of $k$, but vary the proportion of data reserved for the training set, again obtaining training and test error rates for each partition of the data. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(d) Imbalanced training data\n",
    "\n",
    "rubric={reasoning:5, code:5}\n",
    "\n",
    "Now, take a subset of your training data so that you only retain the first 2% of the examples of digits 0-8, but keep 100% of the 9's. This is called an imbalanced training set. \n",
    "\n",
    "What is your training and test error, vs. $k$? How does it compare to your performance with all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2(e) Imbalanced training data -- fair (Optional)\n",
    "\n",
    "What we did in (d) above was an unfair comparison, because reducing the amount of training data will generally hurt performance. To compare, randomly remove examples from the original training set such that the number of examples is the same as in part (d) above. Now compare the training and test error to the results from (d). Briefly comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(f) The influence of added noise (Optional; no marks)\n",
    "\n",
    "__This question is worth no marks.__\n",
    "\n",
    "Now, add noise to the training data: for each example, with probability 20% replace the training label with a label selected uniformly at random. Remake the plot vs. $k$. What do you observe? Is there a generalizable insight here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 Loess\n",
    "\n",
    "We'll use the Titanic data set to try and predict survival of passengers (`Survival`) from `Age`, `Fare`, and `Sex`, using loess. You might find it useful to log-transform `Fare`. The data have been split into a training and test set in the files `titanic_train.csv` and `titanic_test.csv` in the `data` folder. Details of the data can be found at https://www.kaggle.com/c/titanic/data.\n",
    "\n",
    "Note: To include `Sex` in your model, simply fit a loess model to each of `\"male\"` and `\"female\"`. \n",
    "\n",
    "Here are ways to implement loess in python and R:\n",
    "\n",
    "- [sklearn.neighbors.RadiusNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) in python.\n",
    "- [statsmodels.nonparametric.kernel_regression.KernelReg](http://www.statsmodels.org/stable/generated/statsmodels.nonparametric.kernel_regression.KernelReg.html) in python.\n",
    "- [`loess`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/loess.html) function in R.\n",
    "- [`ggplot2::geom_smooth`](http://ggplot2.tidyverse.org/reference/geom_smooth.html) for loess (and related methods) in R's `ggplot2` plotting package.\n",
    "\n",
    "### 3(a) Scaling\n",
    "\n",
    "Estimate the standard deviations of both (numeric) predictors. Is scaling your data justified? Does your decision also apply to $k$-NN, or is scaling only relevant for loess? If scaling is justified, proceed with scaling by subtracting the mean, then dividing by standard deviation (for each numeric predictor). \n",
    "\n",
    "__Hint:__ Be sure to do the same thing with the test set! Just make sure that the mean and standard deviation you use to do the scaling are of the _training_ set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Missing Prediction\n",
    "\n",
    "It's possible that loess won't predict anything for a certain observation on the test set. In what situation will this happen, and why? Could this also be the case for $k$-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Regression\n",
    "\n",
    "Fit a loess model to the training data for various values of the bandwidth parameter. Plot the mean squared error (MSE) on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d) Classification\n",
    "\n",
    "Like you just did, fit a loess model to the training data for various values of the bandwidth parameter, but then add a classification step: predict survival if the probability of survival is greater than 0.5. Plot the error rate on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate? Do you get similar results when you considered the MSE in the regression case above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(e) $k$-NN\n",
    "\n",
    "Try the above classification exercise, but using $k$-NN. Plot the error rate on the training and test sets, and plot these across $k$. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what $k$ is appropriate? How does the error compare with loess? Which would you choose, loess or $k$-NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
