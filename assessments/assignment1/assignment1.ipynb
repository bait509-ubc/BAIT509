{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Class meetings 01-04. \n",
    "\n",
    "__Due__: Wednesday, March 12 at 10:00am (i.e., the start of Class Meeting 05).\n",
    "\n",
    "__Rubrics__: The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [code rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_code.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "## Instructions (5%)\n",
    "\n",
    "- You must use proper spelling and grammar.\n",
    "- Use either R or python to complete this assignment (or both). \n",
    "- Submit your assignment through [UBC Connect](https://connect.ubc.ca/) by the deadline. \n",
    "- If you submit more than one file for your assignment, be sure to also include a README file to inform the grader of how to navigate your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals (20%)\n",
    "\n",
    "\n",
    "Here we will try classification of the famous handwritten digits data set. \n",
    "\n",
    "This data set exists in many forms; we will use the one bundled in `sklearn.datasets`. We will also use `sklearn` for classification.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the documentation for the data by running `print(digits['DESCR'])`. We'll extract the features and labels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits['data'] # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "Y = digits['target'] # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11a699240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncKAwAg4URsCBwgg4UNj/Aqs7hTZq86pjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a685748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title('This is a %d' % digits['target'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to element 0 (`X[0]`) -- element 1 (`X[1]`) or element 2 (`X[2]`)? Report the two distances (Euclidean).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error\n",
    "\n",
    "You'll be using the scikit-learn implementation of the $k$-NN classifier. Documentation is available at http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Using `k=10`, fit a $k$-NN classifier using `X` and `Y` using all of the data as your training data. Obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error\n",
    "\n",
    "Now fit the classifier using `k=1`, using all of your data as training data, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error (15%)\n",
    "\n",
    "This is a continuation of Exercise 1. Each part asks you to investigate some scenario.\n",
    "\n",
    "__Note__: For this specific data set, you might not be able to overfit with $k$-NN! So don't worry if you can't find an example of overfitting.\n",
    "\n",
    "__Attribution__: This exercise was adapted from DSCI 571.\n",
    "\n",
    "### 2(a) The influence of k\n",
    "\n",
    "Now, split the data into _training_ and _test_ sets. You can choose any reasonable fraction for training vs. testing (50% will do). \n",
    "\n",
    "__Note__: It's always a good idea to randomly shuffle the data before splitting, in case the data comes ordered in some way. (For example, if they are ordered by label, then your training set will be all the digits 0-4, and your test set all the digits 5-9, which would be bad... you might end up with 100% error!!) To shuffle your data, you can use [`numpy.random.shuffle`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html).\n",
    "\n",
    "For various values of $k$, fit (a.k.a. _train_) a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? Make a plot to show the trends, and briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) The influence of data partition\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Now, choose your favourite value of $k$, but vary the proportion of data reserved for the training set, again obtaining training and test error rates for each partition of the data. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Loess (50%)\n",
    "\n",
    "We'll use the Titanic data set to try and predict survival of passengers (`Survival`) from `Age`, `Fare`, and `Sex`, using loess. You might find it useful to log-transform `Fare`. The data have been split into a training and test set in the files `titanic_train.csv` and `titanic_test.csv` in the `data` folder. Details of the data can be found at https://www.kaggle.com/c/titanic/data.\n",
    "\n",
    "Note: To include `Sex` in your model, simply fit a loess model to each of `\"male\"` and `\"female\"`. \n",
    "\n",
    "Here are ways to implement loess in python and R:\n",
    "\n",
    "- [sklearn.neighbors.RadiusNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) in python.\n",
    "- [statsmodels.nonparametric.kernel_regression.KernelReg](http://www.statsmodels.org/stable/generated/statsmodels.nonparametric.kernel_regression.KernelReg.html) in python.\n",
    "- [`loess`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/loess.html) function in R.\n",
    "- [`ggplot2::geom_smooth`](http://ggplot2.tidyverse.org/reference/geom_smooth.html) for loess (and related methods) in R's `ggplot2` plotting package.\n",
    "\n",
    "### 3(a) Scaling\n",
    "\n",
    "Estimate the standard deviations of both (numeric) predictors. Is scaling your data justified? Does your decision also apply to $k$-NN, or is scaling only relevant for loess? If scaling is justified, proceed with scaling by subtracting the mean, then dividing by standard deviation (for each numeric predictor). \n",
    "\n",
    "__Hint:__ Be sure to do the same thing with the test set! Just make sure that the mean and standard deviation you use to do the scaling are of the _training_ set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Regression\n",
    "\n",
    "Fit a loess model to the training data for various values of the bandwidth parameter. Plot the mean squared error (MSE) on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Classification\n",
    "\n",
    "Like you just did, fit a loess model to the training data for various values of the bandwidth parameter, but then add a classification step: predict survival if the probability of survival is greater than 0.5. Plot the error rate on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate? Do you get similar results when you considered the MSE in the regression case above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d) $k$-NN\n",
    "\n",
    "Try the above classification exercise, but using $k$-NN. Plot the error rate on the training and test sets, and plot these across $k$. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what $k$ is appropriate? How does the error compare with loess? Which would you choose, loess or $k$-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Concepts (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a) Missing Prediction\n",
    "\n",
    "It's possible that loess won't predict anything for a certain observation on the test set. In what situation will this happen, and why? Could this also be the case for $k$-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4(b) Fundamental tradeoff\n",
    "\n",
    "How do the bandwidth and $k$ hyperparameters in loess and $k$-NN influence the bias/variance tradeoff? Use two brief sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
