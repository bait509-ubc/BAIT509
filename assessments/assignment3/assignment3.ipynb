{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 3\n",
    "\n",
    "__Evaluates__: Class meetings 07, 08, and 09.\n",
    "\n",
    "__Due__: Wednesday, March 28 at 10:00am (i.e., the start of Class Meeting 10).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "You must use proper spelling and grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Regression Beyond the Mean\n",
    "\n",
    "In this exercise, we'll use the Bow River flow data. We've included it in the `data` folder of this assignment. Your task is to investigate the river flow from the day of the year (use `lubridate::yday` to get the day of year from a date).\n",
    "\n",
    "For this exercise, you are free to use R.\n",
    "\n",
    "### 1a Probabilistic Forecasting\n",
    "\n",
    "Produce a probabilistic forecast of river flow on a day during \"peak season\" (i.e., when flow is large -- just eyeball it), and another on a day during the \"low season\". The estimates should take the form of a density function (plotted visually).\n",
    "\n",
    "To produce a predictive distribution on any given day (say day `d`):\n",
    "\n",
    "1. Subset the data that occur on day `d`.\n",
    "2. Obtain the univariate sample of flow data from this subset.\n",
    "3. Use ggplot2's `geom_density` on the subsetted univariate flow data. \n",
    "\n",
    "__Note__: this is the idea behind local methods in their simplest forms. \n",
    "\n",
    "Now, imagine you're interested in setting up a campground along the Bow River at this gauge location (the town of Banff is currently located there, but imagine it isn't). You've also learned that flows above 200 m^3/s typically result in a flood, with severity growing with larger flows. If you were to set up camp there, what does your predictive distribution tell you about the flooding situation you'd have to deal with year-in and year-out? Would you say this is a risky move?\n",
    "\n",
    "### 1b Local Quantile Regression\n",
    "\n",
    "Estimate the 0.1-quantile, mean, and 0.9-quantile on every day of the year, using the method mentioned in 4a. Specifically, to estimate a quantity on day `d`:\n",
    "\n",
    "1. Subset the data that occur on day `d`.\n",
    "2. Obtain the univariate sample of flow data from this subset.\n",
    "3. Estimate the mean or quantile on this univariate sample. For quantile estimation, feel free to use the `quantile` function in R with any `type` argument. \n",
    "\n",
    "Hint: `dplyr` makes this easy -- just `group_by` the day of year, and calculate the estimates in the `summarize` function.\n",
    "\n",
    "\n",
    "Plot your estimates of the 0.1-quantile, mean, and 0.9-quantile against the day of year, using `geom_line` to connect the dots for each of the three quantities. This will form your \"regression curves\": the middle curve being the mean regression curve, and the outer ones being quantile regression curves. Include the original data in this plot, too, but underneath your regression curves, and with quite a bit of alpha transparency. \n",
    "\n",
    "What you have is mean regression, with an 80% prediction interval given by the lower and upper curves.  \n",
    "\n",
    "In 4a, you chose a day of the year corresponding to \"peak season\". What is your estimated 80% prediction interval of the river flow on this day? In addition, provide an interpretation of the 0.9-quantile. \n",
    "\n",
    "### 1c Crossing Quantile Curves\n",
    "\n",
    "Using the local quantile estimation method in 4b, is it possible for quantile curves to \"cross\"? Why or why not?\n",
    "\n",
    "### 1d Reduce Overfitting\n",
    "\n",
    "The regression curves in 4b are overfit -- the curves are too wiggly, suggesting that there are patterns present in the data that aren't actually there. Re-make the plot of the regression curves in 4a, this time using `ggplot2`'s built-in local estimation capabilities. Choose smoothing parameters that look to give a fit that's \"just right\".\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Don't worry about perfecting the fit here. If we really wanted to, we could choose it to optimize the model's score.\n",
    "- Use `geom_smooth` for the mean regression curve, controlling the fit with the `span` parameter.\n",
    "- Use `geom_quantile(method=\"rqss\")` to fit a local quantile regression estimate, controlling the smoothing parameter with the `lambda` argument. Larger values of lambda correspond to wider windows; perhaps start your search with `lambda=10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Diving Deeper\n",
    "\n",
    "Choose one of the methods introduced in Class Meeting 09 (SVM, Naive Bayes, ...). We only covered the basic idea behind each method, with the aim of giving you exposure to a variety of ideas. In this exercise, you'll dive a little deeper into a method of your choice. In particular, answer the following questions:\n",
    "\n",
    "- Briefly research one way we can extend the method, that was not covered in class. What is the main idea behind the learning method? Briefly describe this (no more than a short paragraph). \n",
    "- What parameters/choices in the estimation control the bias-variance tradeoff, and how?\n",
    "- When is this method generally useful, and in what situation might the extension you researched be useful?\n",
    "- Apply this method, _with your researched extension_, to a dataset introduced in one of your assignments from this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6a: Naive Bayes and Laplace smoothing\n",
    "rubric={reasoning:3}\n",
    "\n",
    "The code below loads the newsgroups data from Lab 1 and runs Naive Bayes on it.\n",
    "\n",
    "1. Explain why the conditional independence assumption is needed to make Naive Bayes a practical algorithm.\n",
    "2. Discuss the `alpha` parameter of the sklearn classifier. What is it good for?\n",
    "3. Try running the code with `alpha=0`. What happens? Why?\n",
    "4. Try running the code with `alpha=100`. What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pickle \n",
    "\n",
    "with open('newsgroups.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "Xtrain = data['X'].toarray() # use the dense version for simplicity \n",
    "ytrain = data['y']\n",
    "Xtest = data['Xtest'].toarray()\n",
    "ytest = data['ytest']\n",
    "classnames = data['groupnames']\n",
    "wordnames = data['wordlist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error = 0.201207\n",
      "Test error = 0.187415\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB(alpha=1)\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "pred_train = model.predict(Xtrain)\n",
    "pred_test = model.predict(Xtest)\n",
    "training_error = np.sum(pred_train != ytrain)/len(ytrain)\n",
    "test_error = np.sum(pred_test != ytest)/len(ytest)\n",
    "print('Training error = %f' % training_error)\n",
    "print('Test error = %f' % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 6b: Naive Bayes implementation\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Below is an implementation of a naive Bayes classifier. While the `predict` function of the naive Bayes classifier is already implemented, the calculation of the variable $p\\_xy$ is incorrect (right now, it just sets all values to $1/2$). \n",
    "\n",
    "1. Modify this function so that $p\\_xy$ correctly computes the conditional probability of these values based on the frequencies in the data set. \n",
    "3. Compare your implmentation with [sklearn's implementation](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Predicting...\n",
      "Training error = 0.666297\n",
      "Test error = 0.661249\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayes(): # only works when classes are 0,1,...,C-1\n",
    "\n",
    "    def __init__(self, classnames, inputnames):\n",
    "        self.classnames = classnames\n",
    "        self.inputnames = inputnames\n",
    "        self.C = len(classnames)\n",
    "        \n",
    "    # this implementation is incorrect (see comments below)\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        C = self.C\n",
    "\n",
    "        # Compute the probability of each class i.e p(y==c)\n",
    "        counts = np.bincount(y)\n",
    "        p_y = counts / N\n",
    "\n",
    "        # Compute the conditional probabilities i.e.\n",
    "        # p(x(i,j)=1 | y(i)==c) as p_xy\n",
    "        # p(x(i,j)=0 | y(i)==c) as p_xy\n",
    "        p_xy = np.zeros((D, C, 2)) # the 2 is because we have binary features, so x(i,j) can only take on 2 values\n",
    "        for label in range(C): # loop through classes\n",
    "            for feature in range(D): # loop through features\n",
    "                # these two lines below are not right. replace them with the correct code.\n",
    "                # it's not a lot of code; just a few lines.\n",
    "                p_xy[feature, label, 0] = 0.5\n",
    "                p_xy[feature, label, 1] = 0.5\n",
    "\n",
    "\n",
    "        # Save parameters in model as dict\n",
    "        \n",
    "        self.p_y = p_y\n",
    "        self.p_xy = p_xy\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        C = self.C\n",
    "        p_xy = self.p_xy\n",
    "        p_y = self.p_y\n",
    "\n",
    "        y_pred = np.zeros(N)\n",
    "\n",
    "        for n in range(N):\n",
    "            \n",
    "            probs = p_y.copy()\n",
    "            for d in range(D):\n",
    "                probs *= p_xy[d, :, X[n, d]]\n",
    "\n",
    "            y_pred[n] = np.argmax(probs)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "model = NaiveBayes(classnames=classnames, inputnames=wordnames)\n",
    "print('Fitting...')\n",
    "model.fit(X, y)\n",
    "print('Predicting...')\n",
    "pred_train = model.predict(X)\n",
    "pred_test = model.predict(Xtest)\n",
    "training_error = np.sum(pred_train != y)/len(y)\n",
    "test_error = np.sum(pred_test != ytest)/len(ytest)\n",
    "print('Training error = %f' % training_error)\n",
    "print('Test error = %f' % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7a: quantitative comparison of classifiers\n",
    "rubric={code:2,reasoning:3}\n",
    "\n",
    "Here, we will compare the training and validation errors of the classifiers covered in this course:\n",
    "  * $k$-nearest neighbours\n",
    "  * decision trees\n",
    "  * random forests\n",
    "  * SVM\n",
    "  * naive Bayes\n",
    "  \n",
    "For each classifier, use the scikit-learn implementation with default hyperparameters. \n",
    "\n",
    "Perform the comparison on the following 3 data sets:\n",
    "  * handwritten digits (used in lab 1 and 2)\n",
    "  * newsgroups (used in lab 1 and 2)\n",
    "  * one other classification data set of your choice (for example, something you used in a previous course, something you found online, etc. Just give a brief explanation of where you got the data from.)\n",
    "\n",
    "Report validation errors for all 15 classifier-dataset combinations. Discuss your results: \n",
    " \n",
    "  - Are certain classifiers better for certain problems? Why? \n",
    "  - Would you change any of the hyperparameters? \n",
    "  - What about speed... what would you do for a huge data set? \n",
    "  - Any other considerations?\n",
    "\n",
    "Note: because all sklearn classifiers use the name fit/predict structure, you can put your 5 classifiers in a list/dict and iterate over them with a loop (and likewise with the data). This will probably be easier writing 15 calls to fit and 15 to predict, etc. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'knn'           : KNeighborsClassifier(),\n",
    "    'decision tree' : DecisionTreeClassifier(),\n",
    "    'random forest' : RandomForestClassifier(),\n",
    "    'SVM'           : SVC(),\n",
    "    'naive Bayes'   : BernoulliNB() \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you'll have to be a bit careful with `BernoulliNB`, as it's expecting binary features. You can skip it when it's not applicable, or you can try binarizing the features. Your call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7b: qualitative comparison of classifiers \n",
    "rubric={reasoning:4}\n",
    "\n",
    "Fill in the following table with at least one entry per box.\n",
    "\n",
    "Classifier |      Strengths | Weaknesses | Key hyperparameters |\n",
    "-----------|      ------------|------------|---------------------|\n",
    "$k$-NN              |            |            |                     |\n",
    "decision tree       |            |            |                     |\n",
    "random forest |            |            |                     |\n",
    "SVM                 |            |            |                     |\n",
    "naive Bayes         |            |            |                     |\n",
    "\n",
    "For strengths and weaknesses, some things to consider are:\n",
    "  * ease of use for multi-class classification\n",
    "  * concerns about underfitting\n",
    "  * concerns about overfitting\n",
    "  * speed\n",
    "  * scalability for large data sets\n",
    "  * effectiveness when number of examples $\\ll$ number of features\n",
    "  * effectiveness when number of examples $\\gg$ number of features\n",
    "  * ability to represent uncertainty\n",
    "  * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 8: data scaling\n",
    "rubric={reasoning:3}\n",
    "\n",
    "Of the five classification methods listed below, which ones are sensitive to the normalization of the features? For example, if one of your features is length, which methods would give different results if the unit of length was changed from metres to centimetres across all the (training and testing) data? For each method, answer \"yes\" or \"no\" or \"not applicable\" and provide a one-sentence explanation. \n",
    "\n",
    "Note: you are welcome to perform empirical tests, although it should be possible to answer the question using your knowledge of these classifiers. If testing things out empirically, be careful that your software package (e.g. sklearn) isn't normalizing things by default.\n",
    "\n",
    "Classifier | Sensitive? | Short explanation |\n",
    "-----------| ----------|---------------------|\n",
    "kNN                  |            |\n",
    "decision tree |            |            | \n",
    "random forest |            |            |\n",
    "SVM          |            |            |  \n",
    "naive Bayes  |            |            | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "__Attribution__: Much of this material is from Mark Schmidt via CPSC 340."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
