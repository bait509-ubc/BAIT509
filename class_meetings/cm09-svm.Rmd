---
title: 'BAIT 509 Class Meeting 09'
subtitle: "Support Vector Machines"
date: "Monday, March 26, 2018"
output: 
    html_document:
        keep_md: true
        toc: true
        toc_depth: 2
        number_sections: true
        theme: cerulean
        toc_float: true
---

# Overview

- Maximal Margin Classifier, and hyperplanes
- Support Vector Classifiers (SVC)
- Support Vector Machines (SVM)
- Extensions to multiple classes
- SVM's in python
- Feedback on Assignment 1
- Lab: work on Assignment 3

# The setup

Today, we'll dicuss a new method for __binary classification__ -- that is, classification when there are two categories. The method is called __Support Vector Machines__. We'll build up to it by considering two special cases:

1. The Maximal Margin Classifier (too restrictive to use in practice)
2. The Support Vector Classifier (linear version of SVM)

We'll demonstrate concepts when there are two predictors, because it's more difficult to visualize in higher dimensions. But concepts generalize.

Let's start by loading some useful R packages to demonstrate concepts.

```{r}
suppressPackageStartupMessages(library(tidyverse))
knitr::opts_chunk$set(fig.width=6, fig.height=3, fig.align="center")
```


# Maximal Margin Classifier

## Setup

Consider the following two-predictor example. The response can take on one of two categories: "A" or "B". Also consider the three classifiers, where above a line we predict "B", and below, we predict "A":

```{r, echo=FALSE, fig.width=4, fig.height=3}
set.seed(4392)
n <- 20
mux1 <- c(4,0)
mux2 <- c(0,4)
dat_mmc <- tibble(x1 = rnorm(n) + rep(mux1, each=n/2),
                  x2 = rnorm(n) + rep(mux2, each=n/2),
                  y  = rep(LETTERS[1:2], each=n/2))
lines_mmc <- tibble(beta0_mid=c(-6,1,0.6994075),
                    beta1=c(4,0,0.8943409),
                    line=factor(1:3))
ggplot(dat_mmc, aes(x1,x2)) +
    geom_point(aes(shape=y, colour=y)) +
    theme_bw() +
    theme(axis.title.y = element_text(angle=0, vjust=0.5)) +
    geom_abline(data=lines_mmc,
                mapping=aes(intercept=beta0_mid, 
                            slope=beta1,
                            linetype=line), 
                alpha=0.5) + 
    coord_equal()
```

Each line perfectly classifies the training data -- which one would you prefer, and why?

## The Method

The Maximal Margin Classifier only applies when the two groups can be perfectly separated on the training set by:

- a dividing line (if we have 2 predictors),
- a dividing plane (if we have 3 predictors),
- or in general, we need a __dividing hyperplane__.

We choose the line/hyperplane so that the observations closest to the line/hyperplane are as far away as possible. This minimizes the chance that a new observation will be misclassified. In other words, _we want to fit the widest slab possible between the two classes_. This is called __maximizing the margin__, where the margin is the smallest distance from the hyperplane and the points.

```{r, fig.width=8}
lines_mmc %>% 
    mutate(beta0_min=c(-9.464481,0.6778491,-1.819599),
           beta0_max=c(-2.535519,1.322151,3.218414)) %>%
    gather(key="level", value="beta0",
           beta0_min, beta0_mid, beta0_max) %>%
    mutate(line = paste("Line", line)) %>%
    tidyr::crossing(dat_mmc) %>% 
    ggplot(aes(x1, x2)) +
    facet_wrap(~line) +
    geom_point(aes(shape=y, colour=y)) +
    geom_abline(aes(intercept=beta0, slope=beta1), alpha=0.5) +
    theme_bw() +
    theme(axis.title.y = element_text(angle=0, vjust=0.5)) +
    ylim(c(-2.75, 5.25)) +
    coord_equal()
```


Let's look at the resulting "slabs" from the above three examples.

We can order the lines from best to worst: Line 2 is the worst, and Line 3 is the best. 



Out of the three above lines, Line 3 is the best choice -- but in general, we can run an algorithm to optimize this.

A perfect classification almost never happens with real data, but this is an important setup before moving on to support vector machines. 

