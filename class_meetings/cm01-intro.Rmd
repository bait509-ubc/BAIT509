---
title: "BAIT 509 Class Meeting 01"
output: github_document
---

## GitHub

Your participation will be assessed in class, as well as your attempts at doing the in-class exercises. Be sure to commit and push to your github repo while doing class exercises. I'll be using Source Tree to communicate between my laptop and GitHub.

- 5%: attempting the in-class exercises, as assessed through your commit history. 
    - Key word here is _attempt_ -- we're looking for effort here, not correctness. 
- 5%: in-class discussion
    - You'll be given the opportunity to briefly present your in-class exercises.

Regarding the in-class mini-presentations, don't fret if you or your team can't figure out the answer to a question you are presenting! You won't be penalized _at all_ for not knowing, or getting an answer wrong, but at least try. The floor will be turned to the class for an answer, or I will chime in with some insight. 

## Introduction to class outlines:

When the topic of discussion is a Machine Learning methodology:

1. Brief explanation of the main idea underlying a methodology.
2. Hands-on exploratory work with toy datasets to allow you to build concepts.
3. Connection to higher-level concepts, including more complex extensions. 

In your actual assignments, you'll be working with real data. 

## General outline of class meetings

- 10:00-11:10: hands-on lecture. Typically involves:
    - A short explanation of basic concepts, then 
    - hands-on work with mini presentations, then
    - deeper explanation regarding details.
- 11:10-11:50: lab portion. 
    - Typically involves you working on an assignment, with members of the teaching team available to answer your questions. 

## Terminology

In supervised learning, we try to gain information on a variable $Y$, given observations on variables $X_1,\ldots,X_p$.

The variable $Y$ is called the __response variable__, and sometimes the _dependent variable_.

The variables $X_1, \ldots, X_p$ are called __predictor variables__ (or just "predictors"). There are many other terms for these, including _features_, _independent variables_, and _covariates_.

## Fundamental Concept: Predictors give us more information about the response

### Numeric Example

Take the example where $(X,Y)$ are bivariate normal; $Y$ is also Normal. Using the distribution of $Y$ alone, there is a lot of uncertainty as to what a future outcome of the response might be. But if we observe, say, $X=2$, then the distribution of $Y$ is more certain. This is the __conditional distribution__ of the response given the predictors. 

Almost all supervised learning methods decide to use the __mean__ as the prediction.

### Categorical Example

Take the example where $Y$ can be one of three categories: _A_, _B_, or _C_. 

DGP:

$$ \text{logit}(P(Y=B|X=x)) = 5 + x $$

$$ \text{logit}(P(Y=C|X=x)) = x $$

$X$ is Exponential(1). 

Show the marginal of Y. Show some conditional distributions. 

Almost all supervised learning methods decide to use the __mode__ as the prediction.

## Fundamental Concept: Evaluating Prediction Goodness

Suppose we've come up with a model that makes predictions of the response, given knowledge of predictors. How can we evaulate how good (or bad) the predictions are? It depends on whether the response is categorical or numeric.

### Categorical Response

When the response is _categorical_, the __prediction accuracy__ measures prediction "goodness" as the proportion of correct predictions:
$$ \text{Prediction Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}. $$
Equivalently, we can measure the prediction "badness" with the __error rate__, which is one minus the prediction accuracy, telling us the proportion of _incorrect_ predictions. 

This will be the main focus of the course, but there are others, too. A common measure of goodness is the (Shannon) __information__ (the equivalent measure of badness is __entropy__). These measurements consider the entire conditional distribution, as opposed to just the mode. 

### Numeric Response

When the response is _numeric_, and we're forming predictions using a mean estimate (usually the case), then there are a few measures of goodness. Suppose we've made $N$ predictions $\hat{y}_1, \ldots, \hat{y}_N$, for which the actual response ended up being $y_1, \ldots, y_N$.  

- The __mean squared error__ (MSE) is a universal measure of badness (the larger this number, the worse the model is). $$ \text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2. $$
- The __coefficient of determination__ ($R^2$) is a universal measure of goodness. 
- The __likelihood__ is a measure of goodness, and is only applicable when we've made distributional assumptions. 

There are extensions to these, such as AIC and adjusted $R^2$, but these have a different meaning. We'll touch on these later. 

## LAB

- Give an exercise with discrete and continuous $Y$ with no predictors. Then introduce a categorical predictor. Then a continuous predictor (but without the categorical). 