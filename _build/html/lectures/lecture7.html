
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. Linear Models &#8212; BAIT 509 - Business Applications of Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://bait509-ubc.github.io/BAIT509/intro.html/lectures/lecture7.html" />
    <link rel="shortcut icon" href="../_static/bait_logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Business Objectives/Statistical Questions and Feature Selection" href="lecture8.html" />
    <link rel="prev" title="6. Naive Bayes and Hyperparameter Optimization" href="lecture6.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/bait_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">BAIT 509 - Business Applications of Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption" role="heading">
 <span class="caption-text">
  Things You Should Know
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/who.html">
   Who: Hayley Boyce
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/how.html">
   How: The Course Structure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/what.html">
   What: Learning Outcomes
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture1.html">
   1. Intro to ML &amp;  Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2.html">
   2. Splitting and Cross-validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3.html">
   3. Baseline, k-Nearest Neighbours
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4.html">
   4. SVM with RBF Kernel and Feature Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5.html">
   5. Preprocessing Categorical Features and Column Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6.html">
   6. Naive Bayes and Hyperparameter Optimization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8.html">
   8. Business Objectives/Statistical Questions and Feature Selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9.html">
   9. Classification and Regression Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10.html">
   10. Multi-Class, Pandas Profiling, Finale
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../things_to_know/attribution.html">
   Attribution
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lectures/lecture7.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bait509-ubc/BAIT509"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bait509-ubc/BAIT509/issues/new?title=Issue%20on%20page%20%2Flectures/lecture7.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bait509-ubc/BAIT509/edit/master/lectures/lecture7.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bait509-ubc/BAIT509/master?urlpath=tree/lectures/lecture7.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#house-keeping">
   7.1. House Keeping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lecture-learning-objectives">
   7.2. Lecture Learning Objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#five-minute-recap-lightning-questions">
   7.3. Five Minute Recap/ Lightning Questions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-lingering-questions">
     7.3.1. Some lingering questions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   7.4. Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge">
     7.4.1. Ridge
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-alpha">
     7.4.2. Hyperparameter -
     <code class="docutils literal notranslate">
      <span class="pre">
       alpha
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-linear-regression-ridge">
     7.4.3. Visualizing linear regression (Ridge)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-intuition-behind-linear-regression">
     7.4.4. The intuition behind linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-does-predict-work">
     7.4.5. How does predict work?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-do-we-interpret-these-coefficients">
       7.4.5.1. How do we interpret these coefficients?
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sign">
         7.4.5.1.1. Sign
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#magnitude">
         7.4.5.1.2. Magnitude
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#components-of-a-linear-model">
     7.4.6. Components of a linear model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-sklearn">
     7.4.7. Using sklearn
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#to-summarize">
     7.4.8. To summarize
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-example">
     7.4.9. Prediction Example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-practice">
   7.5. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   7.6. Logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coefficients">
     7.6.1. Coefficients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictions">
     7.6.2. Predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression-hyperparameter-c">
     7.6.3. Logistic Regression Hyperparameter
     <code class="docutils literal notranslate">
      <span class="pre">
       C
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predicting-probabilities">
     7.6.4. Predicting probabilities
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-is-this-being-done">
     7.6.5. How is this being done?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-boundaries-with-predict-proba">
     7.6.6. Decision boundaries with
     <code class="docutils literal notranslate">
      <span class="pre">
       predict_proba
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#limitations-of-linear-classifiers">
   7.7. Limitations of linear classifiers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-to-naive-bayes">
   7.8. Compare to naive Bayes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   7.9. Let’s Practice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-practice-coding">
   7.10. Let’s Practice - Coding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-we-ve-learned-today">
   7.11. What We’ve Learned Today
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-models">
<h1><span class="section-number">7. </span>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this headline">¶</a></h1>
<p><em>Hayley Boyce, May 10th, 2021</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing our libraries</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span><span class="p">,</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span><span class="p">,</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;code/&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">display_tree</span> <span class="kn">import</span> <span class="n">display_tree</span>
<span class="kn">from</span> <span class="nn">plot_classifier</span> <span class="kn">import</span> <span class="n">plot_classifier</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Preprocessing and pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_transformer</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">MinMaxScaler</span>

<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="house-keeping">
<h2><span class="section-number">7.1. </span>House Keeping<a class="headerlink" href="#house-keeping" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The assignment is due today!</p></li>
<li><p>Project instructions this week!</p></li>
<li><p>Quiz results</p></li>
</ul>
</div>
<div class="section" id="lecture-learning-objectives">
<h2><span class="section-number">7.2. </span>Lecture Learning Objectives<a class="headerlink" href="#lecture-learning-objectives" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Explain the general intuition behind linear models.</p></li>
<li><p>Explain the <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> paradigm of linear models.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> classifier.</p>
<ul>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">fit</span></code>, <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">coef_</span></code> to interpret the model weights.</p></li>
</ul>
</li>
<li><p>Explain the advantages and limitations of linear classifiers.</p></li>
<li><p>Apply scikit-learn regression model (e.g., Ridge) to regression problems.</p></li>
<li><p>Relate the Ridge hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> to the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> hyperparameter <code class="docutils literal notranslate"><span class="pre">C</span></code>.</p></li>
<li><p>Compare logistic regression with naive Bayes.</p></li>
</ul>
</div>
<div class="section" id="five-minute-recap-lightning-questions">
<h2><span class="section-number">7.3. </span>Five Minute Recap/ Lightning Questions<a class="headerlink" href="#five-minute-recap-lightning-questions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>To avoid the problem of zero probability in Naïve Bayes, what technique do we use?</p></li>
<li><p><em>True or False:</em> We only calculate the probabilities of the words that occur in our spam/non spam case study?</p></li>
<li><p>What is the name of the function we use to perform Exhaustive Hyperparameter Optimization?</p></li>
<li><p>Repeating cross-validation over and over again with different hyperparameters can cause our model to suffer from what?</p></li>
<li><p>What parameter does <code class="docutils literal notranslate"><span class="pre">RandomizedSearchCV</span></code> have that the Exhaustive Hyperparameter tuning function not have?</p></li>
</ul>
<div class="section" id="some-lingering-questions">
<h3><span class="section-number">7.3.1. </span>Some lingering questions<a class="headerlink" href="#some-lingering-questions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>What was that <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> function we looked at briefly when learning about naive Bayes?</p></li>
<li><p>Are there any models that function in a linear manner?</p></li>
<li><p>Is there any particular model that is best for all problems? -&gt; let’s answer this one right away… <strong>NO!</strong></p></li>
</ul>
</div>
</div>
<div class="section" id="linear-regression">
<h2><span class="section-number">7.4. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>We’ve seen many regression models such as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SVR</span></code></p></li>
</ul>
<p>but now we have a new one that we are going to explore called  <strong>linear regression</strong>.</p>
<p>Linear regression is one of the most basic and popular ML/statistical techniques.</p>
<p>For this section let’s bring back some hypothetical snake data that we saw in Lecture 3 when we were discussing <span class="math notranslate nohighlight">\(K\)</span>-nn regression.</p>
<p><em>(We can ignore how this code was made)</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_1</span><span class="p">[:,</span><span class="kc">None</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;length&#39;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="mi">3</span> <span class="o">+</span> <span class="n">X_1</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span> <span class="o">+</span> <span class="mf">.2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
<span class="n">snakes_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">snakes_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">77</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">&#39;length&#39;</span><span class="p">]]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[[</span><span class="s1">&#39;length&#39;</span><span class="p">]]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span>

<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>73</th>
      <td>1.489130</td>
      <td>10.507995</td>
    </tr>
    <tr>
      <th>53</th>
      <td>1.073233</td>
      <td>7.658047</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1.622709</td>
      <td>9.748797</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.984653</td>
      <td>9.731572</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.484937</td>
      <td>3.016555</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="section" id="ridge">
<h3><span class="section-number">7.4.1. </span>Ridge<a class="headerlink" href="#ridge" title="Permalink to this headline">¶</a></h3>
<p>We can import the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> model as we have for all the previous models we’ve used:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression()
</pre></div>
</div>
</div>
</div>
<p>… except now we are going to instead focus on its close cousin <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> is more flexible than <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> and we will explain why shortly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, has the same fit-predict paradigm as the other models we have seen.</p>
<p>That means we can <code class="docutils literal notranslate"><span class="pre">fit</span></code> on the training set and <code class="docutils literal notranslate"><span class="pre">predict</span></code> a numeric prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rm</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">rm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>We see that <code class="docutils literal notranslate"><span class="pre">predict</span></code> returns the predicted snake weight for our examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([10.09739051,  7.90823334, 10.80050927,  7.44197529,  4.81162144])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8125029624787177
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hyperparameter-alpha">
<h3><span class="section-number">7.4.2. </span>Hyperparameter - <code class="docutils literal notranslate"><span class="pre">alpha</span></code><a class="headerlink" href="#hyperparameter-alpha" title="Permalink to this headline">¶</a></h3>
<p>Ridge has hyperparameters just like the rest of the models we learned.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter is what makes it more flexible than using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p>
<p>Remember the fundamental trade-off?</p>
<p>Well, similar to the other hyperparameters that we saw, <code class="docutils literal notranslate"><span class="pre">alpha</span></code> will control this.</p>
<p>Note: if we set <code class="docutils literal notranslate"><span class="pre">alpha=0</span></code> that is the same as using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rm_a1</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rm_a1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">rm_a1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8125029624787177
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rm_a1000</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">rm_a1000</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">rm_a1000</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.004541128724857568
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span><span class="p">{</span>
<span class="s2">&quot;alpha&quot;</span> <span class="p">:</span><span class="mf">10.0</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="s2">&quot;train_scores&quot;</span> <span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="s2">&quot;cv_scores&quot;</span> <span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]:</span>
    <span class="n">ridge_model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">ridge_model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;train_scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;cv_scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alpha</th>
      <th>train_scores</th>
      <th>cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.01</td>
      <td>0.812961</td>
      <td>0.799169</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.10</td>
      <td>0.812945</td>
      <td>0.799199</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.00</td>
      <td>0.811461</td>
      <td>0.798103</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10.00</td>
      <td>0.735071</td>
      <td>0.721655</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100.00</td>
      <td>0.270059</td>
      <td>0.244916</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1000.00</td>
      <td>0.035217</td>
      <td>0.003744</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10000.00</td>
      <td>0.003629</td>
      <td>-0.028689</td>
    </tr>
    <tr>
      <th>7</th>
      <td>100000.00</td>
      <td>0.000364</td>
      <td>-0.032041</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>As we increase <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, we are <em><strong>decreasing</strong></em> our model complexity which means our training score is lower and we are more likely to underfit.</p></li>
<li><p>If we decrease <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, our model complexity is increasing and consequentially our training score is increasing. Our chances of overfitting are going up.</p></li>
</ul>
</div>
<div class="section" id="visualizing-linear-regression-ridge">
<h3><span class="section-number">7.4.3. </span>Visualizing linear regression (Ridge)<a class="headerlink" href="#visualizing-linear-regression-ridge" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;length&#39;</span><span class="p">:</span>  <span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;length&#39;</span><span class="p">],</span>
                             <span class="s1">&#39;weight&#39;</span><span class="p">:</span>  <span class="n">y_train</span><span class="p">,</span>
                             <span class="s1">&#39;predicted&#39;</span><span class="p">:</span> <span class="n">rm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)})</span>
<span class="n">chart1</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;length:Q&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span> <span class="s2">&quot;Length&quot;</span><span class="p">),</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;weight:Q&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span> <span class="s2">&quot;Weight&quot;</span><span class="p">))</span>
<span class="n">chart2</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;length:Q&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span> <span class="s2">&quot;Length&quot;</span><span class="p">),</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;predicted:Q&#39;</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Weight&quot;</span><span class="p">))</span>

<span class="n">chart3</span> <span class="o">=</span> <span class="n">chart1</span> <span class="o">+</span> <span class="n">chart2</span>
<span class="n">chart3</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-4c50a95947cf462ea06deba05d0e54be"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-4c50a95947cf462ea06deba05d0e54be") {
      outputDiv = document.getElementById("altair-viz-4c50a95947cf462ea06deba05d0e54be");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "layer": [{"mark": "circle", "encoding": {"x": {"type": "quantitative", "field": "length", "title": "Length"}, "y": {"type": "quantitative", "field": "weight", "title": "Weight"}}, "height": 300, "width": 500}, {"mark": {"type": "line", "color": "orange"}, "encoding": {"x": {"type": "quantitative", "field": "length", "title": "Length"}, "y": {"type": "quantitative", "field": "predicted", "title": "Weight"}}, "height": 300, "width": 500}], "data": {"name": "data-9a474cec69cb70890c963e4b8e15291e"}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-9a474cec69cb70890c963e4b8e15291e": [{"length": 1.4891298699548126, "weight": 10.507994972940256, "predicted": 10.09739051488952}, {"length": 1.0732328480626239, "weight": 7.658046569651847, "predicted": 7.908233341040291}, {"length": 1.6227086731986475, "weight": 9.748797005406935, "predicted": 10.800509267856306}, {"length": 0.9846529437049815, "weight": 9.73157160716233, "predicted": 7.441975294273899}, {"length": 0.48493718685407583, "weight": 3.0165548660571386, "predicted": 4.811621441249101}, {"length": 1.3430351854245999, "weight": 8.664086836980864, "predicted": 9.328391917002321}, {"length": 1.6675322302005446, "weight": 12.19561780290861, "predicted": 11.036447026972024}, {"length": 1.660720682708637, "weight": 9.405644658611909, "predicted": 11.00059308411597}, {"length": 0.26001269847434616, "weight": 3.1977109582539485, "predicted": 3.6276864011023258}, {"length": 0.9331241478105046, "weight": 5.685769157789863, "predicted": 7.170743168881453}, {"length": 0.6720209602299276, "weight": 4.18927097207944, "predicted": 5.796374308036151}, {"length": 0.7853383764560611, "weight": 9.98262617791568, "predicted": 6.392843197197385}, {"length": 1.3213974141107516, "weight": 8.896760117529569, "predicted": 9.214497179111895}, {"length": 1.5130241854813398, "weight": 8.158947877684195, "predicted": 10.223163024659195}, {"length": 0.040732242040826254, "weight": 1.5782982907118732, "predicted": 2.473459852178366}, {"length": 0.5225713944348283, "weight": 3.8994269750274193, "predicted": 5.009716621475931}, {"length": 1.8046739799138949, "weight": 12.637307079851663, "predicted": 11.758320061439296}, {"length": 1.4291178052973903, "weight": 9.796829766973492, "predicted": 9.781505007307416}, {"length": 0.12386689835068723, "weight": 1.2898664428659872, "predicted": 2.911055746537523}, {"length": 1.7930735741117094, "weight": 9.94031687027138, "predicted": 11.697259004868421}, {"length": 1.8584634367197943, "weight": 12.739537462768615, "predicted": 12.041451627778361}, {"length": 1.8922534471161385, "weight": 15.819173760406724, "predicted": 12.219312107103391}, {"length": 0.18782316697737733, "weight": 2.513496213544915, "predicted": 3.2477023607396354}, {"length": 1.578179467430011, "weight": 10.086508392023187, "predicted": 10.566120885341057}, {"length": 1.3707354363693722, "weight": 10.266354705831471, "predicted": 9.47419772921343}, {"length": 1.2132595335379277, "weight": 7.501688161037575, "predicted": 8.645291812014516}, {"length": 0.6662245388374752, "weight": 6.765077376939494, "predicted": 5.7658636844779245}, {"length": 0.34467315248721386, "weight": 3.2105715431040456, "predicted": 4.073313636903128}, {"length": 0.5212002466982484, "weight": 4.271877481169299, "predicted": 5.002499311069898}, {"length": 1.2084330274367336, "weight": 9.030136582502383, "predicted": 8.619886531620864}, {"length": 1.8236692693796017, "weight": 10.13866280069108, "predicted": 11.858305567500395}, {"length": 0.28040079204155743, "weight": 5.302575791390477, "predicted": 3.7350032101785393}, {"length": 0.8910599225061118, "weight": 6.200081280641692, "predicted": 6.949329704152695}, {"length": 0.44057304501158057, "weight": 4.314203723495225, "predicted": 4.5781019057297065}, {"length": 0.4257857795969878, "weight": 4.866696809368363, "predicted": 4.500266176065935}, {"length": 1.7112640189830859, "weight": 14.317737709438546, "predicted": 11.26663804565976}, {"length": 1.0515193903571667, "weight": 6.039165902648527, "predicted": 7.793940212686764}, {"length": 0.36638096287396327, "weight": 3.976220693046897, "predicted": 4.187577039464905}, {"length": 0.06468122343602568, "weight": 4.8527710881829424, "predicted": 2.5995201066835625}, {"length": 1.5386732775084204, "weight": 10.776595778382621, "predicted": 10.358172151578017}, {"length": 1.9763768095637113, "weight": 12.898439354694574, "predicted": 12.662112254006491}, {"length": 0.49054371805903935, "weight": 4.996624023446674, "predicted": 4.8411325398184415}, {"length": 1.455631938260402, "weight": 7.751103489400314, "predicted": 9.921067450241704}, {"length": 1.3042327445211057, "weight": 10.058035179582774, "predicted": 9.124147506979195}, {"length": 0.016905257038003562, "weight": 2.8779962599472553, "predicted": 2.3480417500895063}, {"length": 0.5761505311499983, "weight": 5.973354108924219, "predicted": 5.291741125943455}, {"length": 1.382139573004812, "weight": 9.547323609319276, "predicted": 9.534225683762}, {"length": 0.5816938426733856, "weight": 3.959378484871299, "predicted": 5.320919455071886}, {"length": 0.07291885052182343, "weight": 2.0235998443342242, "predicted": 2.6428805047655755}, {"length": 1.0326760487694024, "weight": 10.744326469248147, "predicted": 7.694754514677994}, {"length": 1.734376625000459, "weight": 12.981462281939878, "predicted": 11.388295871049046}, {"length": 1.5629058213993292, "weight": 9.696860623701482, "predicted": 10.48572499400532}, {"length": 0.8164881405022435, "weight": 6.415600290881839, "predicted": 6.5568062116839085}, {"length": 1.116963496652776, "weight": 10.453680423809956, "predicted": 8.138418358097297}, {"length": 0.42054740095226884, "weight": 3.473896448846035, "predicted": 4.472692922145406}, {"length": 0.1212032173535419, "weight": 5.086179556714217, "predicted": 2.8970349287465793}, {"length": 0.220506739610265, "weight": 2.2019587548610247, "predicted": 3.4197388835567084}, {"length": 0.015542646496611877, "weight": 1.400824281695786, "predicted": 2.340869376918189}, {"length": 1.9952807334786649, "weight": 10.6176012566981, "predicted": 12.761616839213758}, {"length": 0.7004491136740391, "weight": 9.199523468555924, "predicted": 5.9460115806675375}, {"length": 1.4315170819826806, "weight": 7.93450850637016, "predicted": 9.794134080109515}, {"length": 0.19576591228053444, "weight": 2.042781160965016, "predicted": 3.289510589568384}, {"length": 0.28849788890539513, "weight": 2.191191549371276, "predicted": 3.777623899324559}, {"length": 1.5738290211540782, "weight": 12.043069112514083, "predicted": 10.543221441072202}, {"length": 0.9719709000230267, "weight": 7.5302347521637145, "predicted": 7.375220820344009}, {"length": 0.71523052881673, "weight": 4.040617417878837, "predicted": 6.023816516254823}, {"length": 1.0895853189334226, "weight": 8.923743001401581, "predicted": 7.994307842738092}, {"length": 1.2038500511426322, "weight": 9.160921210227059, "predicted": 8.59576311908435}, {"length": 0.6299538905705881, "weight": 4.912686458765877, "predicted": 5.574945871476125}, {"length": 0.3685731385196855, "weight": 7.28559741064436, "predicted": 4.199115994515212}, {"length": 0.8718054968572184, "weight": 8.28958002585414, "predicted": 6.847980182951872}, {"length": 0.7845846503596573, "weight": 5.242915603248352, "predicted": 6.388875809108015}, {"length": 1.156531392746962, "weight": 8.258832565398267, "predicted": 8.346691894643953}, {"length": 0.6369873275783813, "weight": 4.471783323151077, "predicted": 5.611967774187461}, {"length": 0.4843946245498784, "weight": 2.8494147216671455, "predicted": 4.808765556022777}, {"length": 0.7620945037468827, "weight": 8.020034168601496, "predicted": 6.270494423316222}, {"length": 1.968286352888498, "weight": 11.950670341380446, "predicted": 12.619526516821647}, {"length": 1.6950094238466173, "weight": 9.024726938439192, "predicted": 11.181078732469395}, {"length": 1.9186332446914185, "weight": 15.984552876645637, "predicted": 12.358167448850907}, {"length": 1.7705446095030246, "weight": 11.246186203797295, "predicted": 11.57867329278972}]}}, {"mode": "vega-lite"});
</script></div></div>
</div>
<p>In this plot, the blue markers are the examples and our orange line is our Ridge regression line.</p>
<p>In our data, we only have 1 feature - <code class="docutils literal notranslate"><span class="pre">length</span></code> which helps predict our target feature <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p>
<p>We can use a 2D graph to plot this and our ridge regression corresponds to a line.</p>
<p>If we had many features, our predictions would be represented as a hyperplane of some sort instead</p>
<p>If we extend our line, we can also see that we will get some predictions for values that don’t really make sense, like if a snake had a negative length, we would predict, a negative weight.</p>
</div>
<div class="section" id="the-intuition-behind-linear-regression">
<h3><span class="section-number">7.4.4. </span>The intuition behind linear regression<a class="headerlink" href="#the-intuition-behind-linear-regression" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>listing number</p></th>
<th class="head"><p>Number of Bedrooms</p></th>
<th class="head"><p>Number of Bathrooms</p></th>
<th class="head"><p>Square Footage</p></th>
<th class="head"><p>Age</p></th>
<th class="head"><p>Price</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>5</p></td>
<td><p>6</p></td>
<td><p>3000</p></td>
<td><p>2</p></td>
<td><p>$6.39 million</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>800</p></td>
<td><p>90</p></td>
<td><p>$1.67 million</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
<td><p>1875</p></td>
<td><p>66</p></td>
<td><p>$3.92 million</p></td>
</tr>
</tbody>
</table>
<p>Unlike with decision trees where we make predictions with rules and analogy-based models where we predict a certain class using distance to other examples, linear classifiers use <strong>coefficients</strong> (or sometimes known as “weights”) associated with features.</p>
<p>When we call fit, each feature is going to obtain a coefficient which is going to be a number that is going to help give meaning to our model.</p>
<p>These coefficients are learned from the training data.</p>
<a class="reference internal image-reference" href="../_images/house_table.png"><img alt="404 image" src="../_images/house_table.png" style="width: 50%;" /></a>
</div>
<div class="section" id="how-does-predict-work">
<h3><span class="section-number">7.4.5. </span>How does predict work?<a class="headerlink" href="#how-does-predict-work" title="Permalink to this headline">¶</a></h3>
<p>Let’s look at the house listing with the following features:</p>
<br>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>listing number</p></th>
<th class="head"><p>Number of Bedrooms</p></th>
<th class="head"><p>Number of Bathrooms</p></th>
<th class="head"><p>Square Footage</p></th>
<th class="head"><p>Age</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
<td><p>1875</p></td>
<td><p>66</p></td>
</tr>
</tbody>
</table>
<br><p>To calculate our predicted price we sum up the multiplication of each coefficient by its feature value plus an intercept.</p>
<p><span class="math notranslate nohighlight">\(\text{predicted}(price) \text{       } =   \text{coefficient}_{\text{bedrooms}} * \text{#bedrooms}  \\ + \text{ }\text{coefficient}_{\text{bathrooms}} * \text{#bathrooms} \\  + \text{ } \text{coefficient}_{\text{sqfeet}} * \text{#sqfeet}  \\ + \text{ } \text{coefficient}_{\text{age}} * \text{age}  \\ + \text{ } \text{intercept}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{predicted}(price) \text{       } =  0.03 * 3  \\ \text{ } 0.04 * 2 \\  + \text{ } 0.002 * 1875  \\ + \text{ } -0.01* 66  \\ + \text{ } 0
\)</span></p>
<p><span class="math notranslate nohighlight">\(
\text{predicted}(price)  =  3.26
\)</span></p>
<div class="section" id="how-do-we-interpret-these-coefficients">
<h4><span class="section-number">7.4.5.1. </span>How do we interpret these coefficients?<a class="headerlink" href="#how-do-we-interpret-these-coefficients" title="Permalink to this headline">¶</a></h4>
<div class="section" id="sign">
<h5><span class="section-number">7.4.5.1.1. </span>Sign<a class="headerlink" href="#sign" title="Permalink to this headline">¶</a></h5>
<p>If the coefficient is a positive number (like 0.03 for the number of bedrooms) that means that this feature is contributing to the prediction in a positive.
This means that the more bedrooms a house has, the higher the price the model will predict.</p>
<p>In this, we have a negative coefficient -&gt; age. Here, as age increases, the bigger the feature value, the lower the predicted price. The feature is contributing to the predicted price in a negative way.
Older houses will have a lower, predicted price.</p>
</div>
<div class="section" id="magnitude">
<h5><span class="section-number">7.4.5.1.2. </span>Magnitude<a class="headerlink" href="#magnitude" title="Permalink to this headline">¶</a></h5>
<p>The magnitude of the coefficient also has a direct effect on the predicted price.
here for every additional bedroom, we are adding 0.03 million dollars to our prediction.</p>
<p>Here we are learning every additional bedroom is worth 0.03 million dollars and every additional year is decreasing 0.01 million dollars to our prediction.</p>
</div>
</div>
</div>
<div class="section" id="components-of-a-linear-model">
<h3><span class="section-number">7.4.6. </span>Components of a linear model<a class="headerlink" href="#components-of-a-linear-model" title="Permalink to this headline">¶</a></h3>
<p><font size="3"><em> predicted(price) = (<font  color="#b1d78c">coefficient<sub>bedrooms</sub></font>   x  <font  color="7bd1ec">#bedrooms</font>)   +  (<font  color="#b1d78c">coefficient<sub>bathrooms</sub></font>  x  <font  color="7bd1ec">#bathrooms</font>)  +  (<font  color="#b1d78c">coefficient<sub>sqfeet</sub></font>   x   <font  color="7bd1ec">#sqfeet</font>)  +  (<font  color="#b1d78c">coefficient<sub>age</sub></font>  x <font  color="7bd1ec">age</font>)  +  <font  color="e8b0d0">intercept</font> </em> </font></p>
<ul class="simple">
<li><p><font  color="7bd1ec"> Input features</font></p></li>
<li><p><font  color="#b1d78c"> Coefficients, one per feature</font></p></li>
<li><p><font  color="e8b0d0"> Bias or intercept</font></p></li>
</ul>
<p>When we build a particular model, the <font  color="#b1d78c"> Coefficients</font> and  <font  color="e8b0d0">intercept</font>  will remain the same, but the blue <font  color="7bd1ec"> Input features</font>  will differ for each observation - in this case for every house listing.</p>
</div>
<div class="section" id="using-sklearn">
<h3><span class="section-number">7.4.7. </span>Using sklearn<a class="headerlink" href="#using-sklearn" title="Permalink to this headline">¶</a></h3>
<p>Let’s take a look at a <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set">Taiwan housing dataset</a> that I’ve wrangled a bit. (Available in the data folder)</p>
<p><strong>Attribution:</strong>
Real Estate Dataset - The UCI Machine Learning Repository
Yeh, I. C., &amp; Hsu, T. K. (2018). Building real estate valuation models with comparative approach through case-based reasoning. Applied Soft Computing, 65, 260-271.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">housing_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/real_estate.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">housing_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>house_age</th>
      <th>distance_station</th>
      <th>num_stores</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>172</th>
      <td>6.6</td>
      <td>90.45606</td>
      <td>9</td>
      <td>24.97433</td>
      <td>121.54310</td>
      <td>58.1</td>
    </tr>
    <tr>
      <th>230</th>
      <td>4.0</td>
      <td>2147.37600</td>
      <td>3</td>
      <td>24.96299</td>
      <td>121.51284</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>346</th>
      <td>13.2</td>
      <td>1712.63200</td>
      <td>2</td>
      <td>24.96412</td>
      <td>121.51670</td>
      <td>30.8</td>
    </tr>
    <tr>
      <th>244</th>
      <td>4.8</td>
      <td>1559.82700</td>
      <td>3</td>
      <td>24.97213</td>
      <td>121.51627</td>
      <td>21.7</td>
    </tr>
    <tr>
      <th>367</th>
      <td>15.0</td>
      <td>1828.31900</td>
      <td>2</td>
      <td>24.96464</td>
      <td>121.51531</td>
      <td>20.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I separate this into our <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> objects for each of our splits as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]),</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]),</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>And now we can now use <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> to predict the house price.</p>
<p>We can make our model as usual and train it, and assess our training score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">training_score</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">training_score</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5170145681350131
</pre></div>
</div>
</div>
</div>
<p>We saw that with linear classifiers we have coefficients associated with each feature of our model.</p>
<p>How do we get that? We can use <code class="docutils literal notranslate"><span class="pre">.coef_</span></code> to obtain them from our trained model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge_coeffs</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">ridge_coeffs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-2.43214368e-01, -5.33723544e-03,  1.25878207e+00,  8.92353624e+00,
       -1.34523313e+00])
</pre></div>
</div>
</div>
</div>
<p>It gives us 5 coefficients one for each feature.</p>
<p>These coefficients are learned during the fit stage.</p>
<p>We can also get the intercept with <code class="docutils literal notranslate"><span class="pre">.intercept_</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge_intercept</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">ridge_intercept</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-16.240516720277654
</pre></div>
</div>
</div>
</div>
<p>But how are these useful?</p>
<p>One of the primary advantages of linear classifiers is their ability to interpret models using these coefficients.</p>
<p>What do these mean?</p>
<p>We have our coefficients but we should see which feature corresponds to which coefficient.</p>
<p>We can do that by making a dataframe with both values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_coeffs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ridge_coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">])</span>
<span class="n">words_coeffs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>house_age</th>
      <td>-0.243214</td>
    </tr>
    <tr>
      <th>distance_station</th>
      <td>-0.005337</td>
    </tr>
    <tr>
      <th>num_stores</th>
      <td>1.258782</td>
    </tr>
    <tr>
      <th>latitude</th>
      <td>8.923536</td>
    </tr>
    <tr>
      <th>longitude</th>
      <td>-1.345233</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can use these coefficients to interpret our model (Not the world! Just how our model makes predictions, we need more serious statistics for that). They show us how much each of these features affects our <strong>model’s</strong> prediction.</p>
<p>For example, if we had a house with 2 stores nearby, our <code class="docutils literal notranslate"><span class="pre">num_stores</span></code> value is 2. That means that 2 * 1.26 = 2.52 will contribute to our predicted price!</p>
<p>The negative coefficients work in the opposite way, for example, every unit increase in age of a house will, subtracts 0.243 from the house’s predicted value.</p>
<p>We can also look at the absolute values of the coefficients to see how important a feature is.</p>
<p>However, we have to be very careful about this - remember scaling?! We can’t necessarily say latitude is the most important since latitude may be on a different scale.</p>
<p>It’s important to be careful here though because this depends on the scaling of the features. Larger features will have smaller coefficients, but if we scale our features before we build our model then they are on a somewhat level playing field! (Another reason we should be scaling our features!)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_coeffs_df</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>distance_station</th>
      <td>0.005337</td>
    </tr>
    <tr>
      <th>house_age</th>
      <td>0.243214</td>
    </tr>
    <tr>
      <th>num_stores</th>
      <td>1.258782</td>
    </tr>
    <tr>
      <th>longitude</th>
      <td>1.345233</td>
    </tr>
    <tr>
      <th>latitude</th>
      <td>8.923536</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="to-summarize">
<h3><span class="section-number">7.4.8. </span>To summarize<a class="headerlink" href="#to-summarize" title="Permalink to this headline">¶</a></h3>
<p>In linear models:</p>
<ul class="simple">
<li><p>if the coefficient is positive, then increasing the feature values increases the prediction value.</p></li>
<li><p>if the coefficient is negative, then increasing the feature values decreases the prediction value.</p></li>
<li><p>if the coefficient is zero, the feature is not used in making a prediction</p></li>
</ul>
</div>
<div class="section" id="prediction-example">
<h3><span class="section-number">7.4.9. </span>Prediction Example<a class="headerlink" href="#prediction-example" title="Permalink to this headline">¶</a></h3>
<p>Let’s take a look at a single example here.</p>
<p>The values in this are the input features.</p>
<p>We can use <code class="docutils literal notranslate"><span class="pre">predict()</span></code> on our features to get a prediction of 52.36.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>house_age</th>
      <th>distance_station</th>
      <th>num_stores</th>
      <th>latitude</th>
      <th>longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>172</th>
      <td>6.6</td>
      <td>90.45606</td>
      <td>9</td>
      <td>24.97433</td>
      <td>121.5431</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([52.35605528])
</pre></div>
</div>
</div>
</div>
<p>Using our coefficients, and the model’s intercept we can calculate the model’s predictions ourselves as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_coeffs_df</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>house_age</th>
      <th>distance_station</th>
      <th>num_stores</th>
      <th>latitude</th>
      <th>longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.243214</td>
      <td>-0.005337</td>
      <td>1.258782</td>
      <td>8.923536</td>
      <td>-1.345233</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>house_age</th>
      <th>distance_station</th>
      <th>num_stores</th>
      <th>latitude</th>
      <th>longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>172</th>
      <td>6.6</td>
      <td>90.45606</td>
      <td>9</td>
      <td>24.97433</td>
      <td>121.5431</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">intercept</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">intercept</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-16.240516720277654
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(
\text{predicted}(price) \text{       } =   \text{coefficient}_{\text{house_age}} * \text{house_age}  \\ + \text{ }\text{coefficient}_{\text{distance_station}} * \text{distance_station} \\  + \text{ } \text{coefficient}_{\text{num_stores}} * \text{num_stores}  \\ + \text{ } \text{coefficient}_{\text{latitude}} * \text{latitude}
\\ + \text{ } \text{coefficient}_{\text{longitude}} * \text{longitude} \\ + \text{ } \text{intercept}
\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">ridge_coeffs</span> <span class="o">*</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>172    52.356055
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>All of these feature values multiplied by the coefficients then adding the intercept, contribute to our prediction.</p>
<p>When we do this by hand using the model’s coefficients and intercept, we get the same as if we used <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p>
</div>
</div>
<div class="section" id="let-s-practice">
<h2><span class="section-number">7.5. </span>Let’s Practice<a class="headerlink" href="#let-s-practice" title="Permalink to this headline">¶</a></h2>
<p>1. What is the name of a well-known <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> hyperparameter?<br />
2. What value of this hyperparameter makes it equivalent to using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>?</p>
<p>3. Use the following equation to answer the questions below:</p>
<p><span class="math notranslate nohighlight">\( \text{predicted(backpack_weight)} =  3.02 * \text{#laptops} + 0.3 * \text{#pencils} + 0.5 \)</span></p>
<p>What is our intercept value?</p>
<p>4. If I had 2 laptops 3 pencils in my backpack, what weight would my model predict for my backpack?</p>
<p><strong>True or False:</strong></p>
<p>5. Ridge is a regression modelling approach.<br />
6. Increasing the hyperparameter from Question 1 increases model complexity.<br />
7. <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> can be used with datasets that have multiple features.<br />
8. With <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, we learn one coefficient per training example.<br />
9. Coefficients can help us interpret our model even if unscaled.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solutions!</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
<li><p>0</p></li>
<li><p>0.5</p></li>
<li><p>7.44</p></li>
<li><p>True</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
</ol>
</div>
</div>
<div class="section" id="logistic-regression">
<h2><span class="section-number">7.6. </span>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>Next, we are going to introduce to you a new model called <strong>logistic regression</strong>.</p>
<p>It’s very similar to <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> we saw earlier but this one has some key differences.</p>
<p>For one, we can use it with classification instead of regression problems. (Super confusing, we know -&gt; in stats ‘regression’ means something else)</p>
<a class="reference internal image-reference" href="../_images/confused.png"><img alt="../_images/confused.png" src="../_images/confused.png" style="width: 50%;" /></a>
<p>Ridge = linear model for regression<br />
Logistic regression = linear model for classification.</p>
<p>We are going to bring back our cities dataset we saw at the beginning of this course.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span>

<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>160</th>
      <td>-76.4813</td>
      <td>44.2307</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>127</th>
      <td>-81.2496</td>
      <td>42.9837</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>169</th>
      <td>-66.0580</td>
      <td>45.2788</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>188</th>
      <td>-73.2533</td>
      <td>45.3057</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>187</th>
      <td>-67.9245</td>
      <td>47.1652</td>
      <td>Canada</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Although we have not always been doing this, we should always be building a baseline model before we do any type of meaningful modelling.</p>
<p>Let’s do that before we get straight into it so we can have a better idea of how well our model performs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="n">dc</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>
<span class="n">dc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5988023952095808
</pre></div>
</div>
</div>
</div>
<p>We import <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> from the <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code> library as we did with <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s make a pipeline and obtain the cross-validation scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(),</span> 
                        <span class="n">StandardScaler</span><span class="p">(),</span>
                       <span class="n">LogisticRegression</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">log_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.007766</td>
      <td>0.001932</td>
      <td>0.852941</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.006833</td>
      <td>0.001637</td>
      <td>0.823529</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.006129</td>
      <td>0.002038</td>
      <td>0.696970</td>
      <td>0.858209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.006212</td>
      <td>0.001670</td>
      <td>0.818182</td>
      <td>0.828358</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.006026</td>
      <td>0.001632</td>
      <td>0.939394</td>
      <td>0.805970</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.006593
score_time     0.001782
test_score     0.826203
train_score    0.829335
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>This time we can see that our training and cross-validation scores have increased from those of our <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code>.</p>
<p>We saw that with SVMs and decision trees that  we could visualize our model with decision boundaries and we can do the same thing with logistic regression.</p>
<p>Here, we can see we get a line(*cough <strong>Linear</strong> *cough) that separates our two target classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">log_pipe</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">16</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">16</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic Regression&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_74_0.png" src="../_images/lecture7_74_0.png" />
</div>
</div>
<p>Note:
With the regression plots we saw earlier we could only show a 1 feature problem since we need one dimension to show the target value.</p>
<p>With classification, we can show the target value with colour instead of using the y-axis so we can use 2 features instead.</p>
<p>If we look at some other models that we did this in comparison for you can understand a bit more on why we call Logistic Regression a “linear classifier”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;KNN&quot;</span><span class="p">:</span> <span class="n">KNeighborsClassifier</span><span class="p">(),</span>    
    <span class="s2">&quot;RBF SVM&quot;</span><span class="p">:</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">),</span>    
    <span class="s2">&quot;Logistic Regression&quot;</span><span class="p">:</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">();</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">();</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_77_0.png" src="../_images/lecture7_77_0.png" />
</div>
</div>
<p>Notice a linear decision boundary (a line in our case).</p>
<div class="section" id="coefficients">
<h3><span class="section-number">7.6.1. </span>Coefficients<a class="headerlink" href="#coefficients" title="Permalink to this headline">¶</a></h3>
<p>Just like we saw for <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>. we can get the equation of that line and the coefficients of our <code class="docutils literal notranslate"><span class="pre">latitude</span></code> and <code class="docutils literal notranslate"><span class="pre">longitude</span></code> features using <code class="docutils literal notranslate"><span class="pre">.coef_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logr_step_pipe</span> <span class="o">=</span> <span class="n">log_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;logisticregression&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model coefficients:&quot;</span><span class="p">,</span> <span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept:&quot;</span><span class="p">,</span> <span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model coefficients: [[-0.72330355 -1.64254763]]
Model intercept: [-0.30837315]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;features&#39;</span><span class="p">:</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s1">&#39;coefficients&#39;</span><span class="p">:</span><span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>longitude</td>
      <td>-0.723304</td>
    </tr>
    <tr>
      <th>1</th>
      <td>latitude</td>
      <td>-1.642548</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this case, we see that both are negative coefficients.</p>
<p>We also can see that the coefficient of latitude is larger in magnitude than that of longitude.</p>
<p>This makes a lot of sense because Canada as a country lies above the USA and so we expect <code class="docutils literal notranslate"><span class="pre">latitude</span></code> values to contribute more to a prediction than <code class="docutils literal notranslate"><span class="pre">longitude</span></code> which Canada and the <code class="docutils literal notranslate"><span class="pre">USA</span></code> have quite similar values.</p>
</div>
<div class="section" id="predictions">
<h3><span class="section-number">7.6.2. </span>Predictions<a class="headerlink" href="#predictions" title="Permalink to this headline">¶</a></h3>
<p>Before we said that larger coefficients “contribute” more to the prediction than smaller ones for <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>. What is the analog for logistic regression?</p>
<p>With logistic regression, the model randomly assigns one of the classes as a positive class and the other as negative.</p>
<p>Here since “Canada” comes first when we call <code class="docutils literal notranslate"><span class="pre">.classes_</span></code> it is the “negative class and <code class="docutils literal notranslate"><span class="pre">USA</span></code> is the positive class. (This is in alphabetical order).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>When it comes to the coefficients, when there is a positive coefficient, increasing that feature will make our prediction more positive which means our prediction is going to lean more toward the positive class (in this case <code class="docutils literal notranslate"><span class="pre">USA</span></code>).</p>
<p>Ok, let’s take an example from our test set and calculate the outcome using our coefficients and intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
<span class="n">example</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-64.8001, 46.098]
</pre></div>
</div>
</div>
</div>
<p>We can do this the exact same way as we did for Ridge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">example</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">*</span> <span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">logr_step_pipe</span><span class="o">.</span><span class="n">intercept_</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-29.15639126])
</pre></div>
</div>
</div>
</div>
<p>We get a value of -29.15639126.</p>
<p>What does that mean? I thought we were predicting a class?</p>
<p>For logistic regression, we check the <strong>sign</strong> of the calculation only.</p>
<p>If the result was positive, it predicts one class; if negative, it predicts the other.</p>
<p>That means everything negative corresponds to “Canada” and everything positive predicts a class of “USA”.</p>
<p>Since it’s negative we predict <code class="docutils literal notranslate"><span class="pre">Canada</span></code>, which is our negative class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict</span></code>, we can see that it predicts the negative class as well!</p>
<p>These are “hard predictions” but we can also use this for something called “soft predictions” as well. (Remember <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>? That’s going to be coming back in a second)</p>
</div>
<div class="section" id="logistic-regression-hyperparameter-c">
<h3><span class="section-number">7.6.3. </span>Logistic Regression Hyperparameter <code class="docutils literal notranslate"><span class="pre">C</span></code><a class="headerlink" href="#logistic-regression-hyperparameter-c" title="Permalink to this headline">¶</a></h3>
<p>At this point, you should be feeling pretty comfortable with hyperparameters.</p>
<p>We saw that <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> has the hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, well <code class="docutils literal notranslate"><span class="pre">C</span></code> (annoyingly) has the opposite effect on the fundamental trade-off.</p>
<p>In general, we say smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> leads to a less complex model (whereas with <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, lower <code class="docutils literal notranslate"><span class="pre">alpha</span></code> means higher complexity).</p>
<p>Higher values of <code class="docutils literal notranslate"><span class="pre">C</span></code> leads to more overfitting and lower values to less overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span><span class="p">{</span>
<span class="s2">&quot;C&quot;</span> <span class="p">:</span><span class="mf">10.0</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="s2">&quot;train_score&quot;</span> <span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="s2">&quot;cv_score&quot;</span> <span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]:</span>
    <span class="n">lr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;train_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s1">&#39;cv_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>train_score</th>
      <th>cv_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000001</td>
      <td>0.598810</td>
      <td>0.598930</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000010</td>
      <td>0.598810</td>
      <td>0.598930</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000100</td>
      <td>0.664707</td>
      <td>0.658645</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.001000</td>
      <td>0.784424</td>
      <td>0.790731</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.010000</td>
      <td>0.827842</td>
      <td>0.826203</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.100000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.000000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10.000000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>’s default <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter is 1.</p>
<p>Let’s see what kind of value we get if we do <code class="docutils literal notranslate"><span class="pre">RandomizedGrid</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;logisticregression__C&quot;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)}</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">log_pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 10 candidates, totalling 50 fits
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;logisticregression__C&#39;: 71.5182105727749}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8201426024955436
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="predicting-probabilities">
<h3><span class="section-number">7.6.4. </span>Predicting probabilities<a class="headerlink" href="#predicting-probabilities" title="Permalink to this headline">¶</a></h3>
<p>we saw that we can make “hard predictions” with logistic regression using <code class="docutils literal notranslate"><span class="pre">predict</span></code> but logistic regression also can make something called “soft predictions”.</p>
<p>We saw this when we use <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> in naive Bayes. These are called “soft predictions” because instead of predicting a specific class, the model returns a probability for each class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>I just want to state here we are using our pipeline to make a prediction but we could have also used our <code class="docutils literal notranslate"><span class="pre">grid_search</span></code> object that calls <code class="docutils literal notranslate"><span class="pre">log_pipe</span></code> to make the prediction as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>We could also have called a simple model without the scaling if we did:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>And now with<code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pipe</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.86175442, 0.13824558]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87868202, 0.12131798]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87848688, 0.12151312]])
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the last one where we predict with <code class="docutils literal notranslate"><span class="pre">lr</span></code>:</p>
<p>So these “probabilities” correspond to the classes in the same order as <code class="docutils literal notranslate"><span class="pre">.classes_</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>This returns an array with a probability of how confident the model is for each target class.</p>
<p>We can see that the model is 87.8% sure that example 1 is class 0 (“Canada”) and 12.15% confident that example 1 is class 0 (“USA”).</p>
<p>We are going to call these values a probability <em>score</em>. It is a score that takes the form of a probability. Take it with a grain of salt.</p>
<p>We don’t want to say “I am 88% sure that this example is ‘Canada’” That’s too in-depth here, but we can say that we are more sure of one class than another.</p>
<p><code class="docutils literal notranslate"><span class="pre">predict</span></code> works by predicting the class with the highest probability.</p>
</div>
<div class="section" id="how-is-this-being-done">
<h3><span class="section-number">7.6.5. </span>How is this being done?<a class="headerlink" href="#how-is-this-being-done" title="Permalink to this headline">¶</a></h3>
<p>For linear regression we used something like this:</p>
<p><span class="math notranslate nohighlight">\(\text{predicted(value)} = \text{coefficient}_\text{feature-1} * \text{feature-1} + \text{coefficient}_\text{feature-2} * \text{feature-2} + ... + \text{coefficient}_\text{feature-n} * \text{feature-n} + \text{intercept} \)</span></p>
<p>But this won’t work with probabilities.</p>
<p>So how do we calculate these probability scores?</p>
<p>We need something that will:</p>
<ol class="simple">
<li><p>Make our predictions bounded between 0 and 1 (since probabilities as between 0 and 1)</p></li>
<li><p>Make our predictions change rapidly around 0.5 (the threshold) and slower away from 0.5</p></li>
</ol>
<p>Enter -&gt; <strong>The sigmoid function</strong>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">raw_model_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;--k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;--k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">16</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">16</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;raw model output&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;predicted probability&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;The sigmoid function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_114_0.png" src="../_images/lecture7_114_0.png" />
</div>
</div>
<p>If we now compare <code class="docutils literal notranslate"><span class="pre">predict</span></code> with <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> we can see how <code class="docutils literal notranslate"><span class="pre">predict</span></code> made a prediction based on the probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_y</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">predict_y</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;Canada&#39;, &#39;USA&#39;, &#39;Canada&#39;, &#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_proba</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_proba</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.69848481, 0.30151519],
       [0.76970638, 0.23029362],
       [0.05301712, 0.94698288],
       [0.63294488, 0.36705512],
       [0.81540165, 0.18459835]])
</pre></div>
</div>
</div>
</div>
<p>Here we can look at the first column and if the probability is greater than 0.5, our <code class="docutils literal notranslate"><span class="pre">predict</span></code> function predicts the target of Canada and if the probability is lower than 0.5, it predicts <code class="docutils literal notranslate"><span class="pre">USA</span></code>.</p>
<p>Let’s take a look and compare them to the actual correct labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">y_train</span><span class="p">,</span> 
             <span class="s2">&quot;pred y&quot;</span><span class="p">:</span> <span class="n">predict_y</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
             <span class="s2">&quot;probabilities&quot;</span><span class="p">:</span> <span class="n">y_proba</span><span class="o">.</span><span class="n">tolist</span><span class="p">()}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>pred y</th>
      <th>probabilities</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>96</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7047596510140418, 0.2952403489859582]</td>
    </tr>
    <tr>
      <th>57</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.03121394423109436, 0.9687860557689056]</td>
    </tr>
    <tr>
      <th>123</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.6537036743991862, 0.3462963256008138]</td>
    </tr>
    <tr>
      <th>106</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.8444267867198362, 0.1555732132801638]</td>
    </tr>
    <tr>
      <th>83</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.6537036743991862, 0.3462963256008138]</td>
    </tr>
    <tr>
      <th>17</th>
      <td>USA</td>
      <td>Canada</td>
      <td>[0.6984848138411375, 0.3015151861588626]</td>
    </tr>
    <tr>
      <th>98</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.769706381275301, 0.23029361872469897]</td>
    </tr>
    <tr>
      <th>66</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.053017116268726405, 0.9469828837312736]</td>
    </tr>
    <tr>
      <th>126</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.6329448842395046, 0.36705511576049543]</td>
    </tr>
    <tr>
      <th>109</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.8154016516676702, 0.1845983483323298]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can see that in the example at index 17, the model incorrectly predicted as “Canada” instead of “USA” but we also see that the model was not extremely confident in this prediction. It was 69.8% confident.</p>
<p>For the rest of this selection, the model corrected predicted each city but the model was more confident in some than others.</p>
</div>
<div class="section" id="decision-boundaries-with-predict-proba">
<h3><span class="section-number">7.6.6. </span>Decision boundaries with <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code><a class="headerlink" href="#decision-boundaries-with-predict-proba" title="Permalink to this headline">¶</a></h3>
<p>When we use <code class="docutils literal notranslate"><span class="pre">predict</span></code>, we get a decision boundary with either blue or red background a colour for each class we are predicting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="n">y_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic regression - predict&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="n">y_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">proba</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic regression - predict_proba&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_122_0.png" src="../_images/lecture7_122_0.png" />
</div>
</div>
<p>With probabilities using <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>,  we can now use a colour to represent the probability, a scale.<br />
We can see that the model is less confident the closer the observations are to the decision boundary.</p>
<p>Let’s find some examples where the model is pretty confident in its predictions.</p>
<p>This time, when we make our dataframe, we are only bringing in the probability of predicting “Canada”. This is because if we are 10% confident a prediction is “Canada”, the model is 90% confident in “USA”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_targets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">y_train</span><span class="p">,</span>
                           <span class="s2">&quot;pred y&quot;</span><span class="p">:</span> <span class="n">predict_y</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                           <span class="s2">&quot;probability_canada&quot;</span><span class="p">:</span> <span class="n">y_proba</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()})</span>
<span class="n">lr_targets</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>pred y</th>
      <th>probability_canada</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>160</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.704607</td>
    </tr>
    <tr>
      <th>127</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.563017</td>
    </tr>
    <tr>
      <th>169</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.838968</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_targets</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;probability_canada&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>pred y</th>
      <th>probability_canada</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>37</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.006547</td>
    </tr>
    <tr>
      <th>78</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.007685</td>
    </tr>
    <tr>
      <th>34</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.008317</td>
    </tr>
    <tr>
      <th>41</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.008958</td>
    </tr>
    <tr>
      <th>38</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.009194</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.924004</td>
    </tr>
    <tr>
      <th>81</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.931792</td>
    </tr>
    <tr>
      <th>0</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.932487</td>
    </tr>
    <tr>
      <th>165</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.951092</td>
    </tr>
    <tr>
      <th>1</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.961902</td>
    </tr>
  </tbody>
</table>
<p>167 rows × 3 columns</p>
</div></div></div>
</div>
<p>Here we can see both extremes.</p>
<p>At the bottom are the observations the model is most confident the class is Canada, and at the top, we can see the observations the model was least confident the class is Canada which is the same saying the most confident in USA.</p>
<p>We are 99.345% (1- 0.006547) confident that city 37 is “USA” and 96.19% confident that city 1 is “Canada”.</p>
<p>The model got the first example right, but the second one, wrong.</p>
<p>Let’s plot this and see why.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">37</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>-134.4197</td>
      <td>58.3019</td>
      <td>USA</td>
    </tr>
    <tr>
      <th>37</th>
      <td>-98.4951</td>
      <td>29.4246</td>
      <td>USA</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I am get each of these observations by calling the index of each city on our training dataset.</p>
<p>The top one is index 37 and the bottom one is index 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">37</span><span class="p">]],</span> <span class="n">y_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">37</span><span class="p">]],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">proba</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="n">lims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">140</span><span class="p">,</span><span class="o">-</span><span class="mi">55</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">60</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic regression - certain predictions&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_129_0.png" src="../_images/lecture7_129_0.png" />
</div>
</div>
<p>Both points are “USA” cities but we can now see why the model was so confident in its predictions for both cities.</p>
<p>The “USA” city it got wrong is likely in Alaska but the model doesn’t know that and predicts more so on how close and on which side it lies to the decision boundary.</p>
<p>Let’s now find an example where the model is less certain on its prediction.</p>
<p>We can do this by finding the absolute value of the difference between the two probabilities.</p>
<p>The smaller the value, the more uncertain the model is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_targets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">y_train</span><span class="p">,</span>
                           <span class="s2">&quot;pred y&quot;</span><span class="p">:</span> <span class="n">predict_y</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                           <span class="s2">&quot;prob_difference&quot;</span><span class="p">:</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y_proba</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_proba</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()})</span>
<span class="n">lr_targets</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;prob_difference&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>pred y</th>
      <th>prob_difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>61</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.001719</td>
    </tr>
    <tr>
      <th>54</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.020025</td>
    </tr>
    <tr>
      <th>13</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.020025</td>
    </tr>
    <tr>
      <th>130</th>
      <td>Canada</td>
      <td>USA</td>
      <td>0.022234</td>
    </tr>
    <tr>
      <th>92</th>
      <td>Canada</td>
      <td>USA</td>
      <td>0.022234</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Here we can see that city 61 and 54 have the model pretty stumped.</p>
<p>Let’s plot them and see why.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">61</span><span class="p">,</span> <span class="mi">54</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>61</th>
      <td>-87.9225</td>
      <td>43.0350</td>
      <td>USA</td>
    </tr>
    <tr>
      <th>54</th>
      <td>-83.0466</td>
      <td>42.3316</td>
      <td>USA</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_classifier</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">61</span><span class="p">,</span> <span class="mi">54</span><span class="p">]],</span> <span class="n">y_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">61</span><span class="p">,</span> <span class="mi">54</span><span class="p">]],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">proba</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="n">lims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">115</span><span class="p">,</span><span class="o">-</span><span class="mi">55</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">60</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span> <span class="mi">14</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic regression - uncertain prediction&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7_134_0.png" src="../_images/lecture7_134_0.png" />
</div>
</div>
<p>Plot the cities with the decision boundary, helps us understand why.</p>
<p>The cities lie almost completely on the boundary, this makes the model very divided on how to classify them.</p>
</div>
</div>
<div class="section" id="limitations-of-linear-classifiers">
<h2><span class="section-number">7.7. </span>Limitations of linear classifiers<a class="headerlink" href="#limitations-of-linear-classifiers" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Is your data “linearly separable”? Can you draw a hyperplane between these datapoints that separates them with low error?</p></li>
<li><p>If the training examples can be separated by a linear decision rule, they are <strong>linearly separable</strong>.</p></li>
</ul>
<p>… but sometimes are data just can’t be linearly separated well and hence these models will not perform well.</p>
</div>
<div class="section" id="compare-to-naive-bayes">
<h2><span class="section-number">7.8. </span>Compare to naive Bayes<a class="headerlink" href="#compare-to-naive-bayes" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Both are simple, fast, probabilistic classifiers.</p></li>
<li><p>Both work well with large numbers of features.</p></li>
<li><p>Naive Bayes has overly strong conditional independence assumptions. So it is not great when features are correlated.</p></li>
<li><p>Logistic regression is much more robust to correlated features.</p></li>
</ul>
</div>
<div class="section" id="id1">
<h2><span class="section-number">7.9. </span>Let’s Practice<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>1. We have the following text, which we wish to classify as either a positive or negative movie review.<br />
Using the words below (which are features in our model) with associated coefficients, answer the next 2 questions.<br />
The input for the feature value is the number of times the word appears in the review.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Word</p></th>
<th class="head"><p>Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>excellent</p></td>
<td><p>2.2</p></td>
</tr>
<tr class="row-odd"><td><p>disappointment</p></td>
<td><p>-2.4</p></td>
</tr>
<tr class="row-even"><td><p>flawless</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-odd"><td><p>boring</p></td>
<td><p>-1.3</p></td>
</tr>
<tr class="row-even"><td><p>unwatchable</p></td>
<td><p>-1.7</p></td>
</tr>
</tbody>
</table>
<p>Intercept = 1.3</p>
<p>What value do you calculate after using the weights in the model above for the above review?</p>
<p><em><strong>I thought it was going to be excellent but instead, it was unwatchable and boring.</strong></em></p>
<p>The input feature value would be the number of times the word appears in the review (like <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>).</p>
<p>2. Would the model classify this review as a positive or negative review (classes are specified alphabetically) ?<br />
We are trying to predict if a job applicant would be hired based on some features contained in their resume.<br />
Below we have the output of <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code> where column 0 shows the probability the model would predict “hired” and column 1 shows the probability the model would predict “not hired”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span><span class="mf">0.04971843</span><span class="p">,</span> <span class="mf">0.95028157</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.94173513</span><span class="p">,</span> <span class="mf">0.05826487</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.74133975</span><span class="p">,</span> <span class="mf">0.25866025</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.13024982</span><span class="p">,</span> <span class="mf">0.86975018</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.17126403</span><span class="p">,</span> <span class="mf">0.82873597</span><span class="p">]])</span>
</pre></div>
</div>
<p>Use this output to answer the following questions.</p>
<p>3. If we had used <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> for these examples instead of <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code>, how many of the examples would the model have predicted “hired”?<br />
4. If the true class labels are below, how many examples would the model have correctly predicted with <code class="docutils literal notranslate"><span class="pre">predict()</span></code>?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;hired&#39;</span><span class="p">,</span> <span class="s1">&#39;hired&#39;</span><span class="p">,</span> <span class="s1">&#39;hired&#39;</span><span class="p">,</span> <span class="s1">&#39;not hired&#39;</span><span class="p">,</span> <span class="s1">&#39;not hired&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>True or False:</strong><br />
5. Increasing logistic regression’s <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter increases the model’s complexity.<br />
6. Unlike with <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> regression, coefficients are not interpretable with logistic regression.<br />
7.  <code class="docutils literal notranslate"><span class="pre">predict</span></code> returns the positive class if the predicted probability of the positive class is greater than 0.5.<br />
8. In logistic regression, a function is applied to convert the raw model output into probabilities.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solutions!</p>
<ol class="simple">
<li><p>0.5</p></li>
<li><p>Positive review</p></li>
<li><p>2</p></li>
<li><p>4</p></li>
<li><p>True</p></li>
<li><p>False</p></li>
<li><p>True</p></li>
<li><p>True</p></li>
</ol>
</div>
</div>
<div class="section" id="let-s-practice-coding">
<h2><span class="section-number">7.10. </span>Let’s Practice - Coding<a class="headerlink" href="#let-s-practice-coding" title="Permalink to this headline">¶</a></h2>
<p>Let’s import the Pokémon dataset from our <code class="docutils literal notranslate"><span class="pre">data</span></code> folder. We want to see how well our model does with logistic regression. Let’s try building a simple model with default parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pk_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/pokemon.csv&#39;</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">pk_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;legendary&#39;</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;legendary&#39;</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;legendary&#39;</span><span class="p">])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;legendary&#39;</span><span class="p">]</span>


<span class="n">numeric_features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;attack&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;defense&quot;</span> <span class="p">,</span>
                    <span class="s2">&quot;sp_attack&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;sp_defense&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;speed&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;capture_rt&quot;</span><span class="p">]</span>

<span class="n">drop_features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;deck_no&quot;</span><span class="p">,</span> <span class="s2">&quot;gen&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;total_bs&quot;</span><span class="p">]</span>

<span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">),</span>
    <span class="n">StandardScaler</span><span class="p">())</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="s2">&quot;drop&quot;</span><span class="p">,</span> <span class="n">drop_features</span><span class="p">),</span>
    <span class="p">(</span><span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>Build and fit a pipeline containing the column transformer and a logistic regression model using the parameter class_weight=”balanced” (you will learn about this in lecture 9!).</p></li>
<li><p>Score your model on the test set.</p></li>
<li><p>Find the model’s feature coefficients and answer the below questions
a. Which feature contributes the most in predicting if an example is legendary or not.
b.As the capture rate value increases, will the model more likely predict a legendary or not legendary Pokémon?</p></li>
</ol>
</div>
<div class="section" id="what-we-ve-learned-today">
<h2><span class="section-number">7.11. </span>What We’ve Learned Today<a class="headerlink" href="#what-we-ve-learned-today" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The name of the function used to bound our values between 0 and 1</p></li>
<li><p>How <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> can be particularly useful when it comes to Logistic Regression.</p></li>
<li><p>The advantages and limitations of linear classifiers.</p></li>
<li><p>How to use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> classifier.</p></li>
<li><p>One of the hyperparameters of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> (<code class="docutils literal notranslate"><span class="pre">alpha</span></code>)</p></li>
<li><p>One of the hyperparameters of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> (<code class="docutils literal notranslate"><span class="pre">C</span></code>).</p></li>
<li><p>How logistic regression is compared to naive Bayes.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "bait509-ubc/BAIT509",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="lecture6.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">6. </span>Naive Bayes and Hyperparameter Optimization</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="lecture8.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">8. </span>Business Objectives/Statistical Questions and Feature Selection</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Hayley Boyce<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>