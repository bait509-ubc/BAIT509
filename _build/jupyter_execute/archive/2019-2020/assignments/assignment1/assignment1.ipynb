{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Lectures 1 - 4. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "__Attribution__: This assignment was created by Tomas Beuzen and Vincenzo Coia.\n",
    "\n",
    "## Tidy Submission (5%)\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- You must use proper English, spelling, and grammar.\n",
    "- You will submit two things to Canvas:\n",
    "    1. This jupyter notebook file containing your responses; and,\n",
    "    2. A html file of your completed notebook (use `jupyter nbconvert --to html_embed assignment.ipynb` in the terminal to generate the html file).\n",
    "- Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/courses/35074) by **11:59pm Monday 20th January**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals (worth a total of 35%)\n",
    "\n",
    "\n",
    "Here we will attempt classification of the famous handwritten digits data set. This data set exists in many forms; but we will use the one bundled in `sklearn.datasets` in Python. You can read more about the data [here](https://scikit-learn.org/stable/datasets/index.html#digits-dataset).\n",
    "\n",
    "Use the following cell to load and extract the data into features (`X`) and target (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits['data']    # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "y = digits['target']  # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/icics-user/Documents/BAIT509/BAIT509/_build/jupyter_execute/archive/2019-2020/assignments/assignment1/assignment1_4_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title(f\"This is a {digits['target'][idx]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals (15%)\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to the digit 0 (`X[0]`): the digit 1 (`X[1]`) or the digit 2 (`X[2]`)? Report the two Euclidean distances (hint: you might find the [sklearn function euclidean_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) useful here).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error (10%)\n",
    "\n",
    "You'll be using a [*k*-NN classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for this question.\n",
    "\n",
    "Using `k=20`, fit a $k$-NN classifier with `X` and `Y` using all of the data as your training data. Then, obtain the predictions of `X`. \n",
    "\n",
    "1. What proportion of these predictions are **incorrect**? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error (10%)\n",
    "\n",
    "Now fit the classifier using `k=1`, using all of your data as training data, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error (worth a total of 20%)\n",
    "\n",
    "In lectures, we explored how the value of $k$ could influence the train/test error. In this question, we will explore how the partition of train/test data can influence error.\n",
    "\n",
    "Choose any value of $k$ between 1 and 10. For different partitions of the full data set into training and testing sets (e.g., 10%/90%, 20%/80%, 30%/70%, etc.), obtain training and test error rates. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Decision Trees (worth a total of 40%)\n",
    "\n",
    "We'll be using the famous Titanic dataset to try and predict survival of passengers (`Survival`) from `Age`, `Fare`, and `Sex`, using a decision tree classifier. You can find the data in the assignment folder on Canvas. Details of the data can be found at https://www.kaggle.com/c/titanic/data. You will need to select only the columns above from the full dataset provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) Feature pre-processing (15%)\n",
    "\n",
    "1. Load the data into a pandas dataframe using the `pd.read_csv()` function. How many observations are there in the data? Remove all rows from the dataframe that contain a `NaN` value using the function `df.dropna()`. How many observations are there now?\n",
    "2. Use one-hot-encoding to encode the `Sex` column to numeric values (don't forget to drop the first column using the argument `drop=\"first\"` and to specify `sparse=False`). Ouput the head of the transformed dataframe.\n",
    "3. Is it necessary to scale the numeric features for use in your decision tree classifier? In 1-2 sentences, briefly explain your answer.\n",
    "4. Split the data into 80% training, 20% testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Hyperparameter optimization (10%)\n",
    "\n",
    "1. Using 10-fold cross validation and the training set only, find an appropriate value for the `max_depth` hyperparameter for a decision tree classifier. Make a plot of training error and cross-validation error for different values of `max_depth`.\n",
    "2. In 1-2 sentences, briefly discuss what sections of your plot likely represent a model that is well fit to the data and which sections represent a model that is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Final model (15%)\n",
    "\n",
    "Based on your results from 3(b) select a value for the hyperparameter `max_depth`. In 1-2 sentences briefly explain why you chose this particular value. Train a decision tree classifier using the training set and your chosen `max_depth` hyperparameter and then obtain the test error on the test set."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}