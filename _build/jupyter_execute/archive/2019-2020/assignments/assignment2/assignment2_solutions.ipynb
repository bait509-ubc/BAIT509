{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 2\n",
    "\n",
    "__Evaluates__: Lectures 1 - 6. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "__Attribution__: This assignment was created by Tomas Beuzen and Vincenzo Coia.\n",
    "\n",
    "## Tidy Submission (5%)\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- You must use proper English, spelling, and grammar.\n",
    "- You will submit two things to Canvas:\n",
    "    1. This jupyter notebook file containing your responses; and,\n",
    "    2. A html file of your completed notebook (use `jupyter nbconvert --to html_embed assignment.ipynb` in the terminal to generate the html file).\n",
    "- Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/courses/35074) by **11:59pm Monday 27th January**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Revision of concepts (worth a total of 20%)\n",
    "The following questions relate to material covered in lectures 1-4. Respond to the questions without using code. Provide clear and concise (1-3 sentence) answers to any written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 (5%)\n",
    "\n",
    "Briefly explain what are the train, validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 1.1</font>\n",
    "\n",
    "- Training: used to fit/learn the model.\n",
    "- Validation: used to optimize the model (e.g., by tuning hyperparameters).\n",
    "- Testing: used as a final test of the model performance (not to be used at all to develop/select the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 (5%)\n",
    "\n",
    "Consider the hypothetical situation where we have two-predictors ($X_1$ and $X_2$) that we have split into 5 groups (A, B, C, D, E). Which of the following partitions of the predictor space into the 5 groups correspond to a decision tree model -- Figure 1 or Figure 2? What is the first split in the tree?\n",
    "\n",
    "<img src='fig1.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 1.2</font>\n",
    "\n",
    "Figure 1 corresponds to a decision tree.\n",
    "\n",
    "The first split in the tree is at $X_1 = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (5%)\n",
    "\n",
    "The following figure is a scatterplot of 10 observations labelled either **x** or **o**, along with one unlabelled observation indicated by a **?**. In this question you are to answer the following using k-Nearest Neighbours:\n",
    "\n",
    "1. Classify ? with k=1.\n",
    "2. Classify ? with k=3.\n",
    "3. Classify ? with k=10.\n",
    "\n",
    "<img src='fig2.png' width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 1.3</font>\n",
    "\n",
    "1. O\n",
    "2. X\n",
    "3. X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 (5%)\n",
    "\n",
    "Briefly describe what is wrong with the following code (assume all packages required to runt he code have already been loaded):\n",
    "\n",
    "```\n",
    "# Load the data from dataset.csv\n",
    "df = pd.read_csv('dataset.csv', index_col=0)\n",
    "X = df.drop(columns=['response'])\n",
    "y = df[['response']]\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "                                                    \n",
    "# Create a KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 1.4</font>\n",
    "\n",
    "The problem with this code is that the data is scaled before splitting into training and testing portions. This means that the (future) test data is being used to fit the StandardScaler which violates the golden rule. In other words, we are using the test data to help us scale the data which is bad practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Logistic regression (worth a total of 35%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will work with a logistic regression model. Recall that logistic regression, despite the name, is used for classification tasks. Typically it is used to model the relationship between a binary target variable and one or more numeric or categorical features.\n",
    "\n",
    "In this exercise we will be focusing on predicting the presence or absence of heart disease in a patient based on a set of 13 different biophysical measures. The classification of heart disease in patients is obviously of great importance for cardiovascular disease diagnosis and prevention. Machine learning offers novel and potentially effective methods of forming predictive models from heart disease data, and this particular dataset was an early example of how data and machine learning can be leveraged to create incredibly effective predictive models. You can download and read more about the original dataset on the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). A slightly modified version of this dataset has been made available to you for this assignment called `heart_disease.csv`. You will see that it contains 303 observations (patients) and 14 columns (the 13 features and 1 target variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (10%)\n",
    "\n",
    "Your first task is to wrangle this dataset into a format suitable for use with the scikit-learn library. This includes:\n",
    "\n",
    "1. Loading the dataset;\n",
    "2. Feature preprocessing (one-hot encoding and scaling); and,\n",
    "3. Splitting data into training and testing sets.\n",
    "\n",
    "This dataset has both numeric and categorical features which makes the feature preprocessing step a little harder. There is an sklearn function called `ColumnTransformer` that helps you to apply numeric feature preprocessing (e.g., `StandardScaler` and categorical feature preprocessing (e.g., `OneHotEncoder`) simultaneously. To help you understand the data wrangling process, the code required to perform the pre-processing tasks above is provided. The code has been arranged into five blocks performing the tasks above but these blocks are in the wrong order. **Rearrange the code below to correctly wrangle the data and add a short, one-line comment to each block to describe what the code is doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, you don't need to rearrange this cell!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4526ffd90ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# YOUR COMMENT HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m X_train = pd.DataFrame(preprocessor.fit_transform(X_train),\n\u001b[0m\u001b[1;32m      5\u001b[0m                        \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        columns=(numeric_features +\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Rearrange this cell to get it to run and properly wrangle the heart disease data\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train),\n",
    "                       index=X_train.index,\n",
    "                       columns=(numeric_features +\n",
    "                                list(preprocessor.named_transformers_['ohe']\n",
    "                                     .get_feature_names(categorical_features))))\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                      index=X_test.index,\n",
    "                      columns=X_train.columns)\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "numeric_features = ['age', 'resting_blood_pressure', 'cholesterol',\n",
    "                    'max_heart_rate_achieved', 'st_depression', 'num_major_vessels']\n",
    "categorical_features = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg',\n",
    "                        'exercise_induced_angina', 'st_slope', 'thalassemia']\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), numeric_features),\n",
    "        ('ohe', OneHotEncoder(drop=\"first\"), categorical_features)])\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "heart_df = pd.read_csv('heart_disease.csv', index_col=0)\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(heart_df.drop(columns='target'),\n",
    "                                                    heart_df[['target']],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 2.1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data\n",
    "heart_df = pd.read_csv('heart_disease.csv', index_col=0)\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(heart_df.drop(columns='target'),\n",
    "                                                    heart_df[['target']],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)\n",
    "\n",
    "\n",
    "# 2. Identify the categorical and numeric columns\n",
    "numeric_features = ['age', 'resting_blood_pressure', 'cholesterol',\n",
    "                    'max_heart_rate_achieved', 'st_depression', 'num_major_vessels']\n",
    "categorical_features = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg',\n",
    "                        'exercise_induced_angina', 'st_slope', 'thalassemia']\n",
    "\n",
    "# 3. Set-up data transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), numeric_features),\n",
    "        ('ohe', OneHotEncoder(drop=\"first\"), categorical_features)])\n",
    "\n",
    "\n",
    "# 5. Apply data transformations and convert to dataframe\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train),\n",
    "                       index=X_train.index,\n",
    "                       columns=(numeric_features +\n",
    "                                list(preprocessor.named_transformers_['ohe']\n",
    "                                     .get_feature_names(categorical_features))))\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                      index=X_test.index,\n",
    "                      columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (5%)\n",
    "\n",
    "Train a logistic regression model on the training data (using default hyperparameter settings) and calculate the model's error on the training data and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 2.2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on training data = 0.14\n",
      "Error on test data = 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tbeuzen/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tbeuzen/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Error on training data = {1 - model.score(X_train, y_train):.2f}\")\n",
    "print(f\"Error on test data = {1 - model.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (5%)\n",
    "\n",
    "Based on your results from Q2.2 would you say your logisitic regression model is overfit? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 2.3</font>\n",
    "The model does not appear to be overfit as the training and testing scores are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (10%)\n",
    "\n",
    "Recall that a logistic regression model outputs a probability between 0 and 1, where, by default, probabilities less than 0.5 are assigned to class 0 and probabilites greater than 0.5 are assigned to class 1. The predictions of the logistic regression model can be calculated using the `predict_proba()` method and the particular classes the predictions refer to can be obtained from the `classes_` attribute.\n",
    "\n",
    "1. What is the predicted probability that the first observation in the test set (patient_id 11) has heart disease (target = 1)?\n",
    "2. What is the predicted probability that the first observation in the test set (patient_id 11) does not have heart disease (target = 0)?\n",
    "3. What patient ID has the highest predicted probability of heart disease (give the actual patient_id number, not the index location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 2.4</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No heart disease</th>\n",
       "      <td>0.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heart disease</th>\n",
       "      <td>0.9708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Probability\n",
       "No heart disease       0.0292\n",
       "Heart disease          0.9708"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of heart disease for the first observation in the test set (patient_id=11)\n",
    "pd.DataFrame(data = model.predict_proba(X_test)[0],\n",
    "             index = ['No heart disease', 'Heart disease'],\n",
    "             columns = ['Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>max_heart_rate_achieved</th>\n",
       "      <th>st_depression</th>\n",
       "      <th>num_major_vessels</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>chest_pain_type_non-anginal pain</th>\n",
       "      <th>fasting_blood_sugar_lower than 120mg/ml</th>\n",
       "      <th>rest_ecg_normal</th>\n",
       "      <th>exercise_induced_angina_yes</th>\n",
       "      <th>st_slope_flat</th>\n",
       "      <th>st_slope_upsloping</th>\n",
       "      <th>thalassemia_normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-1.714983</td>\n",
       "      <td>-2.329621</td>\n",
       "      <td>-0.892243</td>\n",
       "      <td>1.271564</td>\n",
       "      <td>-0.904358</td>\n",
       "      <td>-0.71659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  resting_blood_pressure  cholesterol  \\\n",
       "patient_id                                                  \n",
       "124        -1.714983               -2.329621    -0.892243   \n",
       "\n",
       "            max_heart_rate_achieved  st_depression  num_major_vessels  \\\n",
       "patient_id                                                              \n",
       "124                        1.271564      -0.904358           -0.71659   \n",
       "\n",
       "            sex_male  chest_pain_type_non-anginal pain  \\\n",
       "patient_id                                               \n",
       "124              0.0                               1.0   \n",
       "\n",
       "            fasting_blood_sugar_lower than 120mg/ml  rest_ecg_normal  \\\n",
       "patient_id                                                             \n",
       "124                                             1.0              0.0   \n",
       "\n",
       "            exercise_induced_angina_yes  st_slope_flat  st_slope_upsloping  \\\n",
       "patient_id                                                                   \n",
       "124                                 0.0            0.0                 0.0   \n",
       "\n",
       "            thalassemia_normal  \n",
       "patient_id                      \n",
       "124                        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are a few ways we can find the patient with the highest probability of heart disease\n",
    "# Below I first find the location of the maximum probability using np.argmax\n",
    "# I then use that location to index the relevant dataframe row using .iloc\n",
    "# It's patient 124\n",
    "patient_id = np.argmax(model.predict_proba(X_test)[:, 1])\n",
    "X_test.iloc[[patient_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 (5%)\n",
    "\n",
    "Recall that we can investigate the coefficient values of our logistic regression model to help understand the importance of the different features. Information of the coefficients is exposed by the `coef_` attribute of your model. Which feature appears to have the most influence on the prediction of heart disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 2.5</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex_male</th>\n",
       "      <td>-1.064904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_major_vessels</th>\n",
       "      <td>-0.702799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rest_ecg_normal</th>\n",
       "      <td>-0.649979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_depression</th>\n",
       "      <td>-0.566281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercise_induced_angina_yes</th>\n",
       "      <td>-0.553031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_slope_flat</th>\n",
       "      <td>-0.372431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <td>-0.269019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasting_blood_sugar_lower than 120mg/ml</th>\n",
       "      <td>-0.176224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.093953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol</th>\n",
       "      <td>-0.082226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_heart_rate_achieved</th>\n",
       "      <td>0.506874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_slope_upsloping</th>\n",
       "      <td>0.562205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thalassemia_normal</th>\n",
       "      <td>1.200342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest_pain_type_non-anginal pain</th>\n",
       "      <td>1.550086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             coef\n",
       "sex_male                                -1.064904\n",
       "num_major_vessels                       -0.702799\n",
       "rest_ecg_normal                         -0.649979\n",
       "st_depression                           -0.566281\n",
       "exercise_induced_angina_yes             -0.553031\n",
       "st_slope_flat                           -0.372431\n",
       "resting_blood_pressure                  -0.269019\n",
       "fasting_blood_sugar_lower than 120mg/ml -0.176224\n",
       "age                                     -0.093953\n",
       "cholesterol                             -0.082226\n",
       "max_heart_rate_achieved                  0.506874\n",
       "st_slope_upsloping                       0.562205\n",
       "thalassemia_normal                       1.200342\n",
       "chest_pain_type_non-anginal pain         1.550086"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are looking for the coefficient with the largest absolute value\n",
    "# We see that the feature chest_pain_type_non-anginal pain has the biggest\n",
    "# influence over the probability of heart disease with a value of 1.55\n",
    "pd.DataFrame(data = model.coef_.T,\n",
    "             index = X_train.columns,\n",
    "             columns = [\"coef\"]).sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Naive Bayes for sentiment analysis (40%) <a name=\"3\"></a>\n",
    "\n",
    "Naive Bayes is popular in text classification tasks. In this exercise you will use \n",
    "the Naive Bayes algorithm to conduct [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis), which is a problem of assigning positive or negative labels to text based on the sentiment (attitude) expressed within it. [Here](https://www.youtube.com/watch?v=uXu2uEubV9Q&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=33) is a short video explaining the task of sentiment analysis. \n",
    "\n",
    "We will be using a dataset of movie reviews for this exercise. The dataset is know as the IMDB movie review data set and is available on [kaggle](https://www.kaggle.com/utathya/imdb-review-dataset). It contains 100,000 different movie reviews, labelled as either being positive or negative. The file is quite large (around 129 mb) so please download it directly from kaggle (simply click on the button towards the top of the screen that says `Download (129mb)`), you may need to create a free account to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 (10%)\n",
    "\n",
    "1. Load the data file you downloaded into a pandas DataFrame called `imdb_df` (hint: you will need to use the arguments `index_col=0` and `encoding = \"ISO-8859-1\"` in `pd.read_csv` to open this particular csv file correctly);\n",
    "2. Drop the columns `\"type\"` and `\"file\"` from the dataframe;\n",
    "3. Notice that in the `\"labels\"` columns there are three possible values: `pos`, `neg`, and `unsup`. Discard rows with the `unsup` label from the dataframe. There are several ways to perform this operation, I like to use the `query()` function, but feel free to use any method you wish.\n",
    "\n",
    "Your final dataframe should have two columns and 50,000 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_master.csv', index_col=0, encoding = \"ISO-8859-1\")\n",
    "df = df.drop(columns = ['type', 'file'])\n",
    "df = df.query('label == \"neg\" | label == \"pos\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (5%)\n",
    "Split the data into train (80%) and test (20%) sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], \n",
    "                                                    df['label'], \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (5%)\n",
    "\n",
    "The current data is in the form of movie reviews (text paragraphs) and their targets (`pos` or `neg`). \n",
    "We need to encode the movie reviews into feature vectors so that we can train supervised machine learning models with `scikit-learn`. We will use sklearn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to transform the text data into a *bag-of-words* representation (i.e., when text is represented as a vector of counts, disregarding word order and grammar). Your tasks are to:\n",
    "\n",
    "1. Create a `CountVectorizer` object.\n",
    "2. Call `CountVectorizer`'s `fit_transform` method on the train split of the movie review data to get the features for the train set.\n",
    "3. Call `CountVectorizer`'s `transform` method on the test split to get the features for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)  \n",
    "X_test = vectorizer.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (5%)\n",
    "\n",
    "1. Fit a [multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) model to the training data.\n",
    "2. Report the train and test errors of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.4</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(f'Training error: {1 - model.score(X_train, y_train):.2f}')\n",
    "print(f'    Test error: {1- model.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 (5%)\n",
    "\n",
    "When using sklearn's `CountVectorizer` text is represented as a vector of counts. Do you think this is an adequate representation of the information contained with the text? When dealing with text data, what other information might be important for making accurate predictions with a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.5</font>\n",
    "`CountVectorizer` does not consider word order, grammar or how frequently a partiuclar word occurs in the whole training set (words that occur many times in a dataset may not be as informative as words that only occur a few times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 (10%)\n",
    "\n",
    "Below is an additional movie review not included in the original IMDB dataset.\n",
    "\n",
    "1. Do you think this review is positive or negative overall?\n",
    "2. Use your Naive Bayes model to predict the sentiment of this review. What is the prediction?\n",
    "3. Does you model predict the sentiment you expected? If not, why do you think that might be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ['''It could have been a great movie. It could have been excellent, \n",
    "          and to all the people who have forgotten about the older, \n",
    "          greater movies before it, will think that as well. \n",
    "          It does have beautiful scenery, some of the best since Lord of the Rings. \n",
    "          The acting is well done, and I really liked the son of the leader of the Samurai.\n",
    "          He was a likeable chap, and I hated to see him die...\n",
    "          But, other than all that, this movie is nothing more than hidden rip-offs.\n",
    "          ''']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">Solution 3.5</font>\n",
    "1. The review appears negative overall. But there are a few positives in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "prediction = model.predict(vectorizer.transform(review))\n",
    "print(f\"The Naive Bayes model predicts that this review is: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The model does not predict the senitment we expect, it predicts positive when the overall review is negative. It seems to fail for more complex examples, where understanding the context and overall text is essential to correctly classify reviews. The last example has many positive words in the beginning but the last sentence negates all positivity in the previous text. We need to incorporate deeper linguistic knowledge to correctly classify such cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}