{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 2\n",
    "\n",
    "__Evaluates__: Lectures 1 - 6. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "__Attribution__: This assignment was created by Tomas Beuzen and Vincenzo Coia.\n",
    "\n",
    "## Tidy Submission (5%)\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- You must use proper English, spelling, and grammar.\n",
    "- You will submit two things to Canvas:\n",
    "    1. This jupyter notebook file containing your responses; and,\n",
    "    2. A html file of your completed notebook (use `jupyter nbconvert --to html_embed assignment.ipynb` in the terminal to generate the html file).\n",
    "- Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/courses/35074) by **11:59pm Monday 27th January**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Revision of concepts (worth a total of 20%)\n",
    "The following questions relate to material covered in lectures 1-4. Respond to the questions without using code. Provide clear and concise (1-3 sentence) answers to any written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 (5%)\n",
    "\n",
    "Briefly explain what are the train, validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 (5%)\n",
    "\n",
    "Consider the hypothetical situation where we have two-predictors ($X_1$ and $X_2$) that we have split into 5 groups (A, B, C, D, E). Which of the following partitions of the predictor space into the 5 groups correspond to a decision tree model -- Figure 1 or Figure 2? What is the first split in the tree?\n",
    "\n",
    "<img src='fig1.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (5%)\n",
    "\n",
    "The following figure is a scatterplot of 10 observations labelled either **x** or **o**, along with one unlabelled observation indicated by a **?**. In this question you are to answer the following using k-Nearest Neighbours:\n",
    "\n",
    "1. Classify ? with k=1.\n",
    "2. Classify ? with k=3.\n",
    "3. Classify ? with k=10.\n",
    "\n",
    "<img src='fig2.png' width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 (5%)\n",
    "\n",
    "Briefly describe what is wrong with the following code (assume all packages required to runt he code have already been loaded):\n",
    "\n",
    "```\n",
    "# Load the data from dataset.csv\n",
    "df = pd.read_csv('dataset.csv', index_col=0)\n",
    "X = df.drop(columns=['response'])\n",
    "y = df[['response']]\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "                                                    \n",
    "# Create a KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Logistic regression (worth a total of 35%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will work with a logistic regression model. Recall that logistic regression, despite the name, is used for classification tasks. Typically it is used to model the relationship between a binary target variable and one or more numeric or categorical features.\n",
    "\n",
    "In this exercise we will be focusing on predicting the presence or absence of heart disease in a patient based on a set of 13 different biophysical measures. The classification of heart disease in patients is obviously of great importance for cardiovascular disease diagnosis and prevention. Machine learning offers novel and potentially effective methods of forming predictive models from heart disease data, and this particular dataset was an early example of how data and machine learning can be leveraged to create incredibly effective predictive models. You can download and read more about the original dataset on the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). A slightly modified version of this dataset has been made available to you for this assignment called `heart_disease.csv`. You will see that it contains 303 observations (patients) and 14 columns (the 13 features and 1 target variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (10%)\n",
    "\n",
    "Your first task is to wrangle this dataset into a format suitable for use with the scikit-learn library. This includes:\n",
    "\n",
    "1. Loading the dataset;\n",
    "2. Feature preprocessing (one-hot encoding and scaling); and,\n",
    "3. Splitting data into training and testing sets.\n",
    "\n",
    "This dataset has both numeric and categorical features which makes the feature preprocessing step a little harder. There is an sklearn function called `ColumnTransformer` that helps you to apply numeric feature preprocessing (e.g., `StandardScaler` and categorical feature preprocessing (e.g., `OneHotEncoder`) simultaneously. To help you understand the data wrangling process, the code required to perform the pre-processing tasks above is provided. The code has been arranged into five blocks performing the tasks above but these blocks are in the wrong order. **Rearrange the code below to correctly wrangle the data and add a short, one-line comment to each block to describe what the code is doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, you don't need to rearrange this cell!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4526ffd90ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# YOUR COMMENT HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m X_train = pd.DataFrame(preprocessor.fit_transform(X_train),\n\u001b[0m\u001b[1;32m      5\u001b[0m                        \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        columns=(numeric_features +\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Rearrange this cell to get it to run and properly wrangle the heart disease data\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train),\n",
    "                       index=X_train.index,\n",
    "                       columns=(numeric_features +\n",
    "                                list(preprocessor.named_transformers_['ohe']\n",
    "                                     .get_feature_names(categorical_features))))\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                      index=X_test.index,\n",
    "                      columns=X_train.columns)\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "numeric_features = ['age', 'resting_blood_pressure', 'cholesterol',\n",
    "                    'max_heart_rate_achieved', 'st_depression', 'num_major_vessels']\n",
    "categorical_features = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg',\n",
    "                        'exercise_induced_angina', 'st_slope', 'thalassemia']\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), numeric_features),\n",
    "        ('ohe', OneHotEncoder(drop=\"first\"), categorical_features)])\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "heart_df = pd.read_csv('heart_disease.csv', index_col=0)\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(heart_df.drop(columns='target'),\n",
    "                                                    heart_df[['target']],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (5%)\n",
    "\n",
    "Train a logistic regression model on the training data (using default hyperparameter settings) and calculate the model's error on the training data and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (5%)\n",
    "\n",
    "Based on your results from Q2.2 would you say your logisitic regression model is overfit? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (10%)\n",
    "\n",
    "Recall that a logistic regression model outputs a probability between 0 and 1, where, by default, probabilities less than 0.5 are assigned to class 0 and probabilites greater than 0.5 are assigned to class 1. The predictions of the logistic regression model can be calculated using the `predict_proba()` method and the particular classes the predictions refer to can be obtained from the `classes_` attribute.\n",
    "\n",
    "1. What is the predicted probability that the first observation in the test set (patient_id 11) has heart disease (target = 1)?\n",
    "2. What is the predicted probability that the first observation in the test set (patient_id 11) does not have heart disease (target = 0)?\n",
    "3. What patient ID has the highest predicted probability of heart disease (give the actual patient_id number, not the index location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 (5%)\n",
    "\n",
    "Recall that we can investigate the coefficient values of our logistic regression model to help understand the importance of the different features. Information of the coefficients is exposed by the `coef_` attribute of your model. Which feature appears to have the most influence on the prediction of heart disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Naive Bayes for sentiment analysis (40%) <a name=\"3\"></a>\n",
    "\n",
    "Naive Bayes is popular in text classification tasks. In this exercise you will use \n",
    "the Naive Bayes algorithm to conduct [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis), which is a problem of assigning positive or negative labels to text based on the sentiment (attitude) expressed within it. [Here](https://www.youtube.com/watch?v=uXu2uEubV9Q&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=33) is a short video explaining the task of sentiment analysis. \n",
    "\n",
    "We will be using a dataset of movie reviews for this exercise. The dataset is know as the IMDB movie review data set and is available on [kaggle](https://www.kaggle.com/utathya/imdb-review-dataset). It contains 100,000 different movie reviews, labelled as either being positive or negative. The file is quite large (around 129 mb) so please download it directly from kaggle (simply click on the button towards the top of the screen that says `Download (129mb)`), you may need to create a free account to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 (10%)\n",
    "\n",
    "1. Load the data file you downloaded into a pandas DataFrame called `imdb_df` (hint: you will need to use the arguments `index_col=0` and `encoding = \"ISO-8859-1\"` in `pd.read_csv` to open this particular csv file correctly);\n",
    "2. Drop the columns `\"type\"` and `\"file\"` from the dataframe;\n",
    "3. Notice that in the `\"labels\"` columns there are three possible values: `pos`, `neg`, and `unsup`. Discard rows with the `unsup` label from the dataframe. There are several ways to perform this operation, I like to use the `query()` function, but feel free to use any method you wish.\n",
    "\n",
    "Your final dataframe should have two columns and 50,000 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (5%)\n",
    "Split the data into train (80%) and test (20%) sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (5%)\n",
    "\n",
    "The current data is in the form of movie reviews (text paragraphs) and their targets (`pos` or `neg`). \n",
    "We need to encode the movie reviews into feature vectors so that we can train supervised machine learning models with `scikit-learn`. We will use sklearn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to transform the text data into a *bag-of-words* representation (i.e., when text is represented as a vector of counts, disregarding word order and grammar). Your tasks are to:\n",
    "\n",
    "1. Create a `CountVectorizer` object.\n",
    "2. Call `CountVectorizer`'s `fit_transform` method on the train split of the movie review data to get the features for the train set.\n",
    "3. Call `CountVectorizer`'s `transform` method on the test split to get the features for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (5%)\n",
    "\n",
    "1. Fit a [multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) model to the training data.\n",
    "2. Report the train and test errors of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 (5%)\n",
    "\n",
    "When using sklearn's `CountVectorizer` text is represented as a vector of counts. Do you think this is an adequate representation of the information contained with the text? When dealing with text data, what other information might be important for making accurate predictions with a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 (10%)\n",
    "\n",
    "Below is an additional movie review not included in the original IMDB dataset.\n",
    "\n",
    "1. Do you think this review is positive or negative overall?\n",
    "2. Use your Naive Bayes model to predict the sentiment of this review. What is the prediction?\n",
    "3. Does you model predict the sentiment you expected? If not, why do you think that might be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ['''It could have been a great movie. It could have been excellent, \n",
    "          and to all the people who have forgotten about the older, \n",
    "          greater movies before it, will think that as well. \n",
    "          It does have beautiful scenery, some of the best since Lord of the Rings. \n",
    "          The acting is well done, and I really liked the son of the leader of the Samurai.\n",
    "          He was a likeable chap, and I hated to see him die...\n",
    "          But, other than all that, this movie is nothing more than hidden rip-offs.\n",
    "          ''']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}