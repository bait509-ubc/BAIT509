{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Class meetings 01-04. \n",
    "\n",
    "__Rubrics__: The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [code rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_code.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "## Tidy Submission (5%)\n",
    "\n",
    "- Use either R or python to complete this assignment (or both), by filling out this jupyter notebook.\n",
    "- Submit two things:\n",
    "    1. This jupyter notebook file containing your responses, and\n",
    "    2. A pdf or html file of your completed notebook (in jupyter, go to File -> Download as).\n",
    "- Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/) by the deadline.\n",
    "- You must use proper English, spelling, and grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals (20%)\n",
    "\n",
    "\n",
    "Here we will try classification of the famous handwritten digits data set. \n",
    "\n",
    "This data set exists in many forms; we will use the one bundled in `sklearn.datasets` in python. We suggest you also use `sklearn` for classification, but you can use R if you'd like.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the documentation for the data by running `print(digits['DESCR'])`. We'll extract the features and labels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits['data'] # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "Y = digits['target'] # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'This is a 9')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/icics-user/Documents/BAIT509/BAIT509/_build/jupyter_execute/archive/2018-2019/assessments/assignment1/assignment1_6_1.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title('This is a %d' % digits['target'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to element 0 (`X[0]`) -- element 1 (`X[1]`) or element 2 (`X[2]`)? Report the two distances (Euclidean).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error\n",
    "\n",
    "You'll be using a $k$-NN classifier. Documentation for python is available [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html); for R, [here](https://stat.ethz.ch/R-manual/R-patched/library/class/html/knn.html).\n",
    "\n",
    "Using `k=10`, fit a $k$-NN classifier using `X` and `Y` using all of the data as your training data. Obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error\n",
    "\n",
    "Now fit the classifier using `k=1`, using all of your data as training data, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error (15%)\n",
    "\n",
    "This is a continuation of Exercise 1. Each part asks you to investigate some scenario.\n",
    "\n",
    "__Note__: For this specific data set, you might not be able to overfit with $k$-NN! So don't worry if you can't find an example of overfitting.\n",
    "\n",
    "__Attribution__: This exercise was adapted from DSCI 571.\n",
    "\n",
    "__Choose *ONE* of 2(a) or 2(b) to complete.__ If you did both, we'll grade 2(a) only (or 2(b) if you say so).\n",
    "\n",
    "### 2(a) The influence of k\n",
    "\n",
    "**You do not have to do this question if you completed 2(b)**\n",
    "\n",
    "Now, split the data into _training_ and _test_ sets. You can choose any reasonable fraction for training vs. testing (50% will do). \n",
    "\n",
    "__Note__: It's always a good idea to randomly shuffle the data before splitting, in case the data comes ordered in some way. (For example, if they are ordered by label, then your training set will be all the digits 0-4, and your test set all the digits 5-9, which would be bad... you might end up with 100% error!!) To shuffle your data, you can use [`numpy.random.shuffle`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html) in python, or [`sample_n`](https://dplyr.tidyverse.org/reference/sample.html) in R.\n",
    "\n",
    "For various values of $k$, fit (a.k.a. _train_) a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? Make a plot to show the trends, and briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) The influence of data partition\n",
    "\n",
    "**You do not have to do this question if you completed 2(a)**\n",
    "\n",
    "Now, choose your favourite value of $k$, but vary the proportion of data reserved for the training set, again obtaining training and test error rates for each partition of the data. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Loess (50%)\n",
    "\n",
    "We'll use the Titanic data set to try and predict survival of passengers (`Survival`) from `Age`, `Fare`, and `Sex`, using loess. You might find it useful to log-transform `Fare`. ~~The data have been split into a training and test set in the files `titanic_train.csv` and `titanic_test.csv` in the `data` folder~~ The data can be found [here](https://raw.githubusercontent.com/vincenzocoia/BAIT509/master/assessments/assignment1/data/titanic.csv) as a csv. Details of the data can be found at https://www.kaggle.com/c/titanic/data.\n",
    "\n",
    "Note: To include `Sex` in your model, simply fit a loess model to each of `\"male\"` and `\"female\"`. \n",
    "\n",
    "Here are ways to implement loess in python and R:\n",
    "\n",
    "- [sklearn.neighbors.RadiusNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) in python.\n",
    "- [statsmodels.nonparametric.kernel_regression.KernelReg](http://www.statsmodels.org/stable/generated/statsmodels.nonparametric.kernel_regression.KernelReg.html) in python.\n",
    "- [`loess`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/loess.html) function in R.\n",
    "- [`ggplot2::geom_smooth`](http://ggplot2.tidyverse.org/reference/geom_smooth.html) for loess (and related methods) in R's `ggplot2` plotting package.\n",
    "\n",
    "### 3(a) Scaling\n",
    "\n",
    "Estimate the standard deviations of both (numeric) predictors. Is scaling your data justified? Does your decision also apply to $k$-NN, or is scaling only relevant for loess? If scaling is justified, proceed with scaling by subtracting the mean, then dividing by standard deviation (for each numeric predictor). \n",
    "\n",
    "__Hint:__ Be sure to do the same thing with the test set! Just make sure that the mean and standard deviation you use to do the scaling are of the _training_ set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Regression\n",
    "\n",
    "Fit a loess model to the training data for various values of the bandwidth parameter. Plot the mean squared error (MSE) on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the training error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Classification\n",
    "\n",
    "Like you just did, fit a loess model to the training data for various values of the bandwidth parameter, but then add a classification step: predict survival if the probability of survival is greater than 0.5. Plot the error rate on the training and test sets, and plot these across bandwidth. How does the training error curve differ from the test error curve, and why? From this plot, using the \"validation set approach\" for choosing hyperparameters, what bandwidth is appropriate? Do you get similar results when you considered the MSE in the regression case above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Concepts (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a) Missing Prediction\n",
    "\n",
    "It's possible that loess won't predict anything for a certain observation on the test set. In what situation will this happen, and why? Could this also be the case for $k$-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4(b) Fundamental tradeoff\n",
    "\n",
    "How do the bandwidth and $k$ hyperparameters in loess and $k$-NN influence the bias/variance tradeoff, or the overfitting/underfitting tradeoff? Use one or two brief sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}