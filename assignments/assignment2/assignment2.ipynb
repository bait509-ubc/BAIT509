{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 2: Preprocessing, Pipelines and Hyperparameter Tuning\n",
    "\n",
    "__Evaluates__: Lectures 4 - 6. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions and how the TAs will grade your answers. See the following links for more details:\n",
    "\n",
    "- [mechanics_rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_mech.md): submit an assignment correctly.\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md): evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md): evaluating your written responses.\n",
    "- [autograde rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_autograde.md): evaluating questions that are either right or wrong (can be done either manually or automatically).\n",
    "\n",
    "## Tidy Submission \n",
    "rubric={mechanics:2}\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- Any place you see `...` or `____`, you must fill in the function, variable, or data to complete the code.\n",
    "- Use proper English, spelling, and grammar.\n",
    "- You will submit two files on Canvas:\n",
    "    1. This jupyter notebook file containing your responses ( an `.ipynb` file); and,\n",
    "    2. An `.html` file of your completed notebook that will render directly on Canvas without having to be downloaded.\n",
    "        - To generate this html file you can click `File` -> `Export Notebook As` -> `HTML` in JupyterLab or type the following into a terminal `jupyter nbconvert --to html_embed assignment.ipynb`).\n",
    "    \n",
    "Submit your assignment through UBC Canvas by the deadline listed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction and learning goals <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "Welcome to the assignment! In this assignment, you will practice:\n",
    "\n",
    "- Identify when to implement feature transformations such as imputation and scaling.\n",
    "- Apply `sklearn.pipeline.Pipeline` to build a machine learning pipeline.\n",
    "- Use `sklearn` for applying numerical feature transformations on the data.\n",
    "- Identify when it's appropriate to apply ordinal encoding vs one-hot encoding.\n",
    "- Explain strategies to deal with categorical variables with too many categories.\n",
    "- Use `ColumnTransformer` to build all our transformations together into one object and use it with `scikit-learn` pipelines.\n",
    "- Carry out hyperparameter optimization using `sklearn`'s `GridSearchCV` and `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "A crucial step when using machine learning algorithms on real-world datasets is preprocessing. This assignment will give you some practice to build a preliminary supervised machine learning pipeline on a real-world dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introducing and Exploring the dataset <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In this assignment, you will be working on a sample of [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#) that we provide as `census.csv`. We have made some modifications to this data so that it's easier to work with. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. You can find more information on the dataset and features [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "\n",
    "*Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "census_df = pd.read_csv(\"census.csv\")\n",
    "census_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data splitting \n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "To avoid violation of the golden rule, the first step before we do anything is splitting the data. \n",
    "\n",
    "Split the data into `train_df` (80%) and `test_df` (20%). Keep the target column (`income`) in the splits so that we can use it in EDA. \n",
    "Please use `random_state=893`, so that your results are consistent with what we expect.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our `train_df`,\n",
    "you can just follow along for the next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like things are in order,\n",
    "but there is a hidden gotcha with this dataframe.\n",
    "Let's look at the unique values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML  # This step is just to avoid the long columns being truncated as \"...\"\n",
    "\n",
    "HTML(\n",
    "    train_df\n",
    "    .select_dtypes(object)\n",
    "    .apply(lambda x: sorted(pd.unique(x)))\n",
    "    .to_frame()\n",
    "    .to_html()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are question marks in the columns \"workclass\", \"occupation\", and \"native_country\".\n",
    "Unfortunately it seems like the people collecting this data used a non-conventional way to indicate missing/unknown values\n",
    "instead of using the standard blank/NaN.\n",
    "Our first step would be to do this conversion manually,\n",
    "so that `?` is not interpreted as an actual value by our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_df_nan = train_df.replace(\"?\", np.nan)\n",
    "test_df_nan = test_df.replace(\"?\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Describing your data\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Use `.describe()` to show summary statistics of each feature in the `train_df_nan` dataframe.\n",
    "Show numerical and categorical columns separately,\n",
    "either using the lists we created above,\n",
    "or by reading the docstring to describe to figure out how to limit which column types are shown.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Identifying potentially important features\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "We have provided you with some code that will visualize the distributions of all the numeric and categorical features in the census data.\n",
    "Study the visualizations below\n",
    "to suggest which features you think seem relevant\n",
    "for the given prediction task of building a model to identify who makes over and under 50k.\n",
    "List these features and briefly explain your rationale in why you have selected them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec805c0117b4b350452b5ce348969cf0",
     "grade": true,
     "grade_id": "cell-e30b9dc7eb3f328f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.disable_max_rows()  # Allows us to plot big datasets\n",
    "\n",
    "alt.Chart(train_df.sort_values('income')).mark_bar(opacity=0.6).encode(\n",
    "    alt.X(alt.repeat(), type='quantitative', bin=alt.Bin(maxbins=50)),\n",
    "    alt.Y('count()', stack=None),\n",
    "    alt.Color('income')\n",
    ").properties(\n",
    "    height=200\n",
    ").repeat(\n",
    "    train_df_nan.select_dtypes('number').columns.to_list(),\n",
    "    columns=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(train_df.sort_values('income')).mark_bar(opacity=0.6).encode(\n",
    "    alt.X(alt.repeat(), type='nominal'),\n",
    "    alt.Y('count()', stack=None),\n",
    "    alt.Color('income')\n",
    ").properties(\n",
    "    height=200\n",
    ").repeat(\n",
    "    train_df_nan.select_dtypes('object').columns.to_list(),\n",
    "    columns=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Separating feature vectors and targets  \n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Create `X_train`, `y_train`, `X_test`, `y_test` from `train_df_nan` and `test_df_nan`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "If you train [`sklearn`'s `SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on `X_train` and `y_train` at this point, would it work? Why or why not?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d076ae59049112bffa7aeaa9eceb27d2",
     "grade": true,
     "grade_id": "cell-34bd7adf5001db84",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Preprocessing <a name=\"3\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll be wrangling the dataset so that it's suitable to be used with `scikit-learn` classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Identifying transformations that need to be applied\n",
    "rubric={reasoning:7}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Identify the columns on which transformations need to be applied and tell us what transformation you would apply in what order by filling in the table below. Example transformations are shown for the feature `age` in the table.  \n",
    "\n",
    "Note that for this problem, no ordinal encoding will be executed on this dataset. \n",
    "\n",
    "Are there any columns that you think should be dropped from the features? If so, explain your answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Transformation |\n",
    "| --- | ----------- |\n",
    "| age | imputation, scaling |\n",
    "| workclass |  |\n",
    "| fnlwgt |  |\n",
    "| education |  |\n",
    "| education_num |  |\n",
    "| marital_status |  |\n",
    "| occupation |  |\n",
    "| relationship |  |\n",
    "| race |  |\n",
    "| sex |  |\n",
    "| capital_gain |  |\n",
    "| capital_loss |  |\n",
    "| hours_per_week |  |\n",
    "| native_country |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37cd9cdb78425cf0683ae4be0b482e28",
     "grade": true,
     "grade_id": "cell-d729b4bd1f878816",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numeric vs. categorical features\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Since we will apply different preprocessing steps on the numerical and categorical columns,\n",
    "we first need to identify the numeric and categorical features and create lists for each of them\n",
    "(make sure not to include the target column).\n",
    "\n",
    "*Save the column names as string elements in each of the corresponding list variables below*\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [...]\n",
    "categorical_features = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Numeric feature pipeline\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Let's start making our pipelines. Use `make_pipeline()` or `Pipeline()` to make a pipeline for the numeric features called `numeric_transformer`. \n",
    "This pipeline will only have one step, \n",
    "the `StandardScaler()`,\n",
    "so technically we didn't need to make a pipeline,\n",
    "but it is good to be in the habit of working with pipelines\n",
    "and it also gives us the option to name this step if we want.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Categorical feature pipeline\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Next, make a pipeline for the categorical features called `categorical_transformer`. \n",
    "To keep things simple,\n",
    "we will impute on all columns,\n",
    "including those where we did not find missing values in the training data.\n",
    "Use `SimpleImputation()` with `strategy='most_frequent'`. \n",
    "Add a OneHotEncoder as the second step and configure it to ignore unknown values in the test data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 ColumnTransformer\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Create a column transformer that applies our numeric pipeline transformations to the numeric feature columns\n",
    "and our categorical pipeline transformations to the categorical feature columns.\n",
    "Assign this columns transformer to the variable `preprocessor`.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3: Building a Model <a name=\"4\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have preprocessed features, we are ready to build models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dummy Classifier\n",
    "rubric={accuracy:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Now that we have our preprocessing pipeline setup,\n",
    "let's move on to the model building.\n",
    "First,\n",
    "it's important to build a dummy classifier to establish a baseline score to compare our model to.\n",
    "Make a `DummyClassifier` that predicts the most common label, train it, and then score it on the training and test sets\n",
    "(in two separate cells so that both scores are displayed).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Main pipeline\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Define a main pipeline that transforms all the different features and uses an `SVC` model with default hyperparameters. \n",
    "If you are using `Pipeline` instead of `make_pipeline`, name each of your steps `columntransformer` and `svc` respectively. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipe = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Hyperparameter tuning/optimization\n",
    "\n",
    "rubric={accuracy:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Now that we have our pipelines and a model, let's tune the hyperparameters `gamma` and `C`.\n",
    "For this tuning,\n",
    "construct a grid where each hyperparameter can take the values `0.1, 1, 10, 100`\n",
    "and randomly search for the best combination.\n",
    "\n",
    "To save some running time on your laptops,\n",
    "use 3-fold crossvalidation to evaluate each result\n",
    "and only search for 7 iterations,\n",
    "and set `n_jobs=-1`.\n",
    "Return the train and testing score,\n",
    "set `random_state=289`,\n",
    "and optionally `verbose=2` if you want to see information as the search is occurring.\n",
    "Don't forget to fit the best model from the `RandomizedSearchCV` object\n",
    "on all the training data as the final step.\n",
    "\n",
    "*This search is quite demanding computationally so be prepared for this to take 2 or 3 minutes and your fan may start to run!*\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Choosing your hyperparameters\n",
    "rubric={accuracy:2, reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "We are displaying the results from the random hyperparameter search\n",
    "as a dataframe below.\n",
    "Looking at this table,\n",
    "which values for `gamma` and `C` would you choose for your final model and why? \n",
    "You can answer this by either manually by using the table\n",
    "or by accessing the corresponding attributes from the random search object.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[[\"params\", \"mean_test_score\", \"mean_train_score\", \"rank_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b31308009d9e35e98e105d7364a86159",
     "grade": true,
     "grade_id": "cell-d13f8946874949e1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating on the test set <a name=\"5\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have a best-performing model, it's time to assess our model on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Scoring your final model\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "What is the training and test score of the best scoring model? \n",
    "Score the model in two separate cells so that both the training and test scores are displayed.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Assessing your model\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Compare your final model accuracy with your baseline model from question 3.1,\n",
    "do you consider that our model is performing better than the baseline to such as extent that you would prefer it on deployment data?\n",
    "\n",
    "Briefly describe one aspect of our model development in this notebook that either supports your confidence in the model we have,\n",
    "or one possible improvement to what we did here that you think could have increased our model score.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc94b693e4ba2265a23f3409aec968bc",
     "grade": true,
     "grade_id": "cell-4f9d9a0f9cd44cf8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Canvas\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Convert your notebook to .html format by going to File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Upload your `.ipynb` file and the `.html` file to Canvas under Assignment1. \n",
    "- **DO NOT** upload any `.csv` files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing Assignment 2! Now you are ready to build a simple ML pipeline on real-world datasets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bait]",
   "language": "python",
   "name": "conda-env-bait-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.986px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
