{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 3: Logistic Regression and Evaluation Metrics  \n",
    "\n",
    "__Evaluates__: Lectures 7 - 9. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions and how the TAs will grade your answers. See the following links for more details:\n",
    "\n",
    "- [mechanics_rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_mech.md): submit an assignment correctly.\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md): evaluating your code.\n",
    "- [autograde rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_autograde.md): evaluating questions that are either right or wrong.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md): evaluating your written responses.\n",
    "\n",
    "## Tidy Submission \n",
    "rubric={mechanics:2}\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- You must use proper English, spelling, and grammar.\n",
    "- You will submit two things to Canvas:\n",
    "    1. This jupyter notebook file containing your responses ( an `.ipynb` file); and,\n",
    "    2. An `.html` file of your completed notebook (use `jupyter nbconvert --to html_embed assignment.ipynb` in the terminal to generate the html file or under `File` -> `Export Notebook As` -> `HTML`).\n",
    "    \n",
    " <br>  \n",
    "\n",
    " Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/courses/58082) by **11:59 pm Monday, May 19th**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions\n",
    "\n",
    "- Places that you see `raise NotImplementedError # No Answer - remove if you provide an answer`. Substitute the `None` above it and replace the `raise NotImplementedError # No Answer - remove if you provide an answer` with your completed code and answers, then proceed to run the cell!\n",
    "\n",
    "- Any place you see `____`, you must fill in the function, variable, or data to complete the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "\n",
    "from scipy.stats import lognorm, loguniform, randint\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import test_assignment3 as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and learning goals <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "Welcome to the assignment! After working on this assignment, you should be able to:\n",
    "\n",
    "- Explain components of a confusion matrix.\n",
    "- Define precision, recall, and f1-score and use them to evaluate different classifiers.\n",
    "- Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- Explain `class_weight` and use it to deal with data imbalance.\n",
    "- Apply different scoring functions with `cross_validate` and `GridSearchCV` and `RandomizedSearchCV`.\n",
    "- Explain the general intuition behind linear models.\n",
    "- Explain the `fit` and `predict` paradigm of linear models.\n",
    "- Use `scikit-learn`'s `LogisticRegression` classifier.\n",
    "    - Use `fit`, `predict` and `predict_proba`.   \n",
    "    - Use `coef_` to interpret the model weights.\n",
    "- Explain the advantages and limitations of linear classifiers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:  Precision, recall, and f1 score \"by hand\" (without `sklearn`) <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "\n",
    "Consider the problem of predicting whether a new product will be successful or not and is worth investing in. Below are confusion matrices of two machine learning models: Model A and Model B. \n",
    "\n",
    "##### Model A\n",
    "|    Actual/Predicted         | Predicted successful| Predicted not successful |\n",
    "| :-------------------------- | ------------------: | -----------------------: |\n",
    "| **Actually successful**     | 3                   | 5                        |\n",
    "| **Actually not successful** | 6                   | 96                       |\n",
    " \n",
    "\n",
    "##### Model B\n",
    "|    Actual/Predicted         | Predicted successful| Predicted not successful |\n",
    "| :-------------------------- | ------------------: | -----------------------: |\n",
    "| **Actually successful**     | 6                   |                        14 |\n",
    "| **Actually not successful** | 0                  |                       90 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Positive vs. negative class\n",
    "rubric={autograde:1, reasoning:1}\n",
    "\n",
    "Precision, recall, and f1 score depend crucially upon which class is considered \"positive\", that is the thing you wish to find. In the example above, which class ( `Actually successful` or `Actually not successful`)  is likely to be the \"positive\" class and why?\n",
    "\n",
    "Save the label name in a string object named `answer_1_1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e65417d318c1179930760ec87245fae1",
     "grade": false,
     "grade_id": "cell-6695096e00d90506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_1_1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82d0804a2c8ab91631e0cca63b289200",
     "grade": true,
     "grade_id": "cell-b1563c701db4e6b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_1(answer_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37d32d9947c9054429de3a331bbfbee4",
     "grade": true,
     "grade_id": "cell-38378b6f41b076ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accuracy\n",
    "rubric={autograde:2}\n",
    "\n",
    "Calculate accuracies for Model A and Model B. \n",
    "\n",
    "Save the values of each calculations as a fraction in objects name `model_a_acc` and `model_b_acc` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddabaa2c0ff13749cac5a98331b2640d",
     "grade": false,
     "grade_id": "cell-19893d9ec734e650",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_a_acc = None\n",
    "model_b_acc = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1941dc246005f14438b7a9dcdba41f41",
     "grade": true,
     "grade_id": "cell-9944db188f1b0a07",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_2_1(model_a_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154c2328e08b7b1d2c9a0bcc22b4616d",
     "grade": true,
     "grade_id": "cell-08b41c4c44238a05",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_2_2(model_b_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model would you pick? \n",
    "rubric={reasoning:1}\n",
    "\n",
    "Which model would you pick simply based on the accuracy metric? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65757ae1fd4c02e5c8299d4c829db697",
     "grade": true,
     "grade_id": "cell-803992eb2449ee91",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model A - Precision, recall, f1-score\n",
    "rubric={accuracy:3}\n",
    "\n",
    "Calculate precision, recall, f1-score for **Model A** by designating the appropriate fraction to objects named `a_precision`, `a_recall` and `a_f1`. \n",
    "\n",
    "You can use the objects `a_precision` and `a_recall` to use in your `a_f1` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f44d85f31dc98fa2d18b0c0504f9b879",
     "grade": false,
     "grade_id": "cell-feae0ff2bfe786c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a_precision = None\n",
    "a_recall = None\n",
    "a_f1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a96d2bc1481df3f135cca8fd72bbc20",
     "grade": true,
     "grade_id": "cell-de8db84ca9dea1d3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_1(a_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c84ddf88f96e11d39a26f8365e93288",
     "grade": true,
     "grade_id": "cell-c3738b2fbc19927d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_2(a_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c855318b43be3c26cd1fc862722a6bc",
     "grade": true,
     "grade_id": "cell-4be62ab64cfd9e10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_3(a_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model B - Precision, recall, f1-score\n",
    "rubric={accuracy:3}\n",
    "\n",
    "Calculate precision, recall, f1-score for **Model B** by designating the appropriate fraction to objects named `b_precision`, `b_recall` and `b_f1`. \n",
    "\n",
    "You can use the objects `b_precision` and `b_recall` to use in your `b_f1` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b7605c24f4950cb7a95e2c64619dc7a",
     "grade": false,
     "grade_id": "cell-2ae8130bd6cb0aa3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "b_precision = None\n",
    "b_recall = None\n",
    "b_f1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "351da277d3498aa59df48bd17c7d36a7",
     "grade": true,
     "grade_id": "cell-bde37c35c611ed2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_1(b_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ce4f4abd028f97d5c4875ba1706fb71",
     "grade": true,
     "grade_id": "cell-35ec3bd3b69cbc26",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_2(b_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b061bb0ed9fa3ade2208cb9d8cad7504",
     "grade": true,
     "grade_id": "cell-e536bb027b7eb1cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_3(b_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Metric choice\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Which metric(s) is more informative in this case? Why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac9071406940947e4e0dbdfc8b0dcfa7",
     "grade": true,
     "grade_id": "cell-bc054f540ff066e4",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Model choice\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Which model would you pick based on this information and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "146ed92f11f875e0cf04bdab5ed75261",
     "grade": true,
     "grade_id": "cell-75dcca9250a65c05",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sentiment analysis on the IMDB dataset: model building <a name=\"3\"></a>\n",
    "<hr>\n",
    "\n",
    "<img src=\"https://ia.media-imdb.com/images/M/MV5BMTk3ODA4Mjc0NF5BMl5BcG5nXkFtZTgwNDc1MzQ2OTE@._V1_.png\"  width = \"40%\" alt=\"404 image\" />\n",
    "\n",
    "In this exercise, you will carry out sentiment analysis on a real corpus, [the IMDB movie review dataset](https://www.kaggle.com/utathya/imdb-review-dataset).\n",
    "The starter code below loads the data CSV file (assuming that it's in the data directory) as a pandas DataFrame called `imdb_df`.\n",
    "\n",
    "The supervised learning task is, given the text of a movie review, to predict whether the review sentiment is positive (reviewer liked the movie) or negative (reviewer disliked the movie). We have done a bit of preprocessing on the dataset already where the positive review are labelled `1` and the negative reviews are labelled `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "imdb_df = pd.read_csv(\"imdb_speed.csv\")\n",
    "train_df, test_df = train_test_split(imdb_df, test_size=0.2, random_state=77)\n",
    "train_df.head()\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature and target objects \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Separate our feature vectors from the target.\n",
    "\n",
    "You will need to do this for both `train_df` and `test_df`.\n",
    "\n",
    "Save the results in objects named `X_train`, `y_train`, `X_test` and `y_test`. \n",
    "\n",
    "(Makes sure that all 4 of these objects are of type Pandas Series. We will be using `CountVectorizer` for future questions and this transformation requires an input of Pandas Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "353d0ef9713013aaf2c3ae8396742961",
     "grade": false,
     "grade_id": "cell-27ddb033928d3ba8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = None, None\n",
    "X_test, y_test = None, None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aedbb0b02d3fcff452beb8bdfe91be1",
     "grade": true,
     "grade_id": "cell-14b906c9264189ef",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dummy classifier\n",
    "rubric={accuracy:3}\n",
    "\n",
    "Make a baseline model using `DummyClassifier`.\n",
    "\n",
    "Carry out cross-validation using the `stratified` strategy. Pass the following `scoring` metrics to `cross_validate`. \n",
    "- accuracy\n",
    "- f1\n",
    "- recall\n",
    "- precision\n",
    "\n",
    "(We are using cross-validation here since we can obtain multiple scores at once) \n",
    "\n",
    "Make sure you use  `return_train_score=True` and 5-fold cross-validation.\n",
    "\n",
    "Save your results in a dataframe named `dummy_scores_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "930709672f1b0c85f6a91e6ce96fbfb2",
     "grade": false,
     "grade_id": "cell-03b5ef93dd1fc50d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores_df = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "\n",
    "dummy_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6941b6a4046b537580e4399dea01508",
     "grade": true,
     "grade_id": "cell-597ce4b7c7e7c075",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_2(dummy_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dummy classifier mean\n",
    "rubric={accuracy:1}\n",
    "\n",
    "What is the mean of each column in `dummy_scores_df`?\n",
    "\n",
    "Save your result in an object named `dummy_mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6be97c13b48f50b205a7929529070322",
     "grade": true,
     "grade_id": "cell-e4826f2524485549",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_mean = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "\n",
    "dummy_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Pipeline\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Let's make a pipeline now. \n",
    "\n",
    "Since we only have 1 column to preprocess, we only need 1 main pipeline for this question. \n",
    "\n",
    "Create a pipeline with 2 steps, one for `CountVectorizer` and one with a `LogisticRegression` model. For the LogisticRegression model, it's a good idea to set the argument `max_iter=1000` to avoid any warnings and convergence issues. Also let's balance the classes in our splits by setting the appropriate argument in `LogisticRegression` to \"balanced\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0971678b338d2a5fcb7f10943eae4ba",
     "grade": true,
     "grade_id": "cell-9efae13b40915487",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hyperparameter optimization\n",
    "rubric={accuracy:4}\n",
    "\n",
    "Use `RandomizedSearchCV` to jointly optimize the hyperparameters in the `params_grid` that we have provided for you. Name your `RandomizedSearchCV` object `random_search` as we have some code we will be giving you in exercise 3 that relies on this object name. \n",
    "\n",
    "Specify `n_iter=10`, `cv=5`, `n_jobs=-1`, `verbose=2`, `return_train_score=True` and **make sure  to use an f1 scoring metric here**. \n",
    "\n",
    "Make sure to fit your model on the training portion of the IMDB dataset. \n",
    "\n",
    "This can take quite a while (2-5 minutes for me!) so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "param_grid = {\n",
    "    \"countvectorizer__max_features\": randint(10, 10000),\n",
    "    \"logisticregression__C\": loguniform(0.01, 100), \n",
    "}\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "547804cad6bbd4564aae6b522347b714",
     "grade": true,
     "grade_id": "cell-907dab64153f296f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "random_search = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6  Best hyperparameters\n",
    "rubric={accuracy:2}\n",
    "\n",
    "What are the best hyperparameter values found by `RandomizedSearchCV` for `C` and `max_features`. \n",
    "What was the corresponding validation score? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b88ffa6067b5f903039f1a6985d5cd0",
     "grade": true,
     "grade_id": "cell-735fc79f5b175920",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_parameters = None\n",
    "optimal_score = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Train and test scores of best scoring model\n",
    "rubric={accuracy:2}\n",
    "\n",
    "What is the train and test `f1` score of the best scoring model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec48958d0c62d6ba23c5d14b40cfd408",
     "grade": true,
     "grade_id": "cell-53ee9d02ef0dfb32",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Hyperparameters and the fundamental tradeoff\n",
    "rubric={reasoning:3}\n",
    "\n",
    "From the set of possible models in the search, did your search return a relatively simple CountVectorizer or a relatively complex one? Did it return a relatively simple LogisticRegression or a relatively complex one? Here ‘simple’ and ‘complex’ we mean with respect to the fundamental tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d21b4335823c5e7d03cf88a4f588784",
     "grade": true,
     "grade_id": "cell-6337f9413e31cf5e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2edfde69914febbf8ddd1d20d64d748b",
     "grade": true,
     "grade_id": "cell-e14d767ed9f1a4c3",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Confusion matrix\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Plot a confusion matrix on the test set using your random search object as your estimator. You may also want to add `display_labels` so that it's easier to recognized which class is a positive review and which is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "708a943fdb89140e16b0d69f29276bf5",
     "grade": true,
     "grade_id": "cell-46b8f78c11bf3f83",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Classification report\n",
    "rubric={accuracy:4}\n",
    "\n",
    "Print a classification report on the `X_test` predictions of your random search object's best model with measurements to 4 decimal places . Use this information to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b14a7809f5a7f1bbc667eea65f0dd23",
     "grade": true,
     "grade_id": "cell-6c18e8b59491da88",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) What is the recall if we classify `1` as our \"positive\" class? \n",
    "\n",
    "B) What is the precision weighted average? Save the result to 4 decimal places. \n",
    "\n",
    "C) What is the `f1` score using `1` as your positive class? Save the result to 4 decimal places in an object named `answer3_5c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a9e662e9f3bbeed4ca24535d401ca1a",
     "grade": true,
     "grade_id": "cell-b137e90717db0384",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Model Remarks\n",
    "rubric={reasoning:2}\n",
    "\n",
    "How well did your model do classifying the movie reviews? Take into consideration the new metrics we have learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2950328508347666e84f0e0521666a3c",
     "grade": true,
     "grade_id": "cell-9c3b84c023deaafa",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3: Model Interpretation <a name=\"4\"></a>\n",
    "<hr>\n",
    "\n",
    "One of the primary advantage of linear models is their ability to interpret models in terms of important features. In this exercise, we'll explore the weights learned by logistic regression classifier.\n",
    "\n",
    "Below we've create a dataframe that contains the words used in our optimal model along with their coefficients (remember to name your `RandomizedSearchCV` object in question 2.5 `random_search` or the code here will return an error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "coef_df = pd.DataFrame({'words': best_estimator[ \"countvectorizer\"].get_feature_names(),\n",
    "              'coefficient': best_estimator[\"logisticregression\"].coef_[0]})\n",
    "\n",
    "coef_df\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get the most informative positive words\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "Using the dataframe `coef_df` above, find the 10 words that are most indicative of a positive review.\n",
    "\n",
    "Elaborate on the positive words here - Do they make sense with their target value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "deb736bbb5dddd6491fdc2b857496093",
     "grade": true,
     "grade_id": "cell-cc24c17746873a66",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1db9f2d81bfdf2c9862ba32cc15426bb",
     "grade": true,
     "grade_id": "cell-194decec6e177890",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get the most informative negative words\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "Using the dataframe `coef_df` above, find the 10 words that are most indicative of a negative review.\n",
    "\n",
    "Elaborate on the negative words here - Do they make sense with their target value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8204c067f7d09d61eae73b9dd317e4d3",
     "grade": true,
     "grade_id": "cell-65fef712d6bb84e9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e422f1eee64332a1201d3d619074fa4",
     "grade": true,
     "grade_id": "cell-fdf0115bf49f694c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Explaining the coefficients?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Do the words associated with positive and negative reviews make sense? Why is it useful to get access to this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a3f205c3e959c5342f7d0cbe4b927e",
     "grade": true,
     "grade_id": "cell-63b14cebb7a11cb7",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Using `predict` vs `predict_proba`\n",
    "rubric={accuracy:3}\n",
    "\n",
    "Make a dataframe named `results_df` that contains these 5 columns: \n",
    "\n",
    "- `review` - this should contain the reviews from `X_test`.\n",
    "- `true_label` - This should contain the true `y_test` values. \n",
    "- `predicted_y` - The predicted labels generated from `best_model` for the `X_test` reviews using `.predict()`. \n",
    "- `neg_label_prob` - The probabilities of class `0` generated from `best_model` for the `X_test` reviews. These can be found at index 0 of the `predict_proba` output (you can get that using `[:,0]`). \n",
    "-  `pos_label_prob` - The probabilities of class `1` generated from `best_model` for the `X_test` reviews. These can be found at index 0 of the `predict_proba` output (you can get that using `[:,1]`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401397ea147294573ff27a9847e81efb",
     "grade": false,
     "grade_id": "cell-e9e255458e78e61e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c7f6d4eb49c08cbaae6bea2d43ab2ef",
     "grade": true,
     "grade_id": "cell-7ba72d22c03211d0",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_4(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Looking into the probability scores with positive reviews \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Find the top 5 movie reviews in `results_df` with the highest predicted probability of being positive (i.e., where the model is most confident that the review is positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7455657b3dbaaaa3203d52dba0cdda90",
     "grade": true,
     "grade_id": "cell-b794552666257a21",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore these reviews and see how positive they read!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Looking into the probability scores with negative reviews \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Find the top 5 movie reviews in `results_df` with the highest predicted probability of being negative (i.e., where the model is most confident that the review is negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19501b5e792fc6fd5d630fc710f4693e",
     "grade": true,
     "grade_id": "cell-73ce7298413a8d35",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Looking at uncertain reviews (optional)\n",
    "rubric={0}\n",
    "\n",
    "This is an optional question!\n",
    "\n",
    "(You'll get 0 marks for this one but you may have fun doing it?!) \n",
    "\n",
    "Find the 5 movie reviews in the test set with the most divided probability of being negative or positive (i.e., where the model is least confident in either review sentiment).\n",
    "\n",
    "Can you see why the model is confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "437e3dafe8823026c74bbe181523292b",
     "grade": true,
     "grade_id": "cell-b7cf706150aee3ad",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Looking at wrongly predicted reviews. (optional)\n",
    "rubric={0}\n",
    "\n",
    "Here is another optional question!\n",
    "\n",
    "Examine a review from the test set where our `best_model` is making mistakes, i.e., where the true labels do not match the predicted labels. \n",
    "\n",
    "Is is clear why this model predicted this review incorrectly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb13ce074ed5ce3dc7132b4b71f39319",
     "grade": true,
     "grade_id": "cell-e907b019266bedb7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Canvas\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Verify that the tests from the questions you answered have obtained the output \"Success\"\n",
    "- Convert your notebook to .html format by going to File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Upload your `.ipynb` file and the `.html` file to Canvas under Assignment1. \n",
    "- **DO NOT** upload any `.csv` files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing Assignment 3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.986px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
