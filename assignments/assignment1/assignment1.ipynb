{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Class meetings 01, 02, and 03. \n",
    "\n",
    "__Due__: Wednesday, March 7 at 10:00am (i.e., the start of Class Meeting 04).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "You must use proper spelling and grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals\n",
    "\n",
    "Here we will try classification of the famous handwritten digits data set. \n",
    "\n",
    "This data set exists in many forms; we will use the one bundled in `sklearn.datasets`. We will also use `sklearn` for classification.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the documentation for the data by running `print(digits['DESCR'])`. We'll extract the features and labels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits['data'] # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "Y = digits['target'] # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x118d3e390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADU9JREFUeJzt3X/sXfVdx/HXa1+gkxXaIGDwWzKIgyYo2ViwZumckx/a\nCXYLMRGUiWNJg4QFIskEIQb9Q/SfZkQNpGHMbRTIZJAsA7ewANlIHFJ+uK2UChSE1rGWbIwfUhH6\n8o97a74u3/aeb+85597vu89HcsP9ce55v0/Iq59zzvfc83ESAajpXZNuAEB3CDhQGAEHCiPgQGEE\nHCiMgAOFEfBFxPZ1tm/dz+ebbX90gev8ddtbx24OU4mATxHbr8957LH95pzXfzjq+0l+OcmDC6mZ\n5DtJVh5w0w3YPsX2Jts/GT6+ZfuULmtigIBPkSRL9z4kvSDpd+e8t3HS/Y3hPyX9vqSjh4+vSbpj\noh0dJAj44nOY7S/Zfm24S3763g9sP2/7rOHzVcNR81XbP7K9fr6V2f6o7e1zXv+Z7R3D9W+1feY+\nvneO7ceH63/R9nX7ajjJK0meTfKOJEt6R9L7DmzzsRAEfPFZq8Hot1yDkfDv97HcDZJuSHKkpF+S\n9JVRK7a9UtJlkn41yRGSflvS8/tY/A1JfzTs4xxJf2L7EyPW/4qk3ZL+TtJfj+oH4yPgi89DSe4d\njoZflvT+fSz3P5LeZ/voJK8n+W6Ddb8jaYmkU2wfmuT5JM/Ot2CSB5N8P8meJN+TdLuk39jfypMs\nl7RMg39EHm/QD8ZEwBefl+Y8/y9J77Z9yDzLfVrSyZKesv2I7XNHrTjJM5KukHSdpJ2277D9i/Mt\na/vXbD9ge5ftn0q6RIPj61E13pB0k6Qv2T521PIYDwEvKsnTSS6QdKykv5V0p+33NPjebUk+LOm9\nkjL87nxu0+AQ4fgkyzQIrRu29y5Jh0uabbg8DhABL8r2hbaPSbJH0ivDt/eM+M5K22fYXqLBsfKb\n+/nOEZJ+nGS37VWS/mA/6z3b9mm2Z2wfKWm9pJ9I2rLAzcICEfC61kjabPt1DU64nZ/kzRHfWSLp\nbyS9rMGhwLGSrt7HspdK+ivbr0n6C+3/JN5yDY7RfyrpWQ1O+q1JsrvhtuAAmRs+AHUxggOFEXCg\nMAIOFEbAgcLmu0BibLZLnrmbmZnptd6pp57aW60+t23Xrl291XrhhRd6q9W3JCOvO+jkLHrVgB91\n1FG91tu2bVtvtZYtW9ZbrRtvvLG3WpdeemlvtfrWJODsogOFEXCgMAIOFEbAgcIIOFAYAQcKI+BA\nYQQcKIyAA4U1CrjtNcNb6D5j+6qumwLQjpEBtz0j6R8kfUzSKZIuYFYKYHFoMoKvkvRMkm1J3tLg\nntwf77YtAG1oEvBZSS/Oeb1d89wN0/a64Uwam9pqDsB4Wvu5aJINkjZIdX9NBiw2TUbwHZKOn/N6\nxfA9AFOuScAfkXSS7RNtHybpfA1ueA9gyo3cRU/ytu3LJH1T0oykW5Js7rwzAGNrdAye5F5J93bc\nC4CWcSUbUBgBBwoj4EBhBBwojIADhRFwoDACDhTGzCYLcNJJJ/Va76abbuqt1saNG3urtX79+t5q\nLV++vLdafWNmE+AgR8CBwgg4UBgBBwoj4EBhBBwojIADhRFwoDACDhRGwIHCmsxscovtnbZ/0EdD\nANrTZAT/R0lrOu4DQAdGBjzJtyX9uIdeALSstZlNbK+TtK6t9QEYH1MXAYVxFh0ojIADhTX5M9nt\nkv5F0krb221/uvu2ALShydxkF/TRCID2sYsOFEbAgcIIOFAYAQcKI+BAYQQcKIyAA4W1di36weDp\np5/utd4ll1zSW62LLrqot1pLlizprdbBjhEcKIyAA4URcKAwAg4URsCBwgg4UBgBBwoj4EBhBBwo\njIADhTW5J9vxth+w/aTtzbYv76MxAONrci3625KuTPKY7SMkPWr7viRPdtwbgDE1mbroh0keGz5/\nTdIWSbNdNwZgfAv6NZntEySdJunheT5j6iJgyjQOuO2lkr4q6Yokr/7s50xdBEyfRmfRbR+qQbg3\nJrmr25YAtKXJWXRL+rykLUnWd98SgLY0GcFXS/qkpDNsPzF8/E7HfQFoQZOpix6S5B56AdAyrmQD\nCiPgQGEEHCiMgAOFEXCgMAIOFEbAgcIIOFAYc5NNsbVr1/ZW65prrumt1v33399brYMdIzhQGAEH\nCiPgQGEEHCiMgAOFEXCgMAIOFEbAgcIIOFBYk5suvtv2v9r+t+HURX/ZR2MAxtfkUtX/lnRGkteH\nt09+yPY/J/lux70BGFOTmy5G0uvDl4cOH0xsACwCTSc+mLH9hKSdku5LMu/URbY32d7UdpMADkyj\ngCd5J8kHJK2QtMr2r8yzzIYkpyc5ve0mARyYBZ1FT/KKpAckremmHQBtanIW/Rjby4fPf07S2ZKe\n6roxAONrchb9OElftD2jwT8IX0ny9W7bAtCGJmfRv6fBnOAAFhmuZAMKI+BAYQQcKIyAA4URcKAw\nAg4URsCBwgg4UJgHvwZteaU2PyddZFavXt1brXvuuae3WhdffHFvtSTprrvu6q1WEo9ahhEcKIyA\nA4URcKAwAg4URsCBwgg4UBgBBwoj4EBhBBwojIADhTUO+HDyg8dtc8NFYJFYyAh+uaQtXTUCoH1N\npy5aIekcSTd32w6ANjUdwT8n6bOS9uxrAeYmA6ZPk5lNzpW0M8mj+1uOucmA6dNkBF8taa3t5yXd\nIekM27d22hWAVowMeJKrk6xIcoKk8yXdn+TCzjsDMDb+Dg4U1mTywf+T5EFJD3bSCYDWMYIDhRFw\noDACDhRGwIHCCDhQGAEHCiPgQGFMXbQAO3bs6LXe7Oxsr/X6ct555/VW6/rrr++tliStXLmyt1pM\nXQQc5Ag4UBgBBwoj4EBhBBwojIADhRFwoDACDhRGwIHCCDhQWKNbNg3vqPqapHckvc2tkYHFYSH3\nZPvNJC931gmA1rGLDhTWNOCR9C3bj9peN98CTF0ETJ+mu+gfTrLD9rGS7rP9VJJvz10gyQZJG6S6\nPxcFFptGI3iSHcP/7pR0t6RVXTYFoB1NJh98j+0j9j6X9FuSftB1YwDG12QX/Rck3W177/K3JflG\np10BaMXIgCfZJun9PfQCoGX8mQwojIADhRFwoDACDhRGwIHCCDhQGAEHClvIz0XRsyuvvLK3Ws89\n91xvtc4666zeai1durS3WtOIERwojIADhRFwoDACDhRGwIHCCDhQGAEHCiPgQGEEHCiMgAOFNQq4\n7eW277T9lO0ttj/UdWMAxtf0WvQbJH0jye/ZPkzS4R32BKAlIwNue5mkj0j6Y0lK8pakt7ptC0Ab\nmuyinyhpl6Qv2H7c9s3D+6P/P0xdBEyfJgE/RNIHJd2Y5DRJb0i66mcXSrIhyelMLQxMjyYB3y5p\ne5KHh6/v1CDwAKbcyIAneUnSi7ZXDt86U9KTnXYFoBVNz6J/RtLG4Rn0bZI+1V1LANrSKOBJnpDE\nsTWwyHAlG1AYAQcKI+BAYQQcKIyAA4URcKAwAg4URsCBwpibbAFmZ2d7rbd169beap188sm91dq9\ne3dvta699treak0jRnCgMAIOFEbAgcIIOFAYAQcKI+BAYQQcKIyAA4URcKCwkQG3vdL2E3Mer9q+\noo/mAIxn5KWqSbZK+oAk2Z6RtEPS3R33BaAFC91FP1PSs0n+o4tmALRroT82OV/S7fN9YHudpHVj\ndwSgNY1H8OE90ddK+qf5PmfqImD6LGQX/WOSHkvyo66aAdCuhQT8Au1j9xzAdGoU8OF0wWdLuqvb\ndgC0qenURW9I+vmOewHQMq5kAwoj4EBhBBwojIADhRFwoDACDhRGwIHCCDhQmJO0v1J7l6SF/qT0\naEkvt97MdKi6bWzX5Lw3yTGjFuok4AfC9qaqv0Srum1s1/RjFx0ojIADhU1TwDdMuoEOVd02tmvK\nTc0xOID2TdMIDqBlBBwobCoCbnuN7a22n7F91aT7aYPt420/YPtJ25ttXz7pntpke8b247a/Pule\n2mR7ue07bT9le4vtD026p3FM/Bh8OJnCv2twS6jtkh6RdEGSJyfa2JhsHyfpuCSP2T5C0qOSPrHY\nt2sv238q6XRJRyY5d9L9tMX2FyV9J8nNwzsJH57klUn3daCmYQRfJemZJNuSvCXpDkkfn3BPY0vy\nwySPDZ+/JmmLpNnJdtUO2ysknSPp5kn30ibbyyR9RNLnJSnJW4s53NJ0BHxW0otzXm9XkSDsZfsE\nSadJeniynbTmc5I+K2nPpBtp2YmSdkn6wvDw4+bhDUcXrWkIeGm2l0r6qqQrkrw66X7GZftcSTuT\nPDrpXjpwiKQPSroxyWmS3pC0qM8JTUPAd0g6fs7rFcP3Fj3bh2oQ7o1JqtxyerWktbaf1+Bw6gzb\nt062pdZsl7Q9yd49rTs1CPyiNQ0Bf0TSSbZPHJ7UOF/S1ybc09hsW4NjuS1J1k+6n7YkuTrJiiQn\naPD/6v4kF064rVYkeUnSi7ZXDt86U9KiPim60MkHW5fkbduXSfqmpBlJtyTZPOG22rBa0iclfd/2\nE8P3/jzJvRPsCaN9RtLG4WCzTdKnJtzPWCb+ZzIA3ZmGXXQAHSHgQGEEHCiMgAOFEXCgMAIOFEbA\ngcL+F/NfbvE7oSNbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d298d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title('This is a %d' % digits['target'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to element 0 (`X[0]`) -- element 1 (`X[1]`) or element 2 (`X[2]`)? Report the two distances (Euclidean).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error\n",
    "\n",
    "You'll be using the scikit-learn implementation of the $k$-NN classifier. Documentation is available at http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Using `k=10`, fit a $k$-NN classifier using `X` and `Y`. Obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error\n",
    "\n",
    "Now fit the classifier using `k=1`, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Re-scaling\n",
    "\n",
    "Often, features are measured on different scales. Consider the toy example `data/unequal_scales.csv`, which has two features (`x1` and `x2`) and four labels (`A` through `D`). Feel free to use R for this question (but it's up to you). \n",
    "\n",
    "__Note__: To annotate a `ggplot2` plot with a circle, you can add the function found in [Vincenzo's gist](https://gist.github.com/vincenzocoia/58b26f6778647be1803d0b7348e674ec) as a layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a) Leaving the data as-is\n",
    "\n",
    "\n",
    "1. Make a scatterplot of `x1` and `x2`, and indicate the category using some other aesthetic (like colour and/or shape).\n",
    "2. Draw a circle, centered around the point `(x1,x2)=(1,0)`, containing the $k=10$ nearest points to `(1,0)`. \n",
    "3. What would the point `(1,0)` be categorized as, using these data with $k$-NN, $k=10$? Why? Comment on whether you think it's a good idea to categorize the point `(1,0)` using a circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Re-scaling\n",
    "\n",
    "\n",
    "To prevent this problem from happening, we can re-scale the data so that each feature has unit variance. We can achieve this by _normalizing_ the data -- that is, subtracting by the sample mean, then dividing by the sample standard deviation (you can achieve this using the `scale` function in R).\n",
    "\n",
    "1. Normalize the two features. What does the point `(1,0)` become on this new scale?\n",
    "2. Make another scatterplot of the re-scaled features, with a circle centered around the re-scaled point containing the $k=10$ nearest neighbours. \n",
    "3. What would the re-scaled point be categorized as? Why? Comment on whether you think the re-scaling is appropriate, and why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c) Comparing errors\n",
    "\n",
    "\n",
    "Using all the data as training data, plot the training error as a function of $k$. Do this for both the original and scaled data. How do the errors compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error\n",
    "\n",
    "This is a continuation of Exercise 1. Each part asks you to investigate some scenario.\n",
    "\n",
    "### 2(a) The influence of k\n",
    "\n",
    "Now, split the data into _training_ and _test_ sets. You can choose any reasonable fraction for training vs. testing (50% will do). \n",
    "\n",
    "__Note__: It's always a good idea to randomly shuffle the data before splitting, in case the data comes ordered in some way. (For example, if they are ordered by label, then your training set will be all the digits 0-4, and your test set all the digits 5-9, which would be bad... you might end up with 100% error!!) To shuffle your data, you can use [`numpy.random.shuffle`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html).\n",
    "\n",
    "For various values of $k$, fit (a.k.a. _train_) a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? Make a plot to show the trends, and briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Fundamental Tradeoff\n",
    "\n",
    "\n",
    "Recall the two parts of the fundamental trade-off in machine learning:\n",
    "\n",
    "1. How small we can make the training error.\n",
    "2. How well the training error approximates the test error.\n",
    "\n",
    "In a $k$-nearest neighbour classifier, how does the parameter $k$ affect each of the two parts of the trade-off? What is one way that we could choose $k$ in practice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c) The influence of data partition\n",
    "\n",
    "\n",
    "Now, choose your favourite value of $k$, but vary the proportion of data reserved for the training set, again obtaining training and test error rates for each partition of the data. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(d) Imbalanced training data\n",
    "\n",
    "Now, take a subset of your training data so that you only retain the first 2% of the examples of digits 0-8, but keep 100% of the 9's. This is called an imbalanced training set. \n",
    "\n",
    "What is your training and test error, vs. $k$? How does it compare to your performance with all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2(e) Imbalanced training data -- fair (Optional)\n",
    "\n",
    "What we did in (d) above was an unfair comparison, because reducing the amount of training data will generally hurt performance. To compare, randomly remove examples from the original training set such that the number of examples is the same as in part (d) above. Now compare the training and test error to the results from (d). Briefly comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(f) The influence of added noise (Optional; no marks)\n",
    "\n",
    "__This question is worth no marks.__\n",
    "\n",
    "Now, add noise to the training data: for each example, with probability 20% replace the training label with a label selected uniformly at random. Remake the plot vs. $k$. What do you observe? Is there a generalizable insight here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------\n",
    "__Attribution__: Much of this material was taken from DSCI 571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
