{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 1\n",
    "\n",
    "__Evaluates__: Lectures 1 - 4. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions. In particular, here are the most relevant ones:\n",
    "\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md), for evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md), for evaluating your written responses.\n",
    "\n",
    "__Attribution__: This assignment was created by Tomas Beuzen and Vincenzo Coia.\n",
    "\n",
    "## Tidy Submission (5%)\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- You must use proper English, spelling, and grammar.\n",
    "- You will submit two things to Canvas:\n",
    "    1. This jupyter notebook file containing your responses; and,\n",
    "    2. A html file of your completed notebook (use `jupyter nbconvert --to html_embed assignment.ipynb` in the terminal to generate the html file).\n",
    "- Submit your assignment through [UBC Canvas](https://canvas.ubc.ca/courses/35074) by **6pm Monday 20th January**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals (worth a total of 35%)\n",
    "\n",
    "\n",
    "Here we will attempt classification of the famous handwritten digits data set. This data set exists in many forms; but we will use the one bundled in `sklearn.datasets` in Python. You can read more about the data [here](https://scikit-learn.org/stable/datasets/index.html#digits-dataset).\n",
    "\n",
    "Use the following cell to load and extract the data into features (`X`) and target (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits['data']    # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "y = digits['target']  # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title(f\"This is a {digits['target'][idx]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals (15%)\n",
    "\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to the digit 0 (`X[0]`): the digit 1 (`X[1]`) or the digit 2 (`X[2]`)? Report the two Euclidean distances (hint: you might find the [sklearn function euclidean_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) useful here).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error (10%)\n",
    "\n",
    "You'll be using a [*k*-NN classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for this question.\n",
    "\n",
    "Using `k=20`, fit a $k$-NN classifier with `X` and `Y` using all of the data as your training data. Then, obtain the predictions of `X`. \n",
    "\n",
    "1. What proportion of these predictions are **incorrect**? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error (10%)\n",
    "\n",
    "Now fit the classifier using `k=1`, using all of your data as training data, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error (worth a total of 20%)\n",
    "\n",
    "In lectures, we explored how the value of $k$ could influence the train/test error. In this question, we will explore how the partition of train/test data can influence error.\n",
    "\n",
    "Choose any value of $k$ between 1 and 10. For different partitions of the full data set into training and testing sets (e.g., 10%/90%, 20%/80%, 30%/70%, etc.), obtain training and test error rates. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Decision Trees (worth a total of 40%)\n",
    "\n",
    "We'll be using the famous Titanic dataset to try and predict survival of passengers (`Survival`) from `Age`, `Fare`, and `Sex`, using a decision tree classifier. You can find the data in the assignment folder on Canvas. Details of the data can be found at https://www.kaggle.com/c/titanic/data. You will need to select only the columns above from the full dataset provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) Feature pre-processing (15%)\n",
    "\n",
    "1. Load the data into a pandas dataframe using the `pd.read_csv()` function. How many observations are there in the data? Remove all rows from the dataframe that contain a `NaN` value using the function `df.dropna()`. How many observations are there now?\n",
    "2. Use one-hot-encoding to encode the `Sex` column to numeric values (don't forget to drop the first column using the argument `drop=\"first\"` and to specify `sparse=False`). Ouput the head of the transformed dataframe.\n",
    "3. Is it necessary to scale the numeric features for use in your decision tree classifier? In 1-2 sentences, briefly explain your answer.\n",
    "4. Split the data into 80% training, 20% testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Hyperparameter optimization (10%)\n",
    "\n",
    "1. Using 10-fold cross validation and the training set only, find an appropriate value for the `max_depth` hyperparameter for a decision tree classifier. Make a plot of training error and cross-validation error for different values of `max_depth`.\n",
    "2. In 1-2 sentences, briefly discuss what sections of your plot likely represent a model that is well fit to the data and which sections represent a model that is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Final model (15%)\n",
    "\n",
    "Based on your results from 3(b) select a value for the hyperparameter `max_depth`. In 1-2 sentences briefly explain why you chose this particular value. Train a decision tree classifier using the training set and your chosen `max_depth` hyperparameter and then obtain the test error on the test set."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
